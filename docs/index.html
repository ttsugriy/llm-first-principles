<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Building LLMs from First Principles</title>
    <style>
        :root {
            --bg: #fafafa;
            --text: #1a1a1a;
            --accent: #2563eb;
            --accent-light: #3b82f6;
            --muted: #6b7280;
            --border: #e5e7eb;
            --code-bg: #f3f4f6;
        }

        @media (prefers-color-scheme: dark) {
            :root {
                --bg: #111827;
                --text: #f9fafb;
                --accent: #60a5fa;
                --accent-light: #93c5fd;
                --muted: #9ca3af;
                --border: #374151;
                --code-bg: #1f2937;
            }
        }

        * {
            box-sizing: border-box;
            margin: 0;
            padding: 0;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
            line-height: 1.7;
            color: var(--text);
            background: var(--bg);
            padding: 2rem;
            max-width: 800px;
            margin: 0 auto;
        }

        header {
            margin-bottom: 3rem;
            padding-bottom: 2rem;
            border-bottom: 1px solid var(--border);
        }

        h1 {
            font-size: 2.5rem;
            font-weight: 700;
            margin-bottom: 0.5rem;
            letter-spacing: -0.02em;
        }

        .subtitle {
            font-size: 1.25rem;
            color: var(--muted);
            margin-bottom: 1rem;
        }

        .author {
            font-size: 0.95rem;
            color: var(--muted);
        }

        .author a {
            color: var(--accent);
            text-decoration: none;
        }

        .author a:hover {
            text-decoration: underline;
        }

        h2 {
            font-size: 1.5rem;
            font-weight: 600;
            margin: 2.5rem 0 1rem;
            letter-spacing: -0.01em;
        }

        h3 {
            font-size: 1.1rem;
            font-weight: 600;
            margin: 1.5rem 0 0.75rem;
            color: var(--muted);
            text-transform: uppercase;
            letter-spacing: 0.05em;
        }

        p {
            margin-bottom: 1rem;
        }

        .philosophy {
            background: var(--code-bg);
            border-left: 4px solid var(--accent);
            padding: 1.25rem 1.5rem;
            margin: 1.5rem 0;
            font-style: italic;
        }

        .spiral {
            margin: 2rem 0;
            padding: 1.5rem;
            border: 1px solid var(--border);
            border-radius: 8px;
        }

        .spiral-header {
            display: flex;
            align-items: center;
            gap: 0.75rem;
            margin-bottom: 1rem;
        }

        .spiral-number {
            background: var(--accent);
            color: white;
            width: 2rem;
            height: 2rem;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-weight: 600;
            font-size: 0.9rem;
        }

        .spiral-title {
            font-size: 1.2rem;
            font-weight: 600;
        }

        .stage-list {
            list-style: none;
            display: grid;
            gap: 0.5rem;
        }

        .stage-list li {
            padding: 0.5rem 0;
            border-bottom: 1px solid var(--border);
        }

        .stage-list li:last-child {
            border-bottom: none;
        }

        .stage-list a {
            color: var(--accent);
            text-decoration: none;
            font-weight: 500;
        }

        .stage-list a:hover {
            text-decoration: underline;
        }

        .stage-list .coming-soon {
            color: var(--muted);
        }

        .badge {
            display: inline-block;
            font-size: 0.7rem;
            padding: 0.15rem 0.5rem;
            border-radius: 4px;
            margin-left: 0.5rem;
            font-weight: 500;
            text-transform: uppercase;
        }

        .badge-available {
            background: #dcfce7;
            color: #166534;
        }

        .badge-soon {
            background: var(--code-bg);
            color: var(--muted);
        }

        @media (prefers-color-scheme: dark) {
            .badge-available {
                background: #166534;
                color: #dcfce7;
            }
        }

        footer {
            margin-top: 4rem;
            padding-top: 2rem;
            border-top: 1px solid var(--border);
            color: var(--muted);
            font-size: 0.9rem;
        }

        .links {
            display: flex;
            gap: 1.5rem;
            margin-top: 1rem;
        }

        .links a {
            color: var(--accent);
            text-decoration: none;
        }

        .links a:hover {
            text-decoration: underline;
        }
    </style>
</head>
<body>
    <header>
        <h1>Building LLMs from First Principles</h1>
        <p class="subtitle">Complete mathematical derivations. From-scratch implementation. Performance-focused.</p>
        <p class="author">By <a href="https://softwarebits.substack.com/">Taras Tsugrii</a></p>
    </header>

    <main>
        <section>
            <p>
                This is a comprehensive educational resource that teaches Large Language Model development
                from absolute first principles. Every mathematical formula is derived. Every algorithm is
                implemented from scratch. Every design decision is analyzed for performance trade-offs.
            </p>

            <div class="philosophy">
                "Performance is the product of deep understanding of foundations."
            </div>

            <p>
                No "it's well known that..." hand-waving. No magic library calls. If you finish this series,
                you'll understand <em>exactly</em> why modern LLMs work the way they do.
            </p>
        </section>

        <h2>The Curriculum</h2>
        <p>The content follows a <strong>spiral learning</strong> approach: concepts are introduced simply,
        then revisited with increasing depth across 5 spirals and 18 stages.</p>

        <div class="spiral">
            <div class="spiral-header">
                <div class="spiral-number">1</div>
                <div class="spiral-title">Foundations</div>
            </div>
            <ul class="stage-list">
                <li>
                    <a href="stage-01/">Stage 1: Markov Chains</a>
                    <span class="badge badge-available">Available</span>
                    <br><small>The simplest language model. Training = counting = MLE.</small>
                </li>
                <li>
                    <span class="coming-soon">Stage 2: Automatic Differentiation</span>
                    <span class="badge badge-soon">Coming Soon</span>
                    <br><small>Build autograd from scratch. Forward and reverse mode.</small>
                </li>
                <li>
                    <span class="coming-soon">Stage 3: Backpropagation Deep Dive</span>
                    <span class="badge badge-soon">Coming Soon</span>
                    <br><small>Gradients for every operation. The chain rule in action.</small>
                </li>
                <li>
                    <span class="coming-soon">Stage 4: Neural Language Model</span>
                    <span class="badge badge-soon">Coming Soon</span>
                    <br><small>From counting to learning. Embeddings and MLPs.</small>
                </li>
            </ul>
        </div>

        <div class="spiral">
            <div class="spiral-header">
                <div class="spiral-number">2</div>
                <div class="spiral-title">Training</div>
            </div>
            <ul class="stage-list">
                <li>
                    <span class="coming-soon">Stage 5: Optimization</span>
                    <span class="badge badge-soon">Coming Soon</span>
                    <br><small>SGD to Adam. Derive every update rule.</small>
                </li>
                <li>
                    <span class="coming-soon">Stage 6: Training Stability</span>
                    <span class="badge badge-soon">Coming Soon</span>
                    <br><small>Initialization, normalization, residuals. Why training works.</small>
                </li>
            </ul>
        </div>

        <div class="spiral">
            <div class="spiral-header">
                <div class="spiral-number">3</div>
                <div class="spiral-title">The Transformer</div>
            </div>
            <ul class="stage-list">
                <li>
                    <span class="coming-soon">Stage 7: Attention Mechanism</span>
                    <span class="badge badge-soon">Coming Soon</span>
                    <br><small>Query-Key-Value. Multi-head attention. The heart of transformers.</small>
                </li>
                <li>
                    <span class="coming-soon">Stage 8: Full Transformer</span>
                    <span class="badge badge-soon">Coming Soon</span>
                    <br><small>Putting it all together. Decoder-only architecture.</small>
                </li>
                <li>
                    <span class="coming-soon">Stage 9: Position Encodings</span>
                    <span class="badge badge-soon">Coming Soon</span>
                    <br><small>Sinusoidal to RoPE. How transformers know position.</small>
                </li>
                <li>
                    <span class="coming-soon">Stage 10: Tokenization</span>
                    <span class="badge badge-soon">Coming Soon</span>
                    <br><small>BPE from scratch. Why tokenization matters for O(nÂ²).</small>
                </li>
            </ul>
        </div>

        <div class="spiral">
            <div class="spiral-header">
                <div class="spiral-number">4</div>
                <div class="spiral-title">Making It Fast</div>
            </div>
            <ul class="stage-list">
                <li>
                    <span class="coming-soon">Stage 11: Memory Analysis</span>
                    <span class="badge badge-soon">Coming Soon</span>
                    <br><small>Where does memory go? Activations, gradients, optimizer states.</small>
                </li>
                <li>
                    <span class="coming-soon">Stage 12: Flash Attention</span>
                    <span class="badge badge-soon">Coming Soon</span>
                    <br><small>Memory-efficient attention. Tiling and online softmax.</small>
                </li>
                <li>
                    <span class="coming-soon">Stage 13: Distributed Training</span>
                    <span class="badge badge-soon">Coming Soon</span>
                    <br><small>Data, tensor, and pipeline parallelism. ZeRO.</small>
                </li>
            </ul>
        </div>

        <div class="spiral">
            <div class="spiral-header">
                <div class="spiral-number">5</div>
                <div class="spiral-title">Modern Practice</div>
            </div>
            <ul class="stage-list">
                <li>
                    <span class="coming-soon">Stage 14: Modern Architectures</span>
                    <span class="badge badge-soon">Coming Soon</span>
                    <br><small>GPT to Llama. GQA, SwiGLU, RMSNorm. Scaling laws.</small>
                </li>
                <li>
                    <span class="coming-soon">Stage 15: Fine-Tuning</span>
                    <span class="badge badge-soon">Coming Soon</span>
                    <br><small>SFT, LoRA, QLoRA. Efficient adaptation.</small>
                </li>
                <li>
                    <span class="coming-soon">Stage 16: RLHF & Alignment</span>
                    <span class="badge badge-soon">Coming Soon</span>
                    <br><small>Reward models, PPO, DPO. Making models helpful.</small>
                </li>
                <li>
                    <span class="coming-soon">Stage 17: Inference Optimization</span>
                    <span class="badge badge-soon">Coming Soon</span>
                    <br><small>KV cache, speculative decoding, continuous batching.</small>
                </li>
                <li>
                    <span class="coming-soon">Stage 18: Quantization</span>
                    <span class="badge badge-soon">Coming Soon</span>
                    <br><small>INT8, INT4, GPTQ, AWQ. Running big models on small hardware.</small>
                </li>
            </ul>
        </div>
    </main>

    <footer>
        <p>All code is MIT licensed and available on GitHub.</p>
        <div class="links">
            <a href="https://github.com/USER/llm-first-principles">GitHub</a>
            <a href="https://softwarebits.substack.com/">Substack</a>
        </div>
    </footer>
</body>
</html>
