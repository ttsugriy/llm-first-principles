# Building LLMs from First Principles

**A rigorous, bottom-up approach to understanding language models.**

This book derives every concept from first principles. No hand-waving. No "it's well known that..." Every formula is explained, every claim is proven.

## What Makes This Different

Most LLM tutorials tell you *what* to do. This book shows you *why* it works:

- **Full mathematical derivations** — Chain rule proved by induction, MLE derived with Lagrange multipliers
- **Code from scratch** — Every algorithm implemented, every design decision explained
- **First principles pedagogy** — Each concept builds only on what's already been covered

## The Journey

| Stage | Topic | Status |
|-------|-------|--------|
| 1 | [Markov Chains](stages/stage-01/index.md) | ✅ Complete |
| 2 | [Automatic Differentiation](stages/stage-02/index.md) | ✅ Complete |
| 3-18 | Neural LMs → Transformers → RLHF | Planned |

## Prerequisites

- Basic Python programming
- High school algebra
- Curiosity about how things work

## How to Use This Book

Each stage is self-contained but builds on previous stages:

1. **Read the theory** — Understand the mathematical foundations
2. **Study the code** — See how theory translates to implementation
3. **Do the exercises** — Solidify understanding through practice
4. **Reflect** — Connect new concepts to the bigger picture

The book follows Pólya's problem-solving method:
- **Understand** the problem
- **Devise** a plan
- **Execute** the plan
- **Reflect** on the solution

## Get Started

→ [Begin with Stage 1: Markov Chains](stages/stage-01/index.md)
