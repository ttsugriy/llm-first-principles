# Stage 3: Neural Language Models (Coming Soon)

## Building Your First Neural Language Model

With automatic differentiation in hand, we're ready to build neural networks. In this stage, we'll construct a neural language model from scratchâ€”no PyTorch, no TensorFlow, just the autograd system we built ourselves.

## Planned Topics

### Embeddings
How to represent discrete tokens as continuous vectors. The geometry of word space.

### Feed-Forward Neural Networks
Layers, activations, and the universal approximation theorem. Building blocks for everything that follows.

### Training Dynamics
Loss landscapes, learning rates, and optimization. Why training neural networks is both an art and a science.

### From N-grams to Neural
Replacing count-based probability with learned representations. The conceptual leap from Stage 1.

### Building a Character-Level LM
Our first complete neural language model. Training it to generate text character by character.

## What You'll Build

A working neural language model that:

- Learns from text data
- Generates coherent sequences
- Uses our own autograd engine from Stage 2
- Demonstrates key concepts we'll build on in later stages

## Prerequisites

- Stage 1: Markov Chains (probability, MLE, perplexity)
- Stage 2: Automatic Differentiation (gradients, backprop, autograd)

---

*This stage is under active development. Check back soon!*
