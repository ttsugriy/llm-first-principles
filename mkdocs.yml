site_name: Building LLMs from First Principles
site_url: https://ttsugriy.github.io/llm-first-principles/
site_author: Taras Tsugrii
site_description: A rigorous, bottom-up approach to understanding language models

repo_name: ttsugriy/llm-first-principles
repo_url: https://github.com/ttsugriy/llm-first-principles

theme:
  name: material
  palette:
    - scheme: default
      primary: indigo
      accent: indigo
      toggle:
        icon: material/brightness-7
        name: Switch to dark mode
    - scheme: slate
      primary: indigo
      accent: indigo
      toggle:
        icon: material/brightness-4
        name: Switch to light mode
  features:
    - navigation.sections
    - navigation.expand
    - navigation.top
    - navigation.footer
    - toc.integrate
    - content.code.copy

plugins:
  - search

markdown_extensions:
  - pymdownx.arithmatex:
      generic: true
  - pymdownx.highlight:
      anchor_linenums: true
  - pymdownx.superfences:
      custom_fences:
        - name: mermaid
          class: mermaid
          format: !!python/name:pymdownx.superfences.fence_code_format
  - pymdownx.tabbed:
      alternate_style: true
  - admonition
  - pymdownx.details
  - tables
  - toc:
      permalink: true
  - attr_list
  - md_in_html
  - pymdownx.emoji:
      emoji_index: !!python/name:material.extensions.emoji.twemoji
      emoji_generator: !!python/name:material.extensions.emoji.to_svg

extra_javascript:
  - javascripts/mathjax.js
  - https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js
  - https://unpkg.com/mermaid@10/dist/mermaid.min.js
  - javascripts/mermaid-init.js

nav:
  - Home: index.md
  - Interactive Tools: interactive/index.md
  - Glossary: glossary.md
  - Stage 1 - Markov Chains:
    - Overview: stages/stage-01/index.md
    - 1.1 Probability Foundations: stages/stage-01/01-probability-foundations.md
    - 1.2 Language Modeling Problem: stages/stage-01/02-language-modeling-problem.md
    - 1.3 MLE Derivation: stages/stage-01/03-mle-derivation.md
    - 1.3b Smoothing: stages/stage-01/03b-smoothing.md
    - 1.4 Information Theory: stages/stage-01/04-information-theory.md
    - 1.5 Perplexity: stages/stage-01/05-perplexity.md
    - 1.6 Temperature Sampling: stages/stage-01/06-temperature-sampling.md
    - 1.7 Implementation: stages/stage-01/07-implementation.md
    - 1.8 Trade-offs: stages/stage-01/08-trade-offs.md
    - 1.9 Why Neural Networks: stages/stage-01/09-why-neural.md
  - Stage 2 - Automatic Differentiation:
    - Overview: stages/stage-02/index.md
    - 2.1 What is a Derivative?: stages/stage-02/01-what-is-derivative.md
    - 2.2 Derivative Rules: stages/stage-02/02-derivative-rules.md
    - 2.3 The Chain Rule: stages/stage-02/03-chain-rule.md
    - 2.4 Computational Graphs: stages/stage-02/04-computational-graphs.md
    - 2.5 Forward vs Reverse Mode: stages/stage-02/05-forward-vs-reverse.md
    - 2.6 Building Autograd: stages/stage-02/06-autograd-from-scratch.md
    - 2.7 Testing and Validation: stages/stage-02/07-testing-validation.md
  - Stage 3 - Neural Language Models:
    - Overview: stages/stage-03/index.md
    - 3.1 Why Neural?: stages/stage-03/01-why-neural.md
    - 3.2 Embeddings: stages/stage-03/02-embeddings.md
    - 3.3 Feed-Forward Networks: stages/stage-03/03-feed-forward.md
    - 3.4 Cross-Entropy Loss: stages/stage-03/04-cross-entropy.md
    - 3.5 Implementation: stages/stage-03/05-implementation.md
    - 3.6 Training Dynamics: stages/stage-03/06-training-dynamics.md
    - 3.7 Evaluation: stages/stage-03/07-evaluation.md
  - Stage 4 - Optimization:
    - Overview: stages/stage-04/index.md
    - 4.1 The Optimization Problem: stages/stage-04/01-optimization-problem.md
    - 4.2 Gradient Descent: stages/stage-04/02-gradient-descent.md
    - 4.3 Stochastic Gradient Descent: stages/stage-04/03-sgd.md
    - 4.4 Momentum: stages/stage-04/04-momentum.md
    - 4.5 Adaptive Learning Rates: stages/stage-04/05-adaptive.md
    - 4.6 Learning Rate Schedules: stages/stage-04/06-schedules.md
    - 4.7 Implementation: stages/stage-04/07-implementation.md
    - 4.8 Practical Considerations: stages/stage-04/08-practical.md
  - Stage 5 - Attention:
    - Overview: stages/stage-05/index.md
    - 5.1 The Attention Problem: stages/stage-05/01-attention-problem.md
    - 5.2 Dot-Product Attention: stages/stage-05/02-dot-product-attention.md
    - 5.3 Scaled Attention: stages/stage-05/03-scaled-attention.md
    - 5.4 Self-Attention: stages/stage-05/04-self-attention.md
    - 5.5 Multi-Head Attention: stages/stage-05/05-multi-head.md
    - 5.6 Positional Encoding: stages/stage-05/06-positional-encoding.md
    - 5.7 Causal Masking: stages/stage-05/07-causal-masking.md
    - 5.8 Implementation: stages/stage-05/08-implementation.md
  - Stage 6 - The Complete Transformer:
    - Overview: stages/stage-06/index.md
    - 6.1 Tokenization: stages/stage-06/01-tokenization.md
    - 6.2 Transformer Block: stages/stage-06/02-transformer-block.md
    - 6.3 Deep Networks: stages/stage-06/03-deep-networks.md
    - 6.4 Pre-training Objectives: stages/stage-06/04-pretraining.md
    - 6.5 Training at Scale: stages/stage-06/05-training-scale.md
    - 6.6 Modern Architectures: stages/stage-06/06-architectures.md
    - 6.7 Scaling Laws: stages/stage-06/07-scaling-laws.md
    - 6.8 Implementation: stages/stage-06/08-implementation.md
