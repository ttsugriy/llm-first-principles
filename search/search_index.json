{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Building LLMs from First Principles","text":"<p>A rigorous, bottom-up approach to understanding language models.</p> <p>This book derives every concept from first principles. No hand-waving. No \"it's well known that...\" Every formula is explained, every claim is proven.</p>"},{"location":"#what-makes-this-different","title":"What Makes This Different","text":"<p>Most LLM tutorials tell you what to do. This book shows you why it works:</p> <ul> <li>Full mathematical derivations \u2014 Chain rule proved by induction, MLE derived with Lagrange multipliers</li> <li>Code from scratch \u2014 Every algorithm implemented, every design decision explained</li> <li>First principles pedagogy \u2014 Each concept builds only on what's already been covered</li> </ul>"},{"location":"#the-journey","title":"The Journey","text":"Stage Topic Status 1 Markov Chains \u2705 Complete 2 Automatic Differentiation \u2705 Complete 3 Neural Language Models \u2705 Complete 4-18 RNNs \u2192 Transformers \u2192 RLHF Planned"},{"location":"#prerequisites","title":"Prerequisites","text":"<ul> <li>Basic Python programming</li> <li>High school algebra</li> <li>Curiosity about how things work</li> </ul>"},{"location":"#how-to-use-this-book","title":"How to Use This Book","text":"<p>Each stage is self-contained but builds on previous stages:</p> <ol> <li>Read the theory \u2014 Understand the mathematical foundations</li> <li>Study the code \u2014 See how theory translates to implementation</li> <li>Do the exercises \u2014 Solidify understanding through practice</li> <li>Reflect \u2014 Connect new concepts to the bigger picture</li> </ol> <p>The book follows P\u00f3lya's problem-solving method: - Understand the problem - Devise a plan - Execute the plan - Reflect on the solution</p>"},{"location":"#get-started","title":"Get Started","text":"<p>\u2192 Begin with Stage 1: Markov Chains</p>"},{"location":"stages/stage-04-preview/","title":"Stage 4: Recurrent Neural Networks (Coming Soon)","text":""},{"location":"stages/stage-04-preview/#unbounded-context-learning-to-remember","title":"Unbounded Context: Learning to Remember","text":"<p>Our Stage 3 neural language model uses a fixed context window. But language has structure that spans paragraphs, pages, even entire documents. How do we model dependencies of arbitrary length?</p> <p>Recurrent Neural Networks (RNNs) solve this by maintaining a hidden state that accumulates information over time.</p>"},{"location":"stages/stage-04-preview/#planned-topics","title":"Planned Topics","text":""},{"location":"stages/stage-04-preview/#the-limitations-of-fixed-context","title":"The Limitations of Fixed Context","text":"<p>Why feed-forward networks can't handle variable-length sequences naturally.</p>"},{"location":"stages/stage-04-preview/#the-recurrent-architecture","title":"The Recurrent Architecture","text":"<p>Sharing weights across time steps. How RNNs process sequences.</p>"},{"location":"stages/stage-04-preview/#backpropagation-through-time","title":"Backpropagation Through Time","text":"<p>Extending our autograd to handle temporal dependencies. The chain rule across time.</p>"},{"location":"stages/stage-04-preview/#vanishing-and-exploding-gradients","title":"Vanishing and Exploding Gradients","text":"<p>The fundamental challenge of learning long-range dependencies. Mathematical analysis.</p>"},{"location":"stages/stage-04-preview/#lstm-and-gru","title":"LSTM and GRU","text":"<p>Gated architectures that enable learning over longer sequences. Deriving the gate mechanisms.</p>"},{"location":"stages/stage-04-preview/#building-an-rnn-language-model","title":"Building an RNN Language Model","text":"<p>Implementation from scratch, building on our Stage 2-3 foundations.</p>"},{"location":"stages/stage-04-preview/#what-youll-build","title":"What You'll Build","text":"<p>A recurrent language model that:</p> <ul> <li>Processes sequences of arbitrary length</li> <li>Maintains memory across the sequence</li> <li>Uses gating to control information flow</li> <li>Generates more coherent long-form text</li> </ul>"},{"location":"stages/stage-04-preview/#prerequisites","title":"Prerequisites","text":"<ul> <li>Stage 1: Probability and language modeling</li> <li>Stage 2: Automatic differentiation</li> <li>Stage 3: Neural language models (embeddings, training)</li> </ul> <p>This stage is under active development. Check back soon!</p>"},{"location":"stages/stage-01/","title":"Stage 1: The Simplest Language Model","text":"<p>Markov Chains \u2014 Where It All Begins</p> <p>This stage builds a language model from absolute first principles. Every concept is derived, not stated. By the end, you'll understand not just what language models do, but why they work mathematically.</p> <p>Reading Time</p> <p>60-90 minutes | Prerequisites: Basic Python, high school algebra</p>"},{"location":"stages/stage-01/#overview","title":"Overview","text":"<p>We begin with the simplest possible language model: a Markov chain. Despite its simplicity, this model introduces concepts that underpin all modern LLMs:</p> <ul> <li>Autoregressive factorization \u2014 the same decomposition used by GPT, LLaMA, and Claude</li> <li>Maximum likelihood estimation \u2014 the same training objective</li> <li>Cross-entropy loss \u2014 the same loss function</li> <li>Temperature sampling \u2014 the same generation technique</li> <li>Perplexity \u2014 the same evaluation metric</li> </ul> <p>The difference between a Markov chain and GPT-4 isn't in what they compute, but how they compute it.</p>"},{"location":"stages/stage-01/#what-youll-learn","title":"What You'll Learn","text":"<ol> <li> <p>Probability Foundations \u2014 Kolmogorov axioms, conditional probability, chain rule (proved by induction)</p> </li> <li> <p>The Language Modeling Problem \u2014 Why exponential space makes direct modeling impossible</p> </li> <li> <p>Maximum Likelihood Estimation \u2014 Full Lagrangian derivation proving counting = optimal</p> </li> <li> <p>Information Theory \u2014 Entropy, cross-entropy, KL divergence derived from axioms</p> </li> <li> <p>Perplexity \u2014 The standard evaluation metric, with the \"effective vocabulary\" interpretation</p> </li> <li> <p>Temperature Sampling \u2014 From softmax to Boltzmann distribution</p> </li> <li> <p>Implementation \u2014 Complete ~150-line implementation with every design decision explained</p> </li> <li> <p>The Fundamental Trade-offs \u2014 Why we need neural networks</p> </li> </ol>"},{"location":"stages/stage-01/#key-formulas","title":"Key Formulas","text":"Concept Formula Intuition Chain Rule \\(P(x_{1:n}) = \\prod_i P(x_i \\mid x_{&lt;i})\\) Factor into conditionals MLE \\(P(b\\mid a) = \\frac{\\text{count}(a,b)}{\\text{count}(a,\\cdot)}\\) Counting is optimal Cross-entropy \\(H(P,Q) = -\\mathbb{E}_P[\\log Q]\\) Average surprise Perplexity \\(\\exp(H)\\) Effective vocabulary Temperature \\(P'(x) \\propto P(x)^{1/T}\\) Control randomness"},{"location":"stages/stage-01/#the-central-insight","title":"The Central Insight","text":"<p>The Fundamental Trade-off</p> <p>More context \u2192 better predictions</p> <p>More context \u2192 sparser observations</p> <p>We prove this quantitatively and show experimental data. This limitation is why we need neural networks: they can generalize from similar patterns rather than requiring exact matches.</p>"},{"location":"stages/stage-01/#begin-reading","title":"Begin Reading","text":"<p>Start with the foundations:</p> <p>\u2192 Section 1.1: Probability Foundations</p>"},{"location":"stages/stage-01/01-probability-foundations/","title":"Section 1.1: Probability Foundations","text":"<p>Before we can build a language model, we need to understand probability. Not just use it\u2014understand it. This section builds the foundation everything else rests on.</p>"},{"location":"stages/stage-01/01-probability-foundations/#what-is-probability","title":"What is Probability?","text":"<p>Probability is a way of quantifying uncertainty. When we say \"the probability of rain tomorrow is 70%,\" we're expressing our degree of belief that it will rain.</p> <p>But what does that number actually mean? There are two main interpretations:</p> <p>Frequentist interpretation: If we could repeat tomorrow infinitely many times, it would rain in 70% of those tomorrows. Probability is the long-run frequency of an event.</p> <p>Bayesian interpretation: Probability represents our degree of belief. The 70% encodes our uncertainty given our current knowledge.</p> <p>For language modeling, we'll mostly use the frequentist view: if we sample many sentences from English, what fraction start with \"The\"? That fraction is approximately P(\"The\" is the first word).</p>"},{"location":"stages/stage-01/01-probability-foundations/#the-three-axioms-of-probability","title":"The Three Axioms of Probability","text":"<p>All of probability theory follows from just three axioms, formulated by Kolmogorov in 1933:</p> <p>Axiom 1 (Non-negativity): For any event A, P(A) \u2265 0</p> <p>Probabilities can't be negative. You can't have a -30% chance of something.</p> <p>Axiom 2 (Normalization): P(\u03a9) = 1, where \u03a9 is the entire sample space</p> <p>Something must happen. The probability of some outcome occurring is 1.</p> <p>Axiom 3 (Additivity): For mutually exclusive events A and B, $\\(P(A \\cup B) = P(A) + P(B)\\)$</p> <p>If A and B can't both happen, the probability of either happening is the sum.</p> <p>From these three axioms, we can derive everything else. For example:</p> <p>P(not A) = 1 - P(A): Since A and \"not A\" are mutually exclusive and exhaust all possibilities: $\\(P(A) + P(\\text{not } A) = P(\\Omega) = 1\\)$</p>"},{"location":"stages/stage-01/01-probability-foundations/#conditional-probability","title":"Conditional Probability","text":"<p>Here's where things get interesting. Often we want to know the probability of an event given that we already know something.</p> <p>Definition: The conditional probability of A given B is: $\\(P(A|B) = \\frac{P(A \\cap B)}{P(B)}\\)$</p> <p>where P(B) &gt; 0.</p> <p>What this means: We're restricting our attention to only the cases where B happens, and asking what fraction of those cases also have A happening.</p> <p>Example: - Let A = \"the word is 'cat'\" - Let B = \"the previous word is 'the'\" - P(A|B) = P(\"the cat\") / P(\"the ...\") = (frequency of \"the cat\") / (frequency of \"the\" followed by anything)</p> <p>This is exactly what we need for language modeling.</p>"},{"location":"stages/stage-01/01-probability-foundations/#deriving-the-chain-rule","title":"Deriving the Chain Rule","text":"<p>The chain rule is the mathematical foundation of autoregressive language models. Let's derive it carefully.</p>"},{"location":"stages/stage-01/01-probability-foundations/#two-events","title":"Two Events","text":"<p>Starting from the definition of conditional probability: $\\(P(A|B) = \\frac{P(A \\cap B)}{P(B)}\\)$</p> <p>Rearranging: $\\(P(A \\cap B) = P(A|B) \\cdot P(B)\\)$</p> <p>This is the product rule: the probability of both A and B equals the probability of B times the probability of A given B.</p> <p>We could also write it the other way: $\\(P(A \\cap B) = P(B|A) \\cdot P(A)\\)$</p> <p>Both are correct. The choice depends on what we know.</p>"},{"location":"stages/stage-01/01-probability-foundations/#three-events","title":"Three Events","text":"<p>Now let's extend to three events. We want P(A \u2229 B \u2229 C).</p> <p>First, treat (A \u2229 B) as a single event and apply the product rule: $\\(P(A \\cap B \\cap C) = P(C | A \\cap B) \\cdot P(A \\cap B)\\)$</p> <p>Now expand P(A \u2229 B): $\\(P(A \\cap B \\cap C) = P(C | A \\cap B) \\cdot P(B | A) \\cdot P(A)\\)$</p> <p>Rewriting in a more suggestive order: $\\(P(A, B, C) = P(A) \\cdot P(B | A) \\cdot P(C | A, B)\\)$</p>"},{"location":"stages/stage-01/01-probability-foundations/#the-general-chain-rule","title":"The General Chain Rule","text":"<p>For n events, we apply the same logic inductively:</p> \\[P(X_1, X_2, \\ldots, X_n) = P(X_1) \\cdot P(X_2|X_1) \\cdot P(X_3|X_1,X_2) \\cdots P(X_n|X_1,\\ldots,X_{n-1})\\] <p>Or more compactly: $\\(P(X_1, \\ldots, X_n) = \\prod_{i=1}^{n} P(X_i | X_1, \\ldots, X_{i-1})\\)$</p> <p>Proof by induction:</p> <p>Base case: For n=1, P(X\u2081) = P(X\u2081). \u2713</p> <p>Inductive step: Assume true for n-1. Then: $\\(P(X_1, \\ldots, X_n) = P(X_n | X_1, \\ldots, X_{n-1}) \\cdot P(X_1, \\ldots, X_{n-1})\\)$</p> <p>By inductive hypothesis: $\\(= P(X_n | X_1, \\ldots, X_{n-1}) \\cdot \\prod_{i=1}^{n-1} P(X_i | X_1, \\ldots, X_{i-1})\\)$ $\\(= \\prod_{i=1}^{n} P(X_i | X_1, \\ldots, X_{i-1}) \\quad \\blacksquare\\)$</p> <p>This is not an approximation. The chain rule is an exact mathematical identity. It follows directly from the definition of conditional probability.</p>"},{"location":"stages/stage-01/01-probability-foundations/#why-the-chain-rule-matters-for-language","title":"Why the Chain Rule Matters for Language","text":"<p>Consider a sentence as a sequence of words: \"The cat sat down\"</p> <p>The probability of this sentence is: $\\(P(\\text{\"The cat sat down\"}) = P(\\text{The}, \\text{cat}, \\text{sat}, \\text{down})\\)$</p> <p>By the chain rule: $\\(= P(\\text{The}) \\cdot P(\\text{cat}|\\text{The}) \\cdot P(\\text{sat}|\\text{The cat}) \\cdot P(\\text{down}|\\text{The cat sat})\\)$</p> <p>We've converted the problem of assigning probability to an entire sentence into a sequence of next-word predictions. This is the autoregressive factorization.</p> <p>The term \"autoregressive\" means the model predicts each element based on previous elements\u2014it regresses on its own past outputs.</p>"},{"location":"stages/stage-01/01-probability-foundations/#summary","title":"Summary","text":"Concept Definition Why It Matters Probability Quantified uncertainty Foundation of everything Conditional probability P(A|B) = P(A\u2229B)/P(B) How we model \"given context\" Chain rule P(X\u2081...X\u2099) = \u220fP(X\u1d62|X\u2081...X\u1d62\u208b\u2081) Enables autoregressive modeling <p>We've now established the mathematical foundation. Next, we'll use these tools to define what a language model actually is.</p>"},{"location":"stages/stage-01/02-language-modeling-problem/","title":"Section 1.2: The Language Modeling Problem","text":"<p>Now that we understand probability and the chain rule, let's define precisely what we're trying to build.</p>"},{"location":"stages/stage-01/02-language-modeling-problem/#what-is-a-language-model","title":"What is a Language Model?","text":"<p>A language model is a probability distribution over sequences of tokens.</p> <p>Given a vocabulary V (a finite set of possible tokens), a language model assigns a probability to every possible sequence:</p> \\[P: V^* \\rightarrow [0, 1]\\] <p>where V* means \"sequences of any length from V\" and the probabilities over all possible sequences sum to 1.</p> <p>Examples of what a language model answers: - P(\"The cat sat on the mat\") = ? - P(\"Mat the on sat cat the\") = ? - P(\"asdfghjkl\") = ?</p> <p>A good language model assigns high probability to natural text and low probability to gibberish.</p>"},{"location":"stages/stage-01/02-language-modeling-problem/#the-problem-of-exponential-space","title":"The Problem of Exponential Space","text":"<p>Here's the fundamental challenge. Suppose our vocabulary has |V| = 50,000 tokens (roughly GPT-2's vocabulary size). How many possible sequences of length n exist?</p> Length n Possible sequences 1 50,000 2 2.5 billion 10 10^47 100 10^470 <p>For comparison, there are approximately 10^80 atoms in the observable universe.</p> <p>We cannot possibly store a probability for each sequence. Even for length-10 sequences, we'd need more storage than atoms in the universe.</p>"},{"location":"stages/stage-01/02-language-modeling-problem/#the-autoregressive-solution","title":"The Autoregressive Solution","text":"<p>The chain rule (from Section 1.1) provides the solution. Instead of modeling P(x\u2081, x\u2082, ..., x\u2099) directly, we factor it:</p> \\[P(x_1, x_2, \\ldots, x_n) = \\prod_{i=1}^{n} P(x_i | x_1, \\ldots, x_{i-1})\\] <p>Now we need to model n conditional distributions instead of one joint distribution. Each conditional distribution is over |V| possible next tokens.</p> <p>But wait\u2014each conditional P(x\u1d62 | x\u2081, ..., x\u1d62\u208b\u2081) still depends on a variable-length history. For a sequence of length 100, the last prediction conditions on 99 previous tokens. How many possible 99-token histories exist?</p> <p>Still 50,000^99 \u2248 10^465. We haven't solved the problem!</p>"},{"location":"stages/stage-01/02-language-modeling-problem/#the-markov-assumption-a-simplification","title":"The Markov Assumption: A Simplification","text":"<p>Here's where we make our first modeling assumption\u2014and it's important to recognize that this is a choice, not a mathematical necessity.</p> <p>The Markov assumption: The future depends only on the recent past.</p> <p>Specifically, for an order-k Markov model: $\\(P(x_i | x_1, \\ldots, x_{i-1}) \\approx P(x_i | x_{i-k}, \\ldots, x_{i-1})\\)$</p> <p>The probability of the next token depends only on the last k tokens, not the entire history.</p> <p>Special cases: - k=1: Bigram model. P(x\u1d62 | x\u1d62\u208b\u2081). Next token depends only on previous token. - k=2: Trigram model. P(x\u1d62 | x\u1d62\u208b\u2082, x\u1d62\u208b\u2081). Next token depends on previous two tokens. - k=n-1: Full model. No approximation, but intractable.</p>"},{"location":"stages/stage-01/02-language-modeling-problem/#why-this-helps-quantitatively","title":"Why This Helps (Quantitatively)","text":"<p>With the Markov assumption of order k, how many distinct contexts do we need to model?</p> <p>Each context is a sequence of k tokens, so there are |V|^k possible contexts.</p> <p>| Order k | Possible contexts (|V|=50,000) | |---------|-------------------------------| | 1 | 50,000 | | 2 | 2.5 billion | | 3 | 125 trillion | | 4 | 6.25 \u00d7 10^18 |</p> <p>Even k=2 is stretching it for explicit storage. k=3 or higher requires sparse representations (storing only contexts we've actually seen).</p> <p>For character-level models with |V| \u2248 100: | Order k | Possible contexts | |---------|------------------| | 1 | 100 | | 2 | 10,000 | | 3 | 1,000,000 | | 5 | 10 billion | | 10 | 10^20 |</p> <p>Character-level models can use higher orders because the vocabulary is smaller.</p>"},{"location":"stages/stage-01/02-language-modeling-problem/#why-the-markov-assumption-is-wrong","title":"Why the Markov Assumption is Wrong","text":"<p>Language has long-range dependencies that the Markov assumption cannot capture.</p> <p>Example 1: Subject-verb agreement \"The cat that sat on the mat next to the dogs was sleeping.\"</p> <p>The verb \"was\" must agree with \"cat\" (singular), not \"dogs\" (the nearest noun). A bigram model seeing \"dogs\" would likely predict \"were\".</p> <p>Example 2: Coreference \"John went to the store. He bought milk.\"</p> <p>To know that \"He\" refers to \"John\", we need information from the previous sentence.</p> <p>Example 3: Document-level coherence In a story, characters introduced in paragraph 1 must behave consistently in paragraph 10.</p>"},{"location":"stages/stage-01/02-language-modeling-problem/#why-wrong-models-can-still-be-useful","title":"Why Wrong Models Can Still Be Useful","text":"<p>Despite being wrong, Markov models are useful for several reasons:</p> <ol> <li> <p>They capture local patterns: Most of the information for predicting the next character IS in the recent past. \"th\" \u2192 \"e\" is very likely regardless of earlier context.</p> </li> <li> <p>They're trainable: We can estimate the probabilities from data (as we'll see in Section 1.3).</p> </li> <li> <p>They're interpretable: We can inspect what the model learned by looking at transition probabilities.</p> </li> <li> <p>They're a stepping stone: Understanding Markov models helps us understand why neural language models are better.</p> </li> </ol> <p>The progression through this course will be: - Markov models (this stage): Wrong but simple - Neural LMs (Stage 4): Less wrong, can learn representations - Transformers (Stage 7-8): Can model arbitrary-length dependencies</p> <p>Each step fixes a limitation of the previous approach.</p>"},{"location":"stages/stage-01/02-language-modeling-problem/#formal-definition-order-k-markov-language-model","title":"Formal Definition: Order-k Markov Language Model","text":"<p>An order-k Markov language model is defined by:</p> <ol> <li>A vocabulary V (finite set of tokens)</li> <li>A special start token \u27e8START\u27e9 and end token \u27e8END\u27e9</li> <li> <p>A set of transition probabilities: for each context c \u2208 V^k and each token t \u2208 V \u222a {\u27e8END\u27e9}:    $\\(\\theta_{c \\rightarrow t} = P(t | c)\\)$</p> </li> <li> <p>Constraints:</p> </li> <li>All probabilities non-negative: \u03b8_{c\u2192t} \u2265 0</li> <li>Probabilities sum to 1 for each context: \u2211t \u03b8{c\u2192t} = 1</li> </ol> <p>The probability of a sequence x\u2081, x\u2082, ..., x\u2099 is: $\\(P(x_1, \\ldots, x_n) = \\prod_{i=1}^{n+1} \\theta_{c_i \\rightarrow x_i}\\)$</p> <p>where: - x_{n+1} = \u27e8END\u27e9 - c\u1d62 = (x_{i-k}, ..., x_{i-1}) with padding using \u27e8START\u27e9 for i \u2264 k</p>"},{"location":"stages/stage-01/02-language-modeling-problem/#what-we-need-to-learn","title":"What We Need to Learn","text":"<p>To use a Markov language model, we need to determine the transition probabilities \u03b8.</p> <p>The question is: Given training data (a corpus of text), how do we estimate these probabilities?</p> <p>This is the subject of Section 1.3, where we'll derive that the optimal approach is simply counting\u2014and prove why this is optimal.</p>"},{"location":"stages/stage-01/02-language-modeling-problem/#summary","title":"Summary","text":"Concept Definition Key Insight Language model P(sequence) Probability distribution over text Exponential space |V|^n sequences Can't store explicitly Autoregressive \u220fP(x\u1d62|x&lt;i) Factor into conditionals Markov assumption P(x\u1d62|x&lt;i) \u2248 P(x\u1d62|x\u1d62\u208b\u2096...x\u1d62\u208b\u2081) Only recent history matters Order k Context is last k tokens Higher k = better but sparser <p>Next: How do we learn these probabilities from data?</p>"},{"location":"stages/stage-01/03-mle-derivation/","title":"Section 1.3: Learning from Data \u2014 Maximum Likelihood Estimation","text":"<p>We have a model structure (Markov chain) with parameters \u03b8 (transition probabilities). Now we need to learn those parameters from data.</p> <p>This section derives from first principles why counting is the optimal way to estimate probabilities, and proves it rigorously using calculus.</p>"},{"location":"stages/stage-01/03-mle-derivation/#what-is-a-parameter","title":"What is a Parameter?","text":"<p>A parameter is a number that defines our model's behavior. For a bigram model, the parameters are all the transition probabilities:</p> <p>\u03b8 = {\u03b8_{a\u2192b} : for all a \u2208 V, b \u2208 V \u222a {\u27e8END\u27e9}}</p> <p>where \u03b8_{a\u2192b} = P(b | a).</p> <p>For a vocabulary of size |V|, we have |V| \u00d7 (|V| + 1) parameters (each of |V| contexts can transition to |V| tokens plus END).</p>"},{"location":"stages/stage-01/03-mle-derivation/#the-learning-problem","title":"The Learning Problem","text":"<p>Given: A corpus of text (training data) Find: Values for all parameters \u03b8 that make the model \"good\"</p> <p>But what does \"good\" mean? We need a criterion for evaluating parameter choices.</p>"},{"location":"stages/stage-01/03-mle-derivation/#the-likelihood-function","title":"The Likelihood Function","text":"<p>Key idea: A good model should assign high probability to the training data.</p> <p>If we observed data D, then the likelihood of parameters \u03b8 is: $\\(L(\\theta) = P(D | \\theta)\\)$</p> <p>This reads: \"The probability of observing data D, if the true parameters were \u03b8.\"</p> <p>Important: D is fixed (we observed it). \u03b8 is what varies. The likelihood is a function of \u03b8.</p> <p>Example: Suppose we observed the sequence \"ab\" and we're learning a bigram model.</p> \\[L(\\theta) = P(\\text{\"ab\"} | \\theta) = \\theta_{\\text{START} \\rightarrow a} \\cdot \\theta_{a \\rightarrow b} \\cdot \\theta_{b \\rightarrow \\text{END}}\\] <p>If \u03b8_{START\u2192a} = 0.1, \u03b8_{a\u2192b} = 0.2, \u03b8_{b\u2192END} = 0.5: $\\(L(\\theta) = 0.1 \\times 0.2 \\times 0.5 = 0.01\\)$</p> <p>If \u03b8_{START\u2192a} = 0.5, \u03b8_{a\u2192b} = 0.8, \u03b8_{b\u2192END} = 0.5: $\\(L(\\theta) = 0.5 \\times 0.8 \\times 0.5 = 0.2\\)$</p> <p>The second parameter setting has higher likelihood\u2014it makes the observed data more probable.</p>"},{"location":"stages/stage-01/03-mle-derivation/#maximum-likelihood-estimation-mle","title":"Maximum Likelihood Estimation (MLE)","text":"<p>Principle: Choose parameters that maximize the likelihood of the observed data.</p> \\[\\theta^* = \\arg\\max_\\theta L(\\theta) = \\arg\\max_\\theta P(D | \\theta)\\] <p>Why is this a good principle? 1. Intuitive: We want a model that considers our data likely, not surprising. 2. Consistent: As we get more data, MLE converges to the true parameters. 3. Efficient: MLE achieves the best possible accuracy for large samples.</p>"},{"location":"stages/stage-01/03-mle-derivation/#log-likelihood-a-computational-trick","title":"Log-Likelihood: A Computational Trick","text":"<p>Likelihood involves products of many probabilities. For a corpus of n tokens: $\\(L(\\theta) = \\prod_{i=1}^{n} P(x_i | \\text{context}_i; \\theta)\\)$</p> <p>Products of many small numbers cause numerical problems: - 0.1 \u00d7 0.1 \u00d7 0.1 \u00d7 ... (100 times) = 10^{-100} \u2248 0 (underflow!)</p> <p>Solution: Work with logarithms.</p> \\[\\log L(\\theta) = \\log \\prod_{i=1}^{n} P(x_i | c_i) = \\sum_{i=1}^{n} \\log P(x_i | c_i)\\] <p>Why this works: 1. Log transforms products into sums (easier to compute) 2. Log is monotonically increasing, so max(log L) occurs at same \u03b8 as max(L) 3. Log of small positive numbers is large negative (no underflow)</p> <p>We call log L(\u03b8) the log-likelihood, often written \u2113(\u03b8).</p>"},{"location":"stages/stage-01/03-mle-derivation/#mle-for-bigram-models-the-derivation","title":"MLE for Bigram Models: The Derivation","text":"<p>Now we derive the MLE solution for bigram models. This is the mathematical core of this section.</p>"},{"location":"stages/stage-01/03-mle-derivation/#setup","title":"Setup","text":"<p>We have training data: a sequence x\u2081, x\u2082, ..., x\u2099.</p> <p>Let count(a, b) = number of times token b follows token a in training data. Let count(a, \u00b7) = number of times token a appears (followed by anything) = \u03a3_b count(a, b).</p> <p>We want to find \u03b8_{a\u2192b} = P(b | a) for all a, b.</p>"},{"location":"stages/stage-01/03-mle-derivation/#constraints","title":"Constraints","text":"<p>The probabilities must satisfy: 1. \u03b8_{a\u2192b} \u2265 0 for all a, b (non-negativity) 2. \u03a3_b \u03b8_{a\u2192b} = 1 for all a (normalization: probabilities sum to 1)</p>"},{"location":"stages/stage-01/03-mle-derivation/#the-log-likelihood","title":"The Log-Likelihood","text":"<p>The log-likelihood of the training data is: $\\(\\ell(\\theta) = \\sum_{\\text{all bigrams } (a,b) \\text{ in data}} \\log \\theta_{a \\rightarrow b}\\)$</p> <p>We can rewrite this by grouping identical bigrams: $\\(\\ell(\\theta) = \\sum_{a \\in V} \\sum_{b \\in V \\cup \\{\\text{END}\\}} \\text{count}(a, b) \\cdot \\log \\theta_{a \\rightarrow b}\\)$</p> <p>Each unique bigram (a, b) contributes count(a, b) \u00d7 log \u03b8_{a\u2192b} to the total.</p>"},{"location":"stages/stage-01/03-mle-derivation/#optimization-with-constraints-lagrange-multipliers","title":"Optimization with Constraints: Lagrange Multipliers","text":"<p>We want to maximize \u2113(\u03b8) subject to the constraints \u03a3_b \u03b8_{a\u2192b} = 1.</p> <p>Lagrange multipliers: To optimize f(x) subject to g(x) = 0, we find where \u2207f = \u03bb\u2207g.</p> <p>For our problem, we form the Lagrangian: $\\(\\mathcal{L}(\\theta, \\lambda) = \\ell(\\theta) - \\sum_a \\lambda_a \\left( \\sum_b \\theta_{a \\rightarrow b} - 1 \\right)\\)$</p> <p>We have one Lagrange multiplier \u03bb\u2090 for each context a (one constraint per context).</p>"},{"location":"stages/stage-01/03-mle-derivation/#taking-derivatives","title":"Taking Derivatives","text":"<p>For each parameter \u03b8_{a\u2192b}, we take the partial derivative and set it to zero:</p> \\[\\frac{\\partial \\mathcal{L}}{\\partial \\theta_{a \\rightarrow b}} = \\frac{\\text{count}(a, b)}{\\theta_{a \\rightarrow b}} - \\lambda_a = 0\\] <p>Derivation of \u2202\u2113/\u2202\u03b8_{a\u2192b}: - \u2113(\u03b8) = \u03a3_{a',b'} count(a',b') \u00b7 log \u03b8_{a'\u2192b'} - \u2202/\u2202\u03b8_{a\u2192b} of count(a,b) \u00b7 log \u03b8_{a\u2192b} = count(a,b) / \u03b8_{a\u2192b} - All other terms don't involve \u03b8_{a\u2192b}, so their derivatives are 0</p> <p>From the derivative equation: $\\(\\frac{\\text{count}(a, b)}{\\theta_{a \\rightarrow b}} = \\lambda_a\\)$</p> <p>Solving for \u03b8_{a\u2192b}: $\\(\\theta_{a \\rightarrow b} = \\frac{\\text{count}(a, b)}{\\lambda_a}\\)$</p>"},{"location":"stages/stage-01/03-mle-derivation/#finding-a-using-the-constraint","title":"Finding \u03bb\u2090 Using the Constraint","text":"<p>We know \u03a3_b \u03b8_{a\u2192b} = 1. Substituting: $\\(\\sum_b \\frac{\\text{count}(a, b)}{\\lambda_a} = 1\\)$</p> \\[\\frac{1}{\\lambda_a} \\sum_b \\text{count}(a, b) = 1\\] \\[\\frac{\\text{count}(a, \\cdot)}{\\lambda_a} = 1\\] \\[\\lambda_a = \\text{count}(a, \\cdot)\\]"},{"location":"stages/stage-01/03-mle-derivation/#the-final-result","title":"The Final Result","text":"<p>Substituting \u03bb\u2090 back: $\\(\\theta^*_{a \\rightarrow b} = \\frac{\\text{count}(a, b)}{\\text{count}(a, \\cdot)}\\)$</p> <p>This is remarkable: The optimal probability is simply the frequency!</p> \\[P^*(b | a) = \\frac{\\text{number of times } b \\text{ follows } a}{\\text{number of times } a \\text{ appears}}\\]"},{"location":"stages/stage-01/03-mle-derivation/#the-beautiful-equivalence","title":"The Beautiful Equivalence","text":"<p>Counting = Maximum Likelihood Estimation</p> <p>What we've proven: If you just count how often each transition occurs and divide by the total, you get the mathematically optimal parameters under the maximum likelihood criterion.</p> <p>This isn't an approximation or heuristic. It's provably optimal.</p> <p>Why this matters: 1. Training a Markov model is O(n) where n is corpus size\u2014just one pass through the data 2. No iterative optimization needed (unlike neural networks) 3. The solution is exact, not approximate</p>"},{"location":"stages/stage-01/03-mle-derivation/#verifying-the-solution","title":"Verifying the Solution","text":"<p>Let's verify with a simple example.</p> <p>Training data: \"abab\"</p> <p>Bigram counts (with START and END): - (START, a): 1 - (a, b): 2 - (b, a): 1 - (b, END): 1</p> <p>Context totals: - count(START, \u00b7) = 1 - count(a, \u00b7) = 2 - count(b, \u00b7) = 2</p> <p>MLE probabilities: - P(a | START) = 1/1 = 1.0 - P(b | a) = 2/2 = 1.0 - P(a | b) = 1/2 = 0.5 - P(END | b) = 1/2 = 0.5</p> <p>Verification: P(\"abab\") = P(a|START) \u00d7 P(b|a) \u00d7 P(a|b) \u00d7 P(b|a) \u00d7 P(END|b) = 1.0 \u00d7 1.0 \u00d7 0.5 \u00d7 1.0 \u00d7 0.5 = 0.25</p> <p>This is the highest probability this model can assign to \"abab\" given the constraints.</p>"},{"location":"stages/stage-01/03-mle-derivation/#extending-to-higher-order-models","title":"Extending to Higher-Order Models","text":"<p>The derivation extends naturally to order-k models:</p> \\[\\theta^*_{c \\rightarrow t} = \\frac{\\text{count}(c, t)}{\\text{count}(c, \\cdot)}\\] <p>where c is a context of k tokens.</p> <p>The principle is the same: count transitions and normalize.</p>"},{"location":"stages/stage-01/03-mle-derivation/#the-zero-probability-problem","title":"The Zero Probability Problem","text":"<p>There's a serious issue: What if we never saw a particular bigram in training?</p> <p>Example: If \"xz\" never appears in training, count(x, z) = 0, so P(z | x) = 0.</p> <p>This means any text containing \"xz\" gets probability 0, which means: - Perplexity becomes infinite - The model considers valid text impossible</p> <p>Solutions (we'll explore in exercises): - Laplace smoothing: Add 1 to all counts (as if we saw everything once) - Backoff: If bigram unseen, use unigram probability - Interpolation: Weighted combination of n-gram orders</p> <p>For now, we note this limitation and move on.</p>"},{"location":"stages/stage-01/03-mle-derivation/#summary","title":"Summary","text":"Concept Definition Key Insight Likelihood P(data | \u03b8) How probable data is under \u03b8 Log-likelihood log P(data | \u03b8) Numerically stable version MLE \u03b8* = argmax P(data | \u03b8) Choose \u03b8 that maximizes likelihood MLE for Markov count(a,b) / count(a,\u00b7) Counting IS optimization <p>The key takeaway: Training a Markov model by counting isn't a hack\u2014it's the mathematically optimal solution. We derived this from first principles using calculus.</p> <p>Next: How do we measure whether our model is good? This requires understanding information theory.</p>"},{"location":"stages/stage-01/04-information-theory/","title":"Section 1.4: Information Theory Foundations","text":"<p>To properly understand how to measure model quality, we need information theory. This section derives the key concepts from first principles, explaining why logarithms appear everywhere in machine learning.</p>"},{"location":"stages/stage-01/04-information-theory/#the-core-question-what-is-information","title":"The Core Question: What is Information?","text":"<p>Intuitively, information is what reduces uncertainty. When someone tells you something you already knew, you gain no information. When they tell you something surprising, you gain a lot.</p> <p>Claude Shannon's insight (1948): We can quantify information mathematically.</p>"},{"location":"stages/stage-01/04-information-theory/#deriving-the-information-formula","title":"Deriving the Information Formula","text":"<p>Let's derive the formula for information from basic requirements.</p> <p>Setup: An event has probability p. How much information (in some units) do we gain when we learn it occurred?</p> <p>Let I(p) denote the information gained from an event with probability p.</p> <p>Requirement 1: Rare events give more information. If p\u2081 &lt; p\u2082, then I(p\u2081) &gt; I(p\u2082). Learning something unlikely happened is more informative.</p> <p>Requirement 2: Certain events give no information. I(1) = 0. If something was guaranteed to happen, learning it happened tells us nothing.</p> <p>Requirement 3: Information from independent events adds. If A and B are independent, learning both gives: I(P(A and B)) = I(P(A)) + I(P(B)) For independent events: P(A and B) = P(A) \u00b7 P(B) So: I(P(A) \u00b7 P(B)) = I(P(A)) + I(P(B))</p> <p>The key constraint: We need a function where f(x\u00b7y) = f(x) + f(y).</p> <p>What function turns products into sums?</p> <p>The logarithm! log(x\u00b7y) = log(x) + log(y)</p> <p>So I(p) must be of the form: I(p) = -log(p) \u00d7 (some constant)</p> <p>The negative sign ensures rare events (small p, so log p is negative) give positive information.</p> <p>Choosing the constant: If we use log base 2, the unit is \"bits.\" If base e, \"nats.\" If base 10, \"digits.\"</p> \\[\\boxed{I(p) = -\\log_2(p) = \\log_2(1/p)}\\] <p>This is the information content or surprisal of an event with probability p.</p>"},{"location":"stages/stage-01/04-information-theory/#understanding-the-formula","title":"Understanding the Formula","text":"<p>Let's verify this makes sense:</p> Probability Information (bits) Interpretation 1.0 0 Certain event, no surprise 0.5 1 Like a coin flip 0.25 2 Like two coin flips 0.125 3 Like three coin flips 0.01 6.64 Quite surprising 0.001 9.97 Very surprising <p>The coin flip interpretation: If an event has probability 1/2\u207f, learning it occurred gives n bits of information\u2014equivalent to learning the outcomes of n fair coin flips.</p>"},{"location":"stages/stage-01/04-information-theory/#why-bits","title":"Why Bits?","text":"<p>The term \"bit\" (binary digit) comes from a physical interpretation:</p> <p>To distinguish between N equally likely possibilities, you need log\u2082(N) yes/no questions.</p> <p>Example: There are 8 equally likely outcomes. How many bits to identify which occurred? - log\u2082(8) = 3 bits - Indeed: \"Is it in the first half?\" (3 questions distinguish 8 things)</p> <p>So -log\u2082(p) = log\u2082(1/p) tells us: \"How many yes/no questions would it take to identify this outcome among equally-likely alternatives?\"</p>"},{"location":"stages/stage-01/04-information-theory/#entropy-average-information","title":"Entropy: Average Information","text":"<p>If we have a random variable X with distribution P(X), the entropy H(X) is the expected (average) information:</p> \\[H(X) = \\mathbb{E}[-\\log P(X)] = -\\sum_x P(x) \\log P(x)\\] <p>Entropy measures the average surprise, or equivalently, the average number of bits needed to encode outcomes of X.</p> <p>Example 1: Fair coin P(heads) = P(tails) = 0.5 $\\(H = -0.5 \\log_2(0.5) - 0.5 \\log_2(0.5) = -0.5(-1) - 0.5(-1) = 1 \\text{ bit}\\)$</p> <p>Example 2: Biased coin (90% heads) P(heads) = 0.9, P(tails) = 0.1 $\\(H = -0.9 \\log_2(0.9) - 0.1 \\log_2(0.1)\\)$ $\\(= -0.9(-0.152) - 0.1(-3.322) = 0.137 + 0.332 = 0.469 \\text{ bits}\\)$</p> <p>The biased coin has lower entropy\u2014it's more predictable.</p> <p>Example 3: Certain coin (always heads) P(heads) = 1, P(tails) = 0 $\\(H = -1 \\cdot \\log_2(1) - 0 \\cdot \\log_2(0) = 0 \\text{ bits}\\)$</p> <p>(We define 0 \u00b7 log(0) = 0 by continuity.)</p> <p>No uncertainty, no information needed.</p>"},{"location":"stages/stage-01/04-information-theory/#properties-of-entropy","title":"Properties of Entropy","text":"<ol> <li> <p>Non-negative: H(X) \u2265 0. Entropy is always non-negative.</p> </li> <li> <p>Maximum for uniform distribution: For a random variable over n outcomes, entropy is maximized when all outcomes are equally likely:    $\\(H_{\\max} = \\log_2(n)\\)$</p> </li> <li> <p>Additivity for independent variables: If X and Y are independent:    $\\(H(X, Y) = H(X) + H(Y)\\)$</p> </li> <li> <p>Concavity: Entropy is a concave function of the probability distribution.</p> </li> </ol>"},{"location":"stages/stage-01/04-information-theory/#cross-entropy-using-the-wrong-model","title":"Cross-Entropy: Using the Wrong Model","text":"<p>Now the crucial concept for language modeling.</p> <p>Scenario: The true distribution is P, but we're using a model Q to encode/predict.</p> <p>The cross-entropy is: $\\(H(P, Q) = -\\sum_x P(x) \\log Q(x) = \\mathbb{E}_{x \\sim P}[-\\log Q(x)]\\)$</p> <p>This is the average number of bits needed to encode samples from P using a code optimized for Q.</p> <p>Key insight: Cross-entropy is always at least as large as entropy: $\\(H(P, Q) \\geq H(P)\\)$</p> <p>with equality if and only if P = Q.</p> <p>Using the wrong model (Q \u2260 P) always requires more bits on average.</p>"},{"location":"stages/stage-01/04-information-theory/#cross-entropy-for-language-models","title":"Cross-Entropy for Language Models","text":"<p>For a language model, we're evaluating on a test corpus. Let's connect to our setup:</p> <ul> <li>True distribution P: The actual distribution of text (approximated by test data)</li> <li>Model distribution Q: Our Markov model's predictions</li> <li>Samples: The actual tokens in the test corpus</li> </ul> <p>The cross-entropy of our model on the test data is: $\\(H(P, Q) = -\\frac{1}{N} \\sum_{i=1}^{N} \\log Q(x_i | \\text{context}_i)\\)$</p> <p>This is exactly the average negative log-probability of the test tokens under our model!</p>"},{"location":"stages/stage-01/04-information-theory/#the-connection-to-log-likelihood","title":"The Connection to Log-Likelihood","text":"<p>Remember from Section 1.3, the log-likelihood was: $\\(\\ell(\\theta) = \\sum_i \\log P(x_i | \\text{context}; \\theta)\\)$</p> <p>Cross-entropy is: $\\(H = -\\frac{1}{N} \\sum_i \\log Q(x_i | \\text{context})\\)$</p> <p>So: $\\(H = -\\frac{\\ell}{N}\\)$</p> <p>Minimizing cross-entropy = Maximizing log-likelihood!</p> <p>This is why these objectives are equivalent. MLE is implicitly minimizing the cross-entropy between the empirical data distribution and our model.</p>"},{"location":"stages/stage-01/04-information-theory/#kl-divergence-the-gap","title":"KL Divergence: The Gap","text":"<p>The Kullback-Leibler divergence measures how different Q is from P:</p> \\[D_{KL}(P || Q) = H(P, Q) - H(P) = \\sum_x P(x) \\log \\frac{P(x)}{Q(x)}\\] <p>Properties: - D_KL(P || Q) \u2265 0 (always non-negative) - D_KL(P || Q) = 0 if and only if P = Q - D_KL(P || Q) \u2260 D_KL(Q || P) in general (not symmetric!)</p> <p>Interpretation: KL divergence is the extra bits needed when using Q instead of P.</p> <p>For language modeling: - If our model Q matches the true distribution P: D_KL = 0, cross-entropy = entropy - If our model is bad: D_KL is large, cross-entropy &gt;&gt; entropy</p>"},{"location":"stages/stage-01/04-information-theory/#why-we-minimize-cross-entropy-not-kl-divergence","title":"Why We Minimize Cross-Entropy, Not KL Divergence","text":"<p>In practice, we minimize cross-entropy, not KL divergence. Why?</p> \\[D_{KL}(P || Q) = H(P, Q) - H(P)\\] <p>Since H(P) is fixed (determined by the true data distribution), minimizing H(P, Q) is equivalent to minimizing D_KL(P || Q).</p> <p>We use cross-entropy because H(P) is unknown (we'd need the true distribution), but H(P, Q) can be estimated from data.</p>"},{"location":"stages/stage-01/04-information-theory/#summary","title":"Summary","text":"Concept Formula Meaning Information I(p) = -log p Surprise of event with probability p Entropy H(P) = E[-log P(x)] Average surprise under P Cross-entropy H(P,Q) = E_P[-log Q(x)] Average surprise using model Q on data from P KL divergence D_KL(P||Q) = H(P,Q) - H(P) Extra bits from using Q instead of P <p>The big picture: 1. We want a model Q that matches the true distribution P 2. We measure this by cross-entropy (lower = better) 3. Minimizing cross-entropy = maximizing likelihood 4. This is why log-probability appears everywhere in ML</p> <p>Next: We'll convert cross-entropy into a more interpretable metric\u2014perplexity.</p>"},{"location":"stages/stage-01/05-perplexity/","title":"Section 1.5: Perplexity \u2014 The Standard Evaluation Metric","text":"<p>Cross-entropy is the theoretically correct metric, but it's hard to interpret. \"Our model has cross-entropy 4.2 bits\" doesn't mean much intuitively.</p> <p>Perplexity fixes this by converting cross-entropy into an interpretable number.</p>"},{"location":"stages/stage-01/05-perplexity/#from-cross-entropy-to-perplexity","title":"From Cross-Entropy to Perplexity","text":"<p>Definition: Perplexity is the exponential of cross-entropy:</p> \\[\\text{Perplexity} = 2^{H(P,Q)} = 2^{-\\frac{1}{N}\\sum_i \\log_2 Q(x_i | \\text{context})}\\] <p>Or equivalently, if using natural logarithms:</p> \\[\\text{Perplexity} = \\exp\\left(-\\frac{1}{N}\\sum_i \\ln Q(x_i | \\text{context})\\right)\\] <p>Why exponentiate? To return from log-space to probability-space, giving us an interpretable number.</p>"},{"location":"stages/stage-01/05-perplexity/#the-intuitive-interpretation","title":"The Intuitive Interpretation","text":"<p>Perplexity = effective vocabulary size</p> <p>If your model has perplexity K, it's as uncertain as if it were choosing uniformly among K options at each position.</p> <p>Example interpretations: | Perplexity | Interpretation | |------------|----------------| | 1 | Perfect prediction (always 100% confident in correct answer) | | 10 | Like choosing among 10 equally likely words | | 100 | Like choosing among 100 equally likely words | | 50,000 | Like random guessing over entire vocabulary |</p> <p>A good language model should have perplexity much lower than vocabulary size.</p>"},{"location":"stages/stage-01/05-perplexity/#deriving-the-effective-vocabulary-interpretation","title":"Deriving the \"Effective Vocabulary\" Interpretation","text":"<p>Let's prove that perplexity equals effective vocabulary size.</p> <p>Consider a uniform distribution over K items: Each item has probability 1/K.</p> <p>Cross-entropy of uniform Q on any P with support K: $\\(H = -\\sum_i P(x_i) \\log_2 \\frac{1}{K} = -\\sum_i P(x_i) \\cdot (-\\log_2 K) = \\log_2 K\\)$</p> <p>Perplexity: $\\(\\text{PPL} = 2^{\\log_2 K} = K\\)$</p> <p>So a uniform distribution over K items has perplexity K.</p> <p>The converse: If a model has perplexity K, it has the same average uncertainty as uniform distribution over K items.</p>"},{"location":"stages/stage-01/05-perplexity/#computing-perplexity-a-concrete-example","title":"Computing Perplexity: A Concrete Example","text":"<p>Test sequence: \"the cat\" (pretend this is our entire test set)</p> <p>Model predictions (bigram): - P(\"the\" | START) = 0.2 - P(\"cat\" | \"the\") = 0.1 - P(END | \"cat\") = 0.3</p> <p>Step 1: Compute log-probabilities - log\u2082(0.2) = -2.32 - log\u2082(0.1) = -3.32 - log\u2082(0.3) = -1.74</p> <p>Step 2: Average negative log-probability $\\(H = -\\frac{1}{3}(-2.32 - 3.32 - 1.74) = \\frac{7.38}{3} = 2.46 \\text{ bits}\\)$</p> <p>Step 3: Exponentiate $\\(\\text{PPL} = 2^{2.46} = 5.5\\)$</p> <p>Interpretation: On average, the model was as uncertain as choosing among 5.5 equally likely tokens.</p>"},{"location":"stages/stage-01/05-perplexity/#perplexity-vs-accuracy","title":"Perplexity vs. Accuracy","text":"<p>Why not just use accuracy (% of correct predictions)?</p> <p>Problem: Accuracy ignores confidence.</p> <p>Consider two models predicting \"cat\": - Model A: P(\"cat\") = 0.51, P(\"dog\") = 0.49 - Model B: P(\"cat\") = 0.99, P(\"dog\") = 0.01</p> <p>Both have 100% accuracy if \"cat\" is correct, but Model B is clearly better.</p> <p>Perplexity (via log-probability) captures this: - Model A contribution: -log\u2082(0.51) = 0.97 bits - Model B contribution: -log\u2082(0.99) = 0.01 bits</p> <p>Model B has much lower perplexity because it's more confident in correct answers.</p>"},{"location":"stages/stage-01/05-perplexity/#properties-of-perplexity","title":"Properties of Perplexity","text":"<p>1. Lower is better: Lower perplexity = better model</p> <p>2. Bounded below by 1: Perplexity \u2265 1, with equality only for perfect prediction</p> <p>3. Bounded above by vocabulary size: PPL \u2264 |V| (achieved by uniform random guessing)</p> <p>4. Infinite if any probability is 0: If the model assigns 0 probability to an observed token, perplexity = \u221e</p> <p>5. Geometric mean interpretation: $\\(\\text{PPL} = \\left( \\prod_{i=1}^N \\frac{1}{Q(x_i | \\text{context})} \\right)^{1/N}\\)$</p> <p>Perplexity is the geometric mean of the inverse probabilities.</p>"},{"location":"stages/stage-01/05-perplexity/#perplexity-on-train-vs-test","title":"Perplexity on Train vs. Test","text":"<p>Training perplexity: Evaluate model on data it was trained on. Test perplexity: Evaluate model on held-out data it never saw.</p> <p>Critical insight: Training perplexity always looks better (or equal).</p> What it measures What you want Train perplexity How well model fits training data Test perplexity How well model generalizes <p>Overfitting: When train perplexity &lt;&lt; test perplexity, the model memorized training data but doesn't generalize.</p>"},{"location":"stages/stage-01/05-perplexity/#the-relationship-hierarchy","title":"The Relationship Hierarchy","text":"<p>Let's connect all the metrics we've defined:</p> <pre><code>Probability assigned to test data: P(test | model)\n         \u2193 take log\nLog-likelihood: log P(test | model)\n         \u2193 negate and average\nCross-entropy: H = -(1/N) \u00b7 log P(test | model)\n         \u2193 exponentiate\nPerplexity: PPL = exp(H)\n</code></pre> <p>All contain the same information, just different presentations: - Log-likelihood: raw sum (for optimization) - Cross-entropy: normalized (for comparison across corpus sizes) - Perplexity: intuitive (for human interpretation)</p>"},{"location":"stages/stage-01/05-perplexity/#perplexity-in-practice-real-numbers","title":"Perplexity in Practice: Real Numbers","text":"<p>What perplexity values are typical?</p> Model Perplexity Dataset Unigram (word) ~1000 Typical English Bigram (word) ~150-300 Typical English Trigram (word) ~100-150 Typical English Neural LM (LSTM) ~50-80 Penn Treebank GPT-2 (small) ~35 Penn Treebank GPT-3 ~20 Various <p>Character-level models have different scales: | Model | Perplexity | Dataset | |-------|------------|---------| | Unigram (char) | ~27 | English text | | Order-3 Markov (char) | ~8-12 | English text | | Order-5 Markov (char) | ~4-6 | English text (but often \u221e on test) |</p>"},{"location":"stages/stage-01/05-perplexity/#the-overfitting-pattern","title":"The Overfitting Pattern","text":"<p>For Markov models, you'll observe this pattern as you increase order:</p> Order Train PPL Test PPL States 1 15 15 50 2 8 9 500 3 4 12 5,000 5 1.5 \u221e 50,000 <p>What's happening: - Train PPL keeps improving (more context = better fit) - Test PPL improves initially (capturing real patterns) - Test PPL then explodes (model sees unseen n-grams, assigns probability 0)</p> <p>This is the fundamental limitation of Markov models that we'll address with neural networks.</p>"},{"location":"stages/stage-01/05-perplexity/#summary","title":"Summary","text":"Concept Formula Interpretation Perplexity 2^{cross-entropy} Effective vocabulary size PPL = 1 Perfect model Always correct with 100% confidence PPL = |V| Random guessing No information from context PPL = \u221e Model assigns P=0 Considered token impossible <p>Key takeaways: 1. Perplexity is the standard metric for language models 2. Lower is better 3. It measures how \"surprised\" the model is on average 4. Compare train vs. test to detect overfitting</p> <p>Next: How do we generate text from our model? This requires understanding sampling and temperature.</p>"},{"location":"stages/stage-01/06-temperature-sampling/","title":"Section 1.6: Generating Text \u2014 Sampling and Temperature","text":"<p>We've learned how to train a model and evaluate it. Now: how do we use it to generate text?</p>"},{"location":"stages/stage-01/06-temperature-sampling/#the-generation-problem","title":"The Generation Problem","text":"<p>Given a trained model P(next | context), we want to produce new text that \"sounds like\" the training data.</p> <p>Autoregressive generation: 1. Start with initial context (e.g., \u27e8START\u27e9) 2. Sample next token from P(token | context) 3. Append sampled token to context 4. Repeat until \u27e8END\u27e9 or maximum length</p> <p>But step 2 hides a crucial choice: how do we sample from P(token | context)?</p>"},{"location":"stages/stage-01/06-temperature-sampling/#greedy-decoding-the-obvious-approach","title":"Greedy Decoding: The Obvious Approach","text":"<p>Greedy decoding: Always pick the highest-probability token.</p> \\[x_t = \\arg\\max_{x} P(x | \\text{context})\\] <p>Problems with greedy:</p> <ol> <li> <p>Repetitive: Once you pick a common pattern, you keep repeating it.    \"The the the the the...\"</p> </li> <li> <p>No diversity: Running generation twice gives identical output.</p> </li> <li> <p>Misses good sequences: The most likely sequence isn't always found by greedily picking most likely tokens.</p> </li> </ol> <p>Example: Consider two paths: - Greedy: \"The cat\" (P=0.3 \u00d7 0.2 = 0.06) - Alternative: \"A dog\" (P=0.2 \u00d7 0.5 = 0.10)</p> <p>Greedy picks \"The\" (0.3 &gt; 0.2) but the full sequence is less likely!</p>"},{"location":"stages/stage-01/06-temperature-sampling/#ancestral-sampling-the-theoretically-correct-approach","title":"Ancestral Sampling: The Theoretically Correct Approach","text":"<p>Ancestral sampling: Sample each token from the full distribution.</p> \\[x_t \\sim P(x | \\text{context})\\] <p>This produces samples from the true model distribution\u2014exactly what the model learned.</p> <p>How to sample from a discrete distribution: 1. List all tokens with their probabilities: P(t\u2081), P(t\u2082), ... 2. Draw a random number r uniformly from [0, 1] 3. Find the token where the cumulative probability crosses r</p> <p>Python implementation: <pre><code>import random\n\ndef sample(distribution):\n    \"\"\"Sample from a probability distribution (dict: token -&gt; prob).\"\"\"\n    r = random.random()  # Uniform [0, 1)\n    cumulative = 0.0\n    for token, prob in distribution.items():\n        cumulative += prob\n        if r &lt; cumulative:\n            return token\n    return token  # Handle floating point errors\n</code></pre></p> <p>Or using the standard library: <pre><code>import random\ntokens = list(distribution.keys())\nprobs = list(distribution.values())\nreturn random.choices(tokens, weights=probs, k=1)[0]\n</code></pre></p>"},{"location":"stages/stage-01/06-temperature-sampling/#the-problem-with-pure-sampling","title":"The Problem with Pure Sampling","text":"<p>Pure ancestral sampling can produce low-quality text because it includes the low-probability tokens.</p> <p>If P(\"the\" | context) = 0.1 and P(\"xyzzy\" | context) = 0.001, pure sampling will occasionally output \"xyzzy\"\u2014rare but possible.</p> <p>Over many tokens, unlikely events accumulate, producing incoherent text.</p> <p>We want control over how \"random\" vs \"deterministic\" the generation is.</p>"},{"location":"stages/stage-01/06-temperature-sampling/#temperature-controlling-randomness","title":"Temperature: Controlling Randomness","text":"<p>Temperature is a parameter that rescales the probability distribution before sampling.</p> <p>Given probabilities P(t) for each token t, the temperature-scaled distribution is:</p> \\[P_T(t) = \\frac{P(t)^{1/T}}{\\sum_{t'} P(t')^{1/T}}\\] <p>Or equivalently, working in log-space:</p> \\[P_T(t) = \\frac{\\exp(\\log P(t) / T)}{\\sum_{t'} \\exp(\\log P(t') / T)}\\] <p>What temperature does:</p> Temperature Effect T \u2192 0 Distribution becomes one-hot (greedy) T = 1 Original distribution (no change) T &gt; 1 Distribution becomes flatter (more random) T \u2192 \u221e Distribution becomes uniform"},{"location":"stages/stage-01/06-temperature-sampling/#deriving-the-temperature-formula","title":"Deriving the Temperature Formula","text":"<p>Where does this formula come from? It's inspired by statistical mechanics.</p>"},{"location":"stages/stage-01/06-temperature-sampling/#the-softmax-function","title":"The Softmax Function","text":"<p>First, let's understand softmax. Given \"logits\" (unnormalized log-probabilities) z\u2081, z\u2082, ..., z\u2099:</p> \\[\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_j e^{z_j}}\\] <p>This converts arbitrary real numbers into a probability distribution.</p> <p>Properties: 1. All outputs positive (due to exponential) 2. Outputs sum to 1 (due to normalization) 3. Larger z\u1d62 \u2192 larger probability</p>"},{"location":"stages/stage-01/06-temperature-sampling/#adding-temperature","title":"Adding Temperature","text":"<p>Temperature divides the logits before softmax:</p> \\[\\text{softmax}(z_i / T) = \\frac{e^{z_i/T}}{\\sum_j e^{z_j/T}}\\] <p>Why this works: - Dividing by T &gt; 1 makes logits smaller \u2192 differences smaller \u2192 distribution flatter - Dividing by T &lt; 1 makes logits larger \u2192 differences larger \u2192 distribution sharper</p>"},{"location":"stages/stage-01/06-temperature-sampling/#connection-to-statistical-mechanics","title":"Connection to Statistical Mechanics","text":"<p>In physics, the Boltzmann distribution gives the probability of a system being in state i with energy E\u1d62:</p> \\[P(i) = \\frac{e^{-E_i / kT}}{Z}\\] <p>where T is temperature and k is Boltzmann's constant.</p> <ul> <li>High temperature: System explores many states (high entropy)</li> <li>Low temperature: System settles into low-energy states</li> </ul> <p>Our language model temperature is exactly analogous: high T means exploring more options, low T means sticking to high-probability options.</p>"},{"location":"stages/stage-01/06-temperature-sampling/#visualizing-temperature-effects","title":"Visualizing Temperature Effects","text":"<p>Consider this distribution: P(A) = 0.5, P(B) = 0.3, P(C) = 0.15, P(D) = 0.05</p> <p>After temperature scaling:</p> Token T=0.5 T=1.0 T=2.0 T\u2192\u221e A 0.69 0.50 0.35 0.25 B 0.24 0.30 0.29 0.25 C 0.06 0.15 0.22 0.25 D 0.01 0.05 0.14 0.25 <p>Observations: - T=0.5: \"A\" dominates even more (69% vs 50%) - T=2.0: Distribution is more uniform - T\u2192\u221e: All tokens equally likely (25% each)</p>"},{"location":"stages/stage-01/06-temperature-sampling/#the-temperature-limit-t-0","title":"The Temperature Limit: T \u2192 0","text":"<p>As T \u2192 0, the distribution becomes a one-hot vector pointing at the highest-probability token.</p> <p>Proof: Let z\u2081 &gt; z\u2082 &gt; ... &gt; z\u2099 (sorted logits).</p> \\[\\lim_{T \\to 0} \\frac{e^{z_i/T}}{\\sum_j e^{z_j/T}} = \\lim_{T \\to 0} \\frac{e^{z_i/T}}{e^{z_1/T}(1 + \\sum_{j&gt;1} e^{(z_j-z_1)/T})}\\] <p>Since z\u2081 &gt; z\u2c7c for j &gt; 1, the terms e^{(z\u2c7c-z\u2081)/T} \u2192 0 as T \u2192 0.</p> <p>For i = 1: limit = 1 For i &gt; 1: limit = 0</p> <p>So T \u2192 0 gives greedy decoding.</p>"},{"location":"stages/stage-01/06-temperature-sampling/#implementation","title":"Implementation","text":"<pre><code>import math\n\ndef apply_temperature(distribution, temperature):\n    \"\"\"Apply temperature to a probability distribution.\n\n    Args:\n        distribution: dict mapping token -&gt; probability\n        temperature: float &gt; 0\n\n    Returns:\n        New distribution with temperature applied\n    \"\"\"\n    if temperature == 1.0:\n        return distribution\n\n    # Work in log-space for numerical stability\n    log_probs = {t: math.log(p + 1e-10) / temperature\n                 for t, p in distribution.items()}\n\n    # Subtract max for numerical stability (log-sum-exp trick)\n    max_log = max(log_probs.values())\n    exp_probs = {t: math.exp(lp - max_log)\n                 for t, lp in log_probs.items()}\n\n    # Normalize\n    total = sum(exp_probs.values())\n    return {t: p / total for t, p in exp_probs.items()}\n</code></pre> <p>The log-sum-exp trick: We subtract max before exponentiating to prevent overflow. This doesn't change the result because: $\\(\\frac{e^{z_i - c}}{\\sum_j e^{z_j - c}} = \\frac{e^{z_i} e^{-c}}{\\sum_j e^{z_j} e^{-c}} = \\frac{e^{z_i}}{\\sum_j e^{z_j}}\\)$</p>"},{"location":"stages/stage-01/06-temperature-sampling/#other-sampling-strategies","title":"Other Sampling Strategies","text":"<p>Temperature isn't the only way to control generation:</p>"},{"location":"stages/stage-01/06-temperature-sampling/#top-k-sampling","title":"Top-k Sampling","text":"<p>Only sample from the k highest-probability tokens.</p> <pre><code>def top_k(distribution, k):\n    sorted_tokens = sorted(distribution.items(),\n                          key=lambda x: -x[1])[:k]\n    total = sum(p for _, p in sorted_tokens)\n    return {t: p/total for t, p in sorted_tokens}\n</code></pre>"},{"location":"stages/stage-01/06-temperature-sampling/#nucleus-top-p-sampling","title":"Nucleus (Top-p) Sampling","text":"<p>Sample from the smallest set of tokens whose cumulative probability exceeds p.</p> <pre><code>def top_p(distribution, p):\n    sorted_tokens = sorted(distribution.items(),\n                          key=lambda x: -x[1])\n    cumulative = 0.0\n    result = {}\n    for token, prob in sorted_tokens:\n        result[token] = prob\n        cumulative += prob\n        if cumulative &gt;= p:\n            break\n    total = sum(result.values())\n    return {t: prob/total for t, prob in result.items()}\n</code></pre>"},{"location":"stages/stage-01/06-temperature-sampling/#combining-strategies","title":"Combining Strategies","text":"<p>Modern LLMs often use combinations: apply temperature, then top-p, then sample.</p>"},{"location":"stages/stage-01/06-temperature-sampling/#temperature-in-practice","title":"Temperature in Practice","text":"Use case Recommended T Code generation 0.0 - 0.3 (deterministic) Factual Q&amp;A 0.3 - 0.7 (focused) Creative writing 0.7 - 1.0 (diverse) Brainstorming 1.0 - 1.5 (exploratory) <p>ChatGPT/Claude defaults: Usually around T=0.7 to 1.0 for general use.</p>"},{"location":"stages/stage-01/06-temperature-sampling/#summary","title":"Summary","text":"Concept What it does When to use Greedy (T=0) Always pick max Deterministic output needed Low T (0.3-0.7) Mostly high-prob tokens Focused, coherent text T=1.0 Original distribution Match training distribution High T (&gt;1.0) Flatter distribution Creative, diverse output Top-k Only top k tokens Prevent rare token disasters Top-p Cumulative probability threshold Adaptive vocabulary size <p>Key takeaways: 1. Temperature controls the exploration-exploitation tradeoff 2. T=1 samples from the learned distribution 3. Lower T = more deterministic, higher T = more random 4. The formula comes from statistical mechanics / softmax</p> <p>Next: Let's implement all of this from scratch.</p>"},{"location":"stages/stage-01/07-implementation/","title":"Section 1.7: Implementation \u2014 Building It From Scratch","text":"<p>Now we implement everything we've learned. Every line of code will be explained.</p>"},{"location":"stages/stage-01/07-implementation/#design-decisions","title":"Design Decisions","text":"<p>Before coding, let's make explicit choices:</p> <p>1. Vocabulary level: Character or word? - Character: |V| \u2248 100, can use higher orders, no unknown tokens - Word: |V| \u2248 50,000, richer semantics, but sparse</p> <p>We'll use character-level for this stage\u2014it lets us explore higher-order models without running out of data.</p> <p>2. Data structure for counts: - Dense matrix: |V|^k \u00d7 |V| entries, mostly zeros - Sparse dictionary: Only store observed transitions</p> <p>We'll use nested dictionaries\u2014efficient for sparse data.</p> <p>3. Special tokens: - \u27e8START\u27e9: Marks beginning of sequence (so we can predict first real token) - \u27e8END\u27e9: Marks end of sequence (so model learns when to stop)</p>"},{"location":"stages/stage-01/07-implementation/#the-complete-implementation","title":"The Complete Implementation","text":""},{"location":"stages/stage-01/07-implementation/#part-1-data-structures","title":"Part 1: Data Structures","text":"<pre><code>from collections import defaultdict, Counter\nfrom typing import Dict, List, Tuple\nimport math\nimport random\n</code></pre> <p>Why these imports: - <code>defaultdict</code>: Creates missing keys automatically (avoids KeyError) - <code>Counter</code>: Efficiently counts occurrences - <code>Dict, List, Tuple</code>: Type hints for documentation - <code>math</code>: For log, exp - <code>random</code>: For sampling</p>"},{"location":"stages/stage-01/07-implementation/#part-2-the-markovchain-class","title":"Part 2: The MarkovChain Class","text":"<pre><code>class MarkovChain:\n    \"\"\"\n    N-gram language model using the Markov assumption.\n\n    This class implements training (counting), probability queries,\n    text generation, and evaluation (perplexity).\n    \"\"\"\n\n    def __init__(self, order: int = 1):\n        \"\"\"\n        Initialize an empty Markov chain.\n\n        Args:\n            order: Number of previous tokens to condition on.\n                   order=1 is bigram, order=2 is trigram, etc.\n\n        Data structures:\n            counts[context][token] = how many times token follows context\n            totals[context] = total transitions from context\n            vocab = set of all tokens seen\n        \"\"\"\n        if order &lt; 1:\n            raise ValueError(f\"Order must be \u2265 1, got {order}\")\n\n        self.order = order\n\n        # counts[context_tuple][next_token] = count\n        # Using defaultdict so we can write counts[c][t] += 1\n        # without checking if c or t exist\n        self.counts: Dict[Tuple[str, ...], Counter] = defaultdict(Counter)\n\n        # totals[context_tuple] = sum of all counts from this context\n        # Cached for efficiency (avoid recomputing sum each time)\n        self.totals: Dict[Tuple[str, ...], int] = defaultdict(int)\n\n        # All tokens seen during training (including END, excluding START)\n        self.vocab: set = set()\n</code></pre> <p>Why <code>defaultdict(Counter)</code>: - <code>defaultdict</code> with <code>Counter</code>: If we access <code>counts[new_context]</code>, it automatically creates an empty <code>Counter</code> for that context. - <code>Counter</code> is a dict subclass that defaults missing keys to 0. - Result: We can write <code>self.counts[context][token] += 1</code> without any existence checks.</p> <p>Why cache totals separately: - We'll query P(token | context) frequently - P requires dividing by the sum of all counts for that context - Rather than recompute <code>sum(self.counts[context].values())</code> each time, we maintain <code>totals</code> incrementally</p>"},{"location":"stages/stage-01/07-implementation/#part-3-training","title":"Part 3: Training","text":"<pre><code>    def train(self, tokens: List[str]) -&gt; None:\n        \"\"\"\n        Train the model by counting n-gram transitions.\n\n        This implements Maximum Likelihood Estimation:\n        P(token | context) = count(context, token) / count(context, *)\n\n        Args:\n            tokens: List of tokens (e.g., list of characters)\n        \"\"\"\n        # Pad the sequence with START and END tokens\n        # START tokens let us predict the first real tokens\n        # END token lets the model learn when to stop\n        padded = ['&lt;START&gt;'] * self.order + tokens + ['&lt;END&gt;']\n\n        # Slide a window of size (order + 1) across the sequence\n        # Each window gives us (context, next_token)\n        for i in range(len(padded) - self.order):\n            # Context: order tokens ending at position i+order-1\n            context = tuple(padded[i : i + self.order])\n\n            # Next token: the one right after the context\n            next_token = padded[i + self.order]\n\n            # Update counts\n            self.counts[context][next_token] += 1\n            self.totals[context] += 1\n\n            # Track vocabulary (we'll need this for smoothing, vocab size, etc.)\n            self.vocab.add(next_token)\n</code></pre> <p>Why <code>tuple</code> for context: - Lists are mutable and can't be dictionary keys - Tuples are immutable and hashable \u2192 can be dict keys</p> <p>The sliding window: For \"hello\" with order=2: <pre><code>Padded: [&lt;START&gt;, &lt;START&gt;, h, e, l, l, o, &lt;END&gt;]\nIndex:     0        1      2  3  4  5  6    7\n\ni=0: context=(&lt;START&gt;,&lt;START&gt;), next=h\ni=1: context=(&lt;START&gt;,h),       next=e\ni=2: context=(h,e),             next=l\ni=3: context=(e,l),             next=l\ni=4: context=(l,l),             next=o\ni=5: context=(l,o),             next=&lt;END&gt;\n</code></pre></p>"},{"location":"stages/stage-01/07-implementation/#part-4-probability-queries","title":"Part 4: Probability Queries","text":"<pre><code>    def probability(self, context: Tuple[str, ...], token: str) -&gt; float:\n        \"\"\"\n        Get P(token | context) from the model.\n\n        Args:\n            context: Tuple of previous tokens (must have length = self.order)\n            token: The token to get probability for\n\n        Returns:\n            Probability in [0, 1]. Returns 0 if context never seen.\n        \"\"\"\n        if context not in self.counts:\n            # Context never observed \u2192 we have no information\n            # Could use backoff or smoothing here (see exercises)\n            return 0.0\n\n        # MLE: P(token | context) = count(context, token) / count(context, *)\n        return self.counts[context][token] / self.totals[context]\n\n    def get_distribution(self, context: Tuple[str, ...]) -&gt; Dict[str, float]:\n        \"\"\"\n        Get the full probability distribution P(* | context).\n\n        Args:\n            context: Tuple of previous tokens\n\n        Returns:\n            Dictionary mapping each possible next token to its probability.\n            Empty dict if context never seen.\n        \"\"\"\n        if context not in self.counts:\n            return {}\n\n        total = self.totals[context]\n        return {\n            token: count / total\n            for token, count in self.counts[context].items()\n        }\n</code></pre>"},{"location":"stages/stage-01/07-implementation/#part-5-text-generation","title":"Part 5: Text Generation","text":"<pre><code>    def generate(\n        self,\n        max_length: int = 100,\n        temperature: float = 1.0,\n        seed: str = \"\"\n    ) -&gt; str:\n        \"\"\"\n        Generate text using ancestral sampling.\n\n        Args:\n            max_length: Maximum tokens to generate\n            temperature: Sampling temperature (1.0 = unmodified)\n            seed: Optional starting text\n\n        Returns:\n            Generated text as string\n        \"\"\"\n        # Initialize context from seed or START tokens\n        if seed:\n            tokens = list(seed)\n            # Use last 'order' tokens as context\n            if len(tokens) &gt;= self.order:\n                context = tuple(tokens[-self.order:])\n            else:\n                # Pad with START if seed is too short\n                padding = ['&lt;START&gt;'] * (self.order - len(tokens))\n                context = tuple(padding + tokens)\n            generated = list(seed)\n        else:\n            context = tuple(['&lt;START&gt;'] * self.order)\n            generated = []\n\n        # Generate tokens one at a time\n        for _ in range(max_length):\n            # Get probability distribution for next token\n            dist = self.get_distribution(context)\n\n            if not dist:\n                # No transitions from this context (never seen in training)\n                break\n\n            # Apply temperature\n            if temperature != 1.0:\n                dist = self._apply_temperature(dist, temperature)\n\n            # Sample from distribution\n            next_token = self._sample(dist)\n\n            # Stop if we hit END\n            if next_token == '&lt;END&gt;':\n                break\n\n            # Append to output\n            generated.append(next_token)\n\n            # Update context: slide window right by 1\n            context = tuple(list(context)[1:] + [next_token])\n\n        return ''.join(generated)\n\n    def _apply_temperature(\n        self,\n        dist: Dict[str, float],\n        temperature: float\n    ) -&gt; Dict[str, float]:\n        \"\"\"Apply temperature scaling to distribution.\"\"\"\n        # Convert to log-space, scale, convert back\n        log_probs = {\n            token: math.log(prob + 1e-10) / temperature\n            for token, prob in dist.items()\n        }\n\n        # Subtract max for numerical stability\n        max_log = max(log_probs.values())\n        exp_probs = {\n            token: math.exp(lp - max_log)\n            for token, lp in log_probs.items()\n        }\n\n        # Normalize to sum to 1\n        total = sum(exp_probs.values())\n        return {token: prob / total for token, prob in exp_probs.items()}\n\n    def _sample(self, dist: Dict[str, float]) -&gt; str:\n        \"\"\"Sample a token from a probability distribution.\"\"\"\n        tokens = list(dist.keys())\n        probs = list(dist.values())\n        return random.choices(tokens, weights=probs, k=1)[0]\n</code></pre>"},{"location":"stages/stage-01/07-implementation/#part-6-evaluation","title":"Part 6: Evaluation","text":"<pre><code>    def perplexity(self, tokens: List[str]) -&gt; float:\n        \"\"\"\n        Compute perplexity on a sequence.\n\n        Perplexity = exp(-1/N * sum(log P(token | context)))\n\n        Lower is better. Returns infinity if any token has probability 0.\n\n        Args:\n            tokens: List of tokens to evaluate\n\n        Returns:\n            Perplexity (float, &gt;= 1, possibly inf)\n        \"\"\"\n        padded = ['&lt;START&gt;'] * self.order + tokens + ['&lt;END&gt;']\n\n        log_prob_sum = 0.0\n        n_tokens = 0\n\n        for i in range(len(padded) - self.order):\n            context = tuple(padded[i : i + self.order])\n            next_token = padded[i + self.order]\n\n            prob = self.probability(context, next_token)\n\n            if prob == 0:\n                # Model assigns 0 probability \u2192 perplexity is infinite\n                return float('inf')\n\n            log_prob_sum += math.log(prob)\n            n_tokens += 1\n\n        # Average negative log-likelihood\n        avg_neg_log_prob = -log_prob_sum / n_tokens\n\n        # Exponentiate to get perplexity\n        return math.exp(avg_neg_log_prob)\n</code></pre>"},{"location":"stages/stage-01/07-implementation/#part-7-utility-methods","title":"Part 7: Utility Methods","text":"<pre><code>    def num_states(self) -&gt; int:\n        \"\"\"Return number of unique contexts seen during training.\"\"\"\n        return len(self.counts)\n\n    def __repr__(self) -&gt; str:\n        \"\"\"String representation for debugging.\"\"\"\n        return f\"MarkovChain(order={self.order}, states={self.num_states()}, vocab={len(self.vocab)})\"\n</code></pre>"},{"location":"stages/stage-01/07-implementation/#usage-example","title":"Usage Example","text":"<pre><code># Training data\ntext = \"\"\"\nTo be, or not to be, that is the question:\nWhether 'tis nobler in the mind to suffer\nThe slings and arrows of outrageous fortune,\nOr to take arms against a sea of troubles\n\"\"\"\n\n# Tokenize to characters\ntokens = list(text.lower())\n\n# Create and train model\nmodel = MarkovChain(order=3)\nmodel.train(tokens)\n\nprint(f\"Model: {model}\")\n# Model: MarkovChain(order=3, states=245, vocab=31)\n\n# Generate text\nsample = model.generate(max_length=100, temperature=0.8)\nprint(f\"Sample: {sample}\")\n# Sample: to be, or not to ber the slings and ar...\n\n# Evaluate\ntrain_ppl = model.perplexity(tokens)\nprint(f\"Train perplexity: {train_ppl:.2f}\")\n# Train perplexity: 3.42\n</code></pre>"},{"location":"stages/stage-01/07-implementation/#time-and-space-complexity","title":"Time and Space Complexity","text":"<p>Training: - Time: O(n) where n = length of training data - Space: O(|V|^k) worst case, but typically O(n) in practice (sparse)</p> <p>Probability query: - Time: O(1) average (hash table lookup)</p> <p>Generation: - Time: O(L \u00d7 |V|) where L = output length, |V| = vocabulary size - The |V| factor is for sampling (iterating over distribution)</p> <p>Perplexity: - Time: O(n) where n = evaluation sequence length</p>"},{"location":"stages/stage-01/07-implementation/#summary","title":"Summary","text":"<p>We've implemented a complete Markov chain language model with: - Training via counting (MLE) - Probability queries - Temperature-controlled sampling - Perplexity evaluation</p> <p>The entire implementation is ~150 lines of well-documented Python with no external dependencies beyond the standard library.</p> <p>Key implementation insights: 1. Use <code>defaultdict(Counter)</code> for sparse count storage 2. Cache totals for O(1) probability queries 3. Work in log-space for numerical stability 4. Use tuples for contexts (hashable keys)</p> <p>Next: Let's analyze the fundamental trade-offs of this approach.</p>"},{"location":"stages/stage-01/08-trade-offs/","title":"Section 1.8: The Fundamental Trade-offs","text":"<p>We've built a complete language model. Now let's understand its fundamental limitations and why we need something better.</p>"},{"location":"stages/stage-01/08-trade-offs/#the-context-sparsity-trade-off","title":"The Context-Sparsity Trade-off","text":"<p>This is the central tension in Markov models:</p> <p>More context = better predictions - A bigram model sees \"the\" and might predict \"cat\", \"dog\", \"end\"... - A 5-gram model seeing \"the cat sat on\" can confidently predict \"the\"</p> <p>More context = sparser observations - Every possible 5-gram needs to be observed in training - Most valid 5-grams never appear in any finite corpus</p>"},{"location":"stages/stage-01/08-trade-offs/#quantifying-the-trade-off","title":"Quantifying the Trade-off","text":"<p>For character-level modeling with |V| = 27 (letters + space):</p> Order Possible contexts Typical observed Coverage 1 27 27 100% 2 729 ~400 55% 3 19,683 ~5,000 25% 5 14.3M ~50,000 0.3% 7 10.5B ~100,000 0.001% <p>At order 5, we've seen less than 1% of possible contexts!</p>"},{"location":"stages/stage-01/08-trade-offs/#the-experimental-evidence","title":"The Experimental Evidence","text":"<p>Here's what happens with a typical corpus (100,000 characters of English text):</p> Order Train PPL Test PPL States Unseen rate 1 12.4 12.5 27 0% 2 7.2 7.8 420 2% 3 4.1 6.3 4,800 15% 5 1.8 \u221e 48,000 62% 7 1.1 \u221e 95,000 89% <p>The pattern: 1. Train perplexity keeps improving (memorization) 2. Test perplexity improves, then explodes (overfitting) 3. Eventually, test perplexity becomes infinite (unseen n-grams)</p>"},{"location":"stages/stage-01/08-trade-offs/#why-this-is-fundamental","title":"Why This Is Fundamental","text":"<p>This isn't a bug in our implementation\u2014it's a fundamental limitation of the approach.</p> <p>The problem: Markov models require exact pattern matching.</p> <ul> <li>Training: \"the cat sat\"</li> <li>Test: \"the cat lay\"</li> </ul> <p>Even though \"sat\" and \"lay\" are syntactically similar (both verbs following \"cat\"), the trigram model treats them as completely unrelated. It has no notion of similarity.</p> <p>What we need: A way to generalize from seen patterns to unseen but similar patterns.</p> <p>This is exactly what neural networks provide.</p>"},{"location":"stages/stage-01/08-trade-offs/#state-space-explosion","title":"State Space Explosion","text":"<p>The number of states grows exponentially with order:</p> \\[\\text{States} = |V|^k\\] <p>For word-level models with |V| = 50,000: | Order | States | Storage (4 bytes each) | |-------|--------|----------------------| | 1 | 50K | 200 KB | | 2 | 2.5B | 10 GB | | 3 | 125T | 500 TB |</p> <p>Even storing just the non-zero entries, we run into the sparsity problem.</p>"},{"location":"stages/stage-01/08-trade-offs/#long-range-dependencies","title":"Long-Range Dependencies","text":"<p>Language has structure that spans far beyond what Markov models can capture:</p> <p>Example 1: Subject-verb agreement (distance: 7 words) \"The cats that sat on the mat were sleeping.\"</p> <p>A bigram model sees \"mat were\"\u2014grammatical but not because of local patterns.</p> <p>Example 2: Coreference (distance: variable) \"John walked into the room. He looked around. His eyes...\"</p> <p>\"He\" and \"His\" must agree with \"John\" from sentences ago.</p> <p>Example 3: Topic coherence (distance: paragraph) An article about physics should maintain physics vocabulary throughout.</p> <p>Markov models are blind to all of this.</p>"},{"location":"stages/stage-01/08-trade-offs/#what-we-need-the-preview","title":"What We Need: The Preview","text":"<p>To fix these limitations, we need:</p> Limitation Solution Stage No generalization Learned representations (embeddings) 4 No gradient-based learning Automatic differentiation 2-3 No flexible context Attention mechanism 7 Fixed context size Transformer architecture 8 <p>The key insight: Instead of storing explicit counts for every pattern, we learn a function that maps contexts to predictions. This function can generalize to unseen contexts.</p>"},{"location":"stages/stage-01/08-trade-offs/#what-carries-forward","title":"What Carries Forward","text":"<p>Despite the limitations, Markov models introduced concepts that underpin all modern LLMs:</p> Concept First Introduced Where It Appears Later Autoregressive factorization Chain rule \u2192 Markov GPT, LLaMA, Claude Next-token prediction MLE objective All modern LLMs Perplexity Evaluation metric Standard LLM benchmark Temperature sampling Generation control ChatGPT, Claude, etc. Cross-entropy loss MLE \u2261 min cross-entropy All neural LM training <p>These aren't simplified versions\u2014they're the same concepts.</p> <p>GPT-4 uses the same autoregressive factorization, the same cross-entropy objective, and the same temperature-controlled sampling. The only difference is how P(next | context) is computed.</p>"},{"location":"stages/stage-01/08-trade-offs/#the-bias-variance-perspective","title":"The Bias-Variance Perspective","text":"<p>From statistical learning theory:</p> <p>Bias: Error from the model's assumptions - High-order Markov: Low bias (few assumptions) - Low-order Markov: High bias (strong assumptions about independence)</p> <p>Variance: Error from sensitivity to training data - High-order Markov: High variance (sensitive to exact patterns) - Low-order Markov: Low variance (stable estimates)</p> <p>The trade-off: We can't minimize both simultaneously.</p> <p>Neural networks navigate this differently: - Flexible function class (low bias) - Regularization controls variance - Learning algorithm finds good solutions</p>"},{"location":"stages/stage-01/08-trade-offs/#exercises","title":"Exercises","text":"<ol> <li> <p>Smoothing: Implement Laplace smoothing and show it prevents infinite perplexity.</p> </li> <li> <p>Order sweep: Plot train and test perplexity vs. order for a corpus of your choice. Find the \"elbow\" where test perplexity starts increasing.</p> </li> <li> <p>Vocabulary comparison: Compare character-level and word-level models on the same corpus. How do optimal orders differ?</p> </li> <li> <p>State space visualization: For a trigram model, visualize the transition graph. Which states have the most outgoing edges?</p> </li> <li> <p>Temperature analysis: Generate 100 samples at T=0.5, 1.0, and 2.0. Compute the average perplexity of each set. What do you observe?</p> </li> </ol>"},{"location":"stages/stage-01/08-trade-offs/#summary","title":"Summary","text":"<p>What we built: - A complete language model from first principles - Training, evaluation, and generation - Full mathematical derivations for everything</p> <p>What we learned: - Probability theory foundations - Chain rule \u2192 autoregressive factorization - MLE \u2192 counting is optimal - Information theory \u2192 cross-entropy \u2192 perplexity - Temperature \u2192 controlled sampling</p> <p>Why it's limited: - Context-sparsity trade-off is fundamental - No generalization to similar-but-unseen patterns - Exponential state space explosion - Can't capture long-range dependencies</p> <p>What's next: To build models that generalize, we need to learn representations. That requires gradients. And computing gradients efficiently requires automatic differentiation.</p> <p>\u2192 Stage 2: Automatic Differentiation</p>"},{"location":"stages/stage-01/08-trade-offs/#reflection-the-polya-method","title":"Reflection: The P\u00f3lya Method","text":"<p>Looking back at this stage through P\u00f3lya's problem-solving framework:</p> <p>1. Understand the problem - We want P(next token | context) - The space of sequences is exponentially large - We need both tractability and accuracy</p> <p>2. Devise a plan - Use chain rule to factorize - Make Markov assumption to limit context - Estimate probabilities by counting (MLE)</p> <p>3. Execute the plan - Implemented counting-based training - Added temperature-controlled sampling - Evaluated with perplexity</p> <p>4. Reflect - The approach works but has fundamental limits - Context-sparsity trade-off can't be avoided - Need a different approach (neural networks) for better generalization</p> <p>This reflection pattern\u2014understand, plan, execute, reflect\u2014will continue throughout the book. Each stage builds on the previous, and each reflection motivates the next stage.</p>"},{"location":"stages/stage-02/","title":"Stage 2: Automatic Differentiation","text":""},{"location":"stages/stage-02/#from-calculus-to-code-building-the-foundation-of-deep-learning","title":"From Calculus to Code: Building the Foundation of Deep Learning","text":"<p>In Stage 1, we built a Markov chain language model and found optimal parameters through counting\u2014a closed-form solution. Neural networks are fundamentally different: there's no closed-form solution. We must search for good parameters by iteratively improving them.</p> <p>This search requires knowing: if I change a parameter slightly, how does the output change?</p> <p>This is the domain of automatic differentiation\u2014the technique that makes training neural networks possible. By the end of this stage, you'll understand exactly how PyTorch and TensorFlow compute gradients, because you'll have built the same system from scratch.</p>"},{"location":"stages/stage-02/#what-well-build","title":"What We'll Build","text":"<p>A complete automatic differentiation engine that can:</p> <ul> <li>Track computations as they happen</li> <li>Build computational graphs automatically</li> <li>Compute gradients via reverse-mode differentiation</li> <li>Train neural networks using gradient descent</li> </ul>"},{"location":"stages/stage-02/#sections","title":"Sections","text":""},{"location":"stages/stage-02/#21-what-is-a-derivative","title":"2.1: What is a Derivative?","text":"<p>The geometric and algebraic foundations. Why derivatives matter for optimization, and how they connect to machine learning.</p>"},{"location":"stages/stage-02/#22-derivative-rules-from-first-principles","title":"2.2: Derivative Rules from First Principles","text":"<p>Deriving the power, product, quotient, and exponential rules from the limit definition. We don't just state rules\u2014we prove them.</p>"},{"location":"stages/stage-02/#23-the-chain-rule-the-heart-of-backpropagation","title":"2.3: The Chain Rule \u2014 The Heart of Backpropagation","text":"<p>The most important derivative rule for deep learning. How derivatives chain through compositions, and why this leads directly to backpropagation.</p>"},{"location":"stages/stage-02/#24-computational-graphs","title":"2.4: Computational Graphs","text":"<p>Representing computation as directed acyclic graphs. Forward passes, backward passes, and gradient accumulation.</p>"},{"location":"stages/stage-02/#25-forward-mode-vs-reverse-mode","title":"2.5: Forward Mode vs Reverse Mode","text":"<p>Two fundamentally different ways to apply the chain rule. Why reverse mode is exponentially faster for neural networks.</p>"},{"location":"stages/stage-02/#26-building-autograd-from-scratch","title":"2.6: Building Autograd from Scratch","text":"<p>~100 lines of code that implement complete automatic differentiation. Building and training neural networks with our own engine.</p>"},{"location":"stages/stage-02/#27-testing-and-validation","title":"2.7: Testing and Validation","text":"<p>How to verify your gradients are correct. Numerical checking, property-based testing, and debugging strategies.</p>"},{"location":"stages/stage-02/#prerequisites","title":"Prerequisites","text":"<ul> <li>Basic calculus (we'll derive everything from limits)</li> <li>Python programming</li> <li>Completion of Stage 1 (for context on why we need this)</li> </ul>"},{"location":"stages/stage-02/#key-takeaways","title":"Key Takeaways","text":"<p>By the end of this stage, you will understand:</p> <ol> <li>Derivatives from first principles: Not just rules to memorize, but why they work</li> <li>The chain rule deeply: How it enables differentiating any composition</li> <li>Computational graphs: The data structure behind modern deep learning</li> <li>Why reverse mode wins: The complexity analysis that explains backpropagation's efficiency</li> <li>How to build autograd: The ~100 lines of core code that power gradient computation</li> <li>Testing gradients: Essential techniques for verifying correctness</li> </ol>"},{"location":"stages/stage-02/#the-journey-so-far","title":"The Journey So Far","text":"Stage Topic Key Insight 1 Markov Chains Language modeling is probability estimation over sequences 2 Automatic Differentiation Gradients enable iterative optimization\u2014no closed-form needed 3 (Coming) Building our first neural language model"},{"location":"stages/stage-02/#lets-begin","title":"Let's Begin","text":"<p>The derivative is where it all starts. Understanding it deeply\u2014not just as a formula, but as a concept\u2014unlocks everything that follows.</p> <p>\u2192 Start with Section 2.1: What is a Derivative?</p>"},{"location":"stages/stage-02/01-what-is-derivative/","title":"Section 2.1: What is a Derivative?","text":"<p>In Stage 1, we found the optimal parameters for a Markov model by counting. The math worked out to a closed-form solution\u2014no iteration required.</p> <p>Neural networks are different. There's no closed-form solution. We need to search for good parameters by iteratively improving them. This requires knowing: if I change a parameter slightly, how does the output change?</p> <p>This is exactly what derivatives measure.</p>"},{"location":"stages/stage-02/01-what-is-derivative/#the-geometric-picture","title":"The Geometric Picture","text":"<p>Consider a function f(x). At any point x, we can ask: what's the slope of f at that point?</p> <p>For a straight line f(x) = mx + b, the slope is constant: it's m everywhere.</p> <p>For a curve like f(x) = x\u00b2, the slope varies. At x=0, the curve is flat (slope 0). At x=2, it's steeper (slope 4). At x=-1, it slopes downward (slope -2).</p> <p>The derivative f'(x) gives us the slope at each point x.</p>"},{"location":"stages/stage-02/01-what-is-derivative/#tangent-lines","title":"Tangent Lines","text":"<p>Geometrically, f'(x) is the slope of the tangent line to f at x.</p> <p>The tangent line is the straight line that: 1. Touches the curve at point (x, f(x)) 2. Has the same \"direction\" as the curve at that point</p> <p>If you zoom in far enough on any smooth curve, it looks like a straight line. That straight line is the tangent.</p>"},{"location":"stages/stage-02/01-what-is-derivative/#the-algebraic-definition","title":"The Algebraic Definition","text":"<p>How do we compute the slope at a single point? We can't directly\u2014a single point has no \"rise over run.\"</p> <p>The key insight: Approximate the slope using two nearby points, then take a limit.</p>"},{"location":"stages/stage-02/01-what-is-derivative/#the-difference-quotient","title":"The Difference Quotient","text":"<p>For two points x and x+h on the curve: - Rise: f(x+h) - f(x) - Run: h</p> <p>The difference quotient is:</p> \\[\\frac{f(x+h) - f(x)}{h}\\] <p>This gives the slope of the secant line connecting the two points.</p>"},{"location":"stages/stage-02/01-what-is-derivative/#taking-the-limit","title":"Taking the Limit","text":"<p>As h \u2192 0, the two points get closer together, and the secant line approaches the tangent line.</p> <p>Definition (Derivative): The derivative of f at x is:</p> \\[f'(x) = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}\\] <p>if this limit exists.</p> <p>Alternative notation: - f'(x) \u2014 Lagrange notation - df/dx \u2014 Leibniz notation (suggests \"infinitesimal change in f per infinitesimal change in x\") - Df(x) \u2014 Operator notation</p>"},{"location":"stages/stage-02/01-what-is-derivative/#computing-a-derivative-from-definition","title":"Computing a Derivative from Definition","text":"<p>Let's derive the derivative of f(x) = x\u00b2 from first principles.</p> <p>Step 1: Write the difference quotient. $\\(\\frac{f(x+h) - f(x)}{h} = \\frac{(x+h)^2 - x^2}{h}\\)$</p> <p>Step 2: Expand the numerator. $\\((x+h)^2 = x^2 + 2xh + h^2\\)$ $\\(\\frac{x^2 + 2xh + h^2 - x^2}{h} = \\frac{2xh + h^2}{h}\\)$</p> <p>Step 3: Simplify. $\\(\\frac{2xh + h^2}{h} = \\frac{h(2x + h)}{h} = 2x + h\\)$</p> <p>Step 4: Take the limit as h \u2192 0. $\\(\\lim_{h \\to 0} (2x + h) = 2x\\)$</p> <p>Result: f'(x) = 2x</p> <p>This means: - At x=0: slope = 0 (the parabola is flat at the bottom) - At x=1: slope = 2 (rising) - At x=3: slope = 6 (rising steeply) - At x=-2: slope = -4 (falling)</p>"},{"location":"stages/stage-02/01-what-is-derivative/#another-example-fx-1x","title":"Another Example: f(x) = 1/x","text":"<p>Let's derive the derivative of f(x) = 1/x for x \u2260 0.</p> <p>Step 1: Difference quotient. $\\(\\frac{f(x+h) - f(x)}{h} = \\frac{\\frac{1}{x+h} - \\frac{1}{x}}{h}\\)$</p> <p>Step 2: Find common denominator for the numerator. $\\(\\frac{1}{x+h} - \\frac{1}{x} = \\frac{x - (x+h)}{x(x+h)} = \\frac{-h}{x(x+h)}\\)$</p> <p>Step 3: Divide by h. $\\(\\frac{-h}{h \\cdot x(x+h)} = \\frac{-1}{x(x+h)}\\)$</p> <p>Step 4: Take limit. $\\(\\lim_{h \\to 0} \\frac{-1}{x(x+h)} = \\frac{-1}{x \\cdot x} = -\\frac{1}{x^2}\\)$</p> <p>Result: If f(x) = 1/x, then f'(x) = -1/x\u00b2</p>"},{"location":"stages/stage-02/01-what-is-derivative/#why-derivatives-matter-for-optimization","title":"Why Derivatives Matter for Optimization","text":"<p>Suppose we want to find the minimum of a function f(x). The derivative tells us which way is \"downhill\":</p> <ul> <li>If f'(x) &gt; 0: f is increasing at x. Move left to decrease f.</li> <li>If f'(x) &lt; 0: f is decreasing at x. Move right to decrease f.</li> <li>If f'(x) = 0: x might be a minimum (or maximum, or saddle point).</li> </ul> <p>This is the foundation of gradient descent: 1. Start at some x 2. Compute f'(x) 3. Update: x \u2190 x - \u03b1\u00b7f'(x) (where \u03b1 is a small step size) 4. Repeat until f'(x) \u2248 0</p> <p>The negative sign makes us move opposite to the gradient\u2014toward lower values of f.</p>"},{"location":"stages/stage-02/01-what-is-derivative/#connection-to-machine-learning","title":"Connection to Machine Learning","text":"<p>In machine learning: - f is the loss function (measures how bad our predictions are) - x is all the parameters (weights and biases) - We want to minimize f (make predictions better)</p> <p>The derivative tells us how to adjust each parameter to reduce the loss.</p>"},{"location":"stages/stage-02/01-what-is-derivative/#functions-of-multiple-variables","title":"Functions of Multiple Variables","text":"<p>Real neural networks have millions of parameters. We need derivatives with respect to each one.</p> <p>Partial Derivative: For f(x, y), the partial derivative with respect to x is:</p> \\[\\frac{\\partial f}{\\partial x} = \\lim_{h \\to 0} \\frac{f(x+h, y) - f(x, y)}{h}\\] <p>We treat y as a constant and differentiate only with respect to x.</p> <p>Example: f(x, y) = x\u00b2y + y\u00b3</p> \\[\\frac{\\partial f}{\\partial x} = 2xy\\] \\[\\frac{\\partial f}{\\partial y} = x^2 + 3y^2\\] <p>The Gradient: The gradient \u2207f collects all partial derivatives:</p> \\[\\nabla f = \\left( \\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y} \\right)\\] <p>For f with n variables: \u2207f is an n-dimensional vector.</p> <p>The gradient points in the direction of steepest ascent. To minimize, move opposite to the gradient.</p>"},{"location":"stages/stage-02/01-what-is-derivative/#what-comes-next","title":"What Comes Next","text":"<p>We now know what a derivative is. But computing derivatives by hand is tedious and error-prone.</p> <p>In Section 2.2, we'll derive the fundamental rules (power, product, chain) that let us compute derivatives of complex expressions systematically.</p> <p>In Section 2.3, we'll focus on the chain rule\u2014the key to computing derivatives of composed functions, which is exactly what neural networks are.</p>"},{"location":"stages/stage-02/01-what-is-derivative/#exercises","title":"Exercises","text":"<ol> <li> <p>Derive from definition: Find f'(x) for f(x) = x\u00b3 using the limit definition.</p> </li> <li> <p>Verify geometrically: Plot f(x) = x\u00b2 and draw tangent lines at x = -1, 0, and 2. Verify the slopes match 2x.</p> </li> <li> <p>Partial derivatives: For f(x,y) = sin(x)\u00b7cos(y), find \u2202f/\u2202x and \u2202f/\u2202y.</p> </li> <li> <p>Gradient descent by hand: Starting at x = 5, apply 5 steps of gradient descent to minimize f(x) = x\u00b2 with step size \u03b1 = 0.1. What value do you approach?</p> </li> <li> <p>Thinking question: The derivative is undefined when the limit doesn't exist. Give an example of a function that has no derivative at a particular point, and explain why geometrically.</p> </li> </ol>"},{"location":"stages/stage-02/01-what-is-derivative/#summary","title":"Summary","text":"Concept Definition Intuition Derivative lim_{h\u21920} [f(x+h)-f(x)]/h Instantaneous rate of change Geometric meaning Slope of tangent line How steep is the curve? Partial derivative Derivative with one variable Hold others constant Gradient Vector of all partials Direction of steepest ascent Gradient descent x \u2190 x - \u03b1\u00b7\u2207f Walk downhill to minimize <p>Key takeaway: Derivatives tell us how outputs change when inputs change. For optimization, this tells us which direction to move to improve.</p>"},{"location":"stages/stage-02/02-derivative-rules/","title":"Section 2.2: Derivative Rules from First Principles","text":"<p>Computing derivatives from the limit definition every time would be exhausting. Fortunately, a few key rules let us differentiate almost any function quickly.</p> <p>But we won't just state these rules\u2014we'll derive them from first principles. This ensures we understand not just what the rules are, but why they work.</p>"},{"location":"stages/stage-02/02-derivative-rules/#constants-and-linear-functions","title":"Constants and Linear Functions","text":""},{"location":"stages/stage-02/02-derivative-rules/#constant-rule","title":"Constant Rule","text":"<p>If f(x) = c (a constant), what is f'(x)?</p> \\[f'(x) = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h} = \\lim_{h \\to 0} \\frac{c - c}{h} = \\lim_{h \\to 0} \\frac{0}{h} = 0\\] <p>Result: The derivative of a constant is zero.</p> <p>This makes sense: a horizontal line has slope 0 everywhere.</p>"},{"location":"stages/stage-02/02-derivative-rules/#constant-multiple-rule","title":"Constant Multiple Rule","text":"<p>If g(x) = c\u00b7f(x), what is g'(x)?</p> \\[g'(x) = \\lim_{h \\to 0} \\frac{c \\cdot f(x+h) - c \\cdot f(x)}{h} = \\lim_{h \\to 0} c \\cdot \\frac{f(x+h) - f(x)}{h} = c \\cdot f'(x)\\] <p>Result: (c\u00b7f)' = c\u00b7f'</p> <p>Constants factor out of derivatives.</p>"},{"location":"stages/stage-02/02-derivative-rules/#the-sum-rule","title":"The Sum Rule","text":"<p>If h(x) = f(x) + g(x), what is h'(x)?</p> \\[h'(x) = \\lim_{k \\to 0} \\frac{[f(x+k) + g(x+k)] - [f(x) + g(x)]}{k}\\] <p>Rearranging: $\\(= \\lim_{k \\to 0} \\frac{[f(x+k) - f(x)] + [g(x+k) - g(x)]}{k}\\)$</p> \\[= \\lim_{k \\to 0} \\frac{f(x+k) - f(x)}{k} + \\lim_{k \\to 0} \\frac{g(x+k) - g(x)}{k}\\] \\[= f'(x) + g'(x)\\] <p>Result: (f + g)' = f' + g'</p> <p>The derivative of a sum is the sum of derivatives. This is called linearity\u2014differentiation is a linear operation.</p>"},{"location":"stages/stage-02/02-derivative-rules/#the-power-rule","title":"The Power Rule","text":"<p>The power rule states: if f(x) = x^n, then f'(x) = n\u00b7x^{n-1}.</p> <p>Let's prove this for positive integers using the binomial theorem.</p>"},{"location":"stages/stage-02/02-derivative-rules/#proof-using-binomial-expansion","title":"Proof Using Binomial Expansion","text":"<p>For positive integer n:</p> \\[(x+h)^n = \\sum_{k=0}^{n} \\binom{n}{k} x^{n-k} h^k = x^n + nx^{n-1}h + \\frac{n(n-1)}{2}x^{n-2}h^2 + \\cdots + h^n\\] <p>The difference quotient: $\\(\\frac{(x+h)^n - x^n}{h} = \\frac{nx^{n-1}h + \\frac{n(n-1)}{2}x^{n-2}h^2 + \\cdots + h^n}{h}\\)$</p> \\[= nx^{n-1} + \\frac{n(n-1)}{2}x^{n-2}h + \\cdots + h^{n-1}\\] <p>Taking the limit as h \u2192 0: $\\(\\lim_{h \\to 0} \\left[ nx^{n-1} + \\frac{n(n-1)}{2}x^{n-2}h + \\cdots + h^{n-1} \\right] = nx^{n-1}\\)$</p> <p>All terms with h vanish, leaving only the first term.</p> <p>Result: d/dx(x^n) = n\u00b7x^{n-1}</p>"},{"location":"stages/stage-02/02-derivative-rules/#examples","title":"Examples","text":"Function Derivative x\u00b9 1\u00b7x\u2070 = 1 x\u00b2 2x x\u00b3 3x\u00b2 x^{10} 10x\u2079"},{"location":"stages/stage-02/02-derivative-rules/#extension-to-negative-and-fractional-powers","title":"Extension to Negative and Fractional Powers","text":"<p>The power rule also works for negative and fractional exponents. Let's verify for n = -1:</p> <p>We already proved: d/dx(1/x) = -1/x\u00b2</p> <p>Using the power rule: d/dx(x^{-1}) = -1\u00b7x^{-2} = -1/x\u00b2 \u2713</p> <p>For n = 1/2 (square root): $\\(\\frac{d}{dx}\\sqrt{x} = \\frac{d}{dx}x^{1/2} = \\frac{1}{2}x^{-1/2} = \\frac{1}{2\\sqrt{x}}\\)$</p> <p>This can be verified from the definition (more involved).</p>"},{"location":"stages/stage-02/02-derivative-rules/#the-product-rule","title":"The Product Rule","text":"<p>Now things get interesting. If h(x) = f(x)\u00b7g(x), is h'(x) = f'(x)\u00b7g'(x)?</p> <p>No! Let's see what actually happens.</p>"},{"location":"stages/stage-02/02-derivative-rules/#derivation","title":"Derivation","text":"\\[h'(x) = \\lim_{k \\to 0} \\frac{f(x+k)g(x+k) - f(x)g(x)}{k}\\] <p>The trick is to add and subtract a \"bridge\" term: $\\(= \\lim_{k \\to 0} \\frac{f(x+k)g(x+k) - f(x+k)g(x) + f(x+k)g(x) - f(x)g(x)}{k}\\)$</p> <p>Grouping: $\\(= \\lim_{k \\to 0} \\frac{f(x+k)[g(x+k) - g(x)] + g(x)[f(x+k) - f(x)]}{k}\\)$</p> \\[= \\lim_{k \\to 0} f(x+k) \\cdot \\frac{g(x+k) - g(x)}{k} + g(x) \\cdot \\lim_{k \\to 0} \\frac{f(x+k) - f(x)}{k}\\] <p>Since f is continuous (differentiable functions are continuous): $\\(\\lim_{k \\to 0} f(x+k) = f(x)\\)$</p> <p>Therefore: $\\(h'(x) = f(x) \\cdot g'(x) + g(x) \\cdot f'(x)\\)$</p> <p>Result: (f\u00b7g)' = f\u00b7g' + f'\u00b7g</p>"},{"location":"stages/stage-02/02-derivative-rules/#intuition","title":"Intuition","text":"<p>Why isn't the derivative of a product the product of derivatives?</p> <p>Think of a rectangle with sides f and g. Its area is f\u00b7g.</p> <p>If both sides grow: - The area grows by f\u00b7\u0394g (original f, additional g) - The area grows by g\u00b7\u0394f (original g, additional f) - There's also a tiny \u0394f\u00b7\u0394g corner (negligible as \u0394 \u2192 0)</p> <p>Total growth \u2248 f\u00b7\u0394g + g\u00b7\u0394f, which gives the product rule.</p>"},{"location":"stages/stage-02/02-derivative-rules/#example","title":"Example","text":"<p>Let h(x) = x\u00b2\u00b7sin(x). (We'll derive sin'(x) = cos(x) later.)</p> <p>Using the product rule: $\\(h'(x) = x^2 \\cdot \\cos(x) + \\sin(x) \\cdot 2x = x^2\\cos(x) + 2x\\sin(x)\\)$</p>"},{"location":"stages/stage-02/02-derivative-rules/#the-quotient-rule","title":"The Quotient Rule","text":"<p>If h(x) = f(x)/g(x), what is h'(x)?</p>"},{"location":"stages/stage-02/02-derivative-rules/#derivation_1","title":"Derivation","text":"<p>We can derive this from the product rule by writing f/g = f \u00b7 (1/g).</p> <p>First, we need d/dx(1/g). Let's use the limit definition:</p> \\[\\frac{d}{dx}\\left(\\frac{1}{g(x)}\\right) = \\lim_{k \\to 0} \\frac{\\frac{1}{g(x+k)} - \\frac{1}{g(x)}}{k}\\] \\[= \\lim_{k \\to 0} \\frac{g(x) - g(x+k)}{k \\cdot g(x) \\cdot g(x+k)}\\] \\[= \\lim_{k \\to 0} \\frac{-[g(x+k) - g(x)]}{k} \\cdot \\frac{1}{g(x) \\cdot g(x+k)}\\] \\[= -g'(x) \\cdot \\frac{1}{g(x)^2} = -\\frac{g'(x)}{g(x)^2}\\] <p>Now apply the product rule to h = f \u00b7 (1/g):</p> \\[h' = f \\cdot \\left(-\\frac{g'}{g^2}\\right) + \\frac{1}{g} \\cdot f' = \\frac{f'}{g} - \\frac{f \\cdot g'}{g^2}\\] \\[= \\frac{f' \\cdot g - f \\cdot g'}{g^2}\\] <p>Result: (f/g)' = (f'g - fg')/g\u00b2</p>"},{"location":"stages/stage-02/02-derivative-rules/#memory-aid","title":"Memory Aid","text":"<p>Some remember this as \"low d-high minus high d-low, over low squared\": $\\(\\frac{d}{dx}\\frac{f}{g} = \\frac{g \\cdot f' - f \\cdot g'}{g^2}\\)$</p>"},{"location":"stages/stage-02/02-derivative-rules/#the-exponential-function","title":"The Exponential Function","text":"<p>The exponential function e^x is special: it's its own derivative.</p>"},{"location":"stages/stage-02/02-derivative-rules/#what-is-e","title":"What is e?","text":"<p>The number e \u2248 2.71828... is defined as: $\\(e = \\lim_{n \\to \\infty} \\left(1 + \\frac{1}{n}\\right)^n\\)$</p> <p>Or equivalently, e is the unique number such that: $\\(\\lim_{h \\to 0} \\frac{e^h - 1}{h} = 1\\)$</p>"},{"location":"stages/stage-02/02-derivative-rules/#derivative-of-ex","title":"Derivative of e^x","text":"\\[\\frac{d}{dx}e^x = \\lim_{h \\to 0} \\frac{e^{x+h} - e^x}{h} = \\lim_{h \\to 0} \\frac{e^x \\cdot e^h - e^x}{h}\\] \\[= e^x \\cdot \\lim_{h \\to 0} \\frac{e^h - 1}{h} = e^x \\cdot 1 = e^x\\] <p>Result: d/dx(e^x) = e^x</p> <p>This is why e^x is so important in mathematics\u2014it's the unique function (up to scaling) that equals its own derivative.</p>"},{"location":"stages/stage-02/02-derivative-rules/#general-exponential","title":"General Exponential","text":"<p>For a^x where a &gt; 0:</p> <p>Using a^x = e^{x\u00b7ln(a)} and the chain rule (next section): $\\(\\frac{d}{dx}a^x = a^x \\cdot \\ln(a)\\)$</p>"},{"location":"stages/stage-02/02-derivative-rules/#the-natural-logarithm","title":"The Natural Logarithm","text":"<p>If y = ln(x), what is dy/dx?</p>"},{"location":"stages/stage-02/02-derivative-rules/#derivation-using-inverse-functions","title":"Derivation Using Inverse Functions","text":"<p>Since e^{ln(x)} = x, differentiate both sides: $\\(e^{\\ln(x)} \\cdot \\frac{d}{dx}\\ln(x) = 1\\)$</p> \\[x \\cdot \\frac{d}{dx}\\ln(x) = 1\\] \\[\\frac{d}{dx}\\ln(x) = \\frac{1}{x}\\] <p>Result: d/dx(ln x) = 1/x</p>"},{"location":"stages/stage-02/02-derivative-rules/#log-of-other-bases","title":"Log of Other Bases","text":"<p>For log_a(x) = ln(x)/ln(a): $\\(\\frac{d}{dx}\\log_a(x) = \\frac{1}{x \\cdot \\ln(a)}\\)$</p>"},{"location":"stages/stage-02/02-derivative-rules/#summary-of-rules","title":"Summary of Rules","text":"Rule Formula Derived From Constant (c)' = 0 Limit definition Power (x^n)' = nx^{n-1} Binomial theorem Sum (f+g)' = f'+g' Linearity of limits Product (fg)' = fg' + f'g Add-subtract trick Quotient (f/g)' = (f'g-fg')/g\u00b2 Product rule + reciprocal Exponential (e^x)' = e^x Definition of e Logarithm (ln x)' = 1/x Inverse function"},{"location":"stages/stage-02/02-derivative-rules/#whats-missing-the-chain-rule","title":"What's Missing: The Chain Rule","text":"<p>Notice we haven't handled compositions like sin(x\u00b2) or e^{-x\u00b2} or ln(1+x).</p> <p>These require the chain rule, which is so important it gets its own section. The chain rule is the heart of automatic differentiation.</p>"},{"location":"stages/stage-02/02-derivative-rules/#exercises","title":"Exercises","text":"<ol> <li> <p>Derive the power rule for n=3 by directly expanding (x+h)\u00b3 - x\u00b3.</p> </li> <li> <p>Product rule practice: Find d/dx(x\u00b3\u00b7e^x).</p> </li> <li> <p>Quotient rule practice: Find d/dx(x\u00b2/(1+x)).</p> </li> <li> <p>Why the product rule?: Give a geometric argument for why (fg)' \u2260 f'g'.</p> </li> <li> <p>Verify log derivative: Using the limit definition, show that d/dx(ln x) = 1/x by computing the limit directly. (Hint: use the substitution k = h/x and the definition of e.)</p> </li> </ol>"},{"location":"stages/stage-02/02-derivative-rules/#summary","title":"Summary","text":"<p>We derived all fundamental derivative rules from the limit definition:</p> <ul> <li>Constants disappear, sums split, constants factor out</li> <li>The power rule handles polynomials</li> <li>The product rule handles products (it's not just f'g')</li> <li>The quotient rule is the product rule for reciprocals</li> <li>e^x is its own derivative (remarkable!)</li> <li>ln(x) differentiates to 1/x</li> </ul> <p>With these rules, we can differentiate any polynomial, rational function, or expression involving exponentials and logarithms\u2014as long as there's no function composition.</p> <p>For compositions, we need the chain rule.</p>"},{"location":"stages/stage-02/03-chain-rule/","title":"Section 2.3: The Chain Rule \u2014 The Heart of Backpropagation","text":"<p>The chain rule is the single most important derivative rule for machine learning. Neural networks are compositions of functions, and the chain rule tells us how to differentiate compositions.</p> <p>This section is the mathematical foundation of backpropagation.</p>"},{"location":"stages/stage-02/03-chain-rule/#the-problem-nested-functions","title":"The Problem: Nested Functions","text":"<p>Consider h(x) = (x\u00b2 + 1)\u00b3.</p> <p>This isn't a simple polynomial or product. It's a composition: the cube function applied to (x\u00b2 + 1).</p> <p>If we write: - g(x) = x\u00b2 + 1 (inner function) - f(u) = u\u00b3 (outer function)</p> <p>Then h(x) = f(g(x)) = (g(x))\u00b3.</p> <p>How do we find h'(x)?</p>"},{"location":"stages/stage-02/03-chain-rule/#intuition-rates-of-change-multiply","title":"Intuition: Rates of Change Multiply","text":"<p>Suppose: - x changes by a small amount \u0394x - This causes g(x) to change by \u0394g - Which causes f(g(x)) to change by \u0394f</p> <p>The rate of change of f with respect to x is:</p> \\[\\frac{\\Delta f}{\\Delta x} = \\frac{\\Delta f}{\\Delta g} \\cdot \\frac{\\Delta g}{\\Delta x}\\] <p>The rates multiply!</p> <p>If g is 3 times as sensitive to x as f is to g, then f is 3 times as sensitive to x overall.</p>"},{"location":"stages/stage-02/03-chain-rule/#the-chain-rule-formal-statement","title":"The Chain Rule: Formal Statement","text":"<p>For h(x) = f(g(x)), if g is differentiable at x and f is differentiable at g(x):</p> \\[h'(x) = f'(g(x)) \\cdot g'(x)\\] <p>Or in Leibniz notation, if y = f(u) and u = g(x):</p> \\[\\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx}\\] <p>The derivatives \"chain\" together\u2014hence the name.</p>"},{"location":"stages/stage-02/03-chain-rule/#proof-of-the-chain-rule","title":"Proof of the Chain Rule","text":"<p>Setup: Let h(x) = f(g(x)). We want to show h'(x) = f'(g(x)) \u00b7 g'(x).</p> <p>Step 1: Write the difference quotient for h.</p> \\[h'(x) = \\lim_{k \\to 0} \\frac{f(g(x+k)) - f(g(x))}{k}\\] <p>Step 2: Let \u0394g = g(x+k) - g(x).</p> <p>As k \u2192 0, we have \u0394g \u2192 0 (since g is continuous).</p> <p>Step 3: Multiply and divide by \u0394g (when \u0394g \u2260 0).</p> \\[\\frac{f(g(x+k)) - f(g(x))}{k} = \\frac{f(g(x) + \\Delta g) - f(g(x))}{\\Delta g} \\cdot \\frac{\\Delta g}{k}\\] \\[= \\frac{f(g(x) + \\Delta g) - f(g(x))}{\\Delta g} \\cdot \\frac{g(x+k) - g(x)}{k}\\] <p>Step 4: Take limits.</p> <p>As k \u2192 0: - The first factor \u2192 f'(g(x)) (definition of derivative of f at g(x)) - The second factor \u2192 g'(x) (definition of derivative of g at x)</p> <p>Therefore: $\\(h'(x) = f'(g(x)) \\cdot g'(x) \\quad \\blacksquare\\)$</p> <p>Technical note: The proof needs care when \u0394g = 0 for some k \u2260 0. A rigorous proof handles this with a modified definition. The intuition above captures the essence.</p>"},{"location":"stages/stage-02/03-chain-rule/#example-hx-x2-13","title":"Example: h(x) = (x\u00b2 + 1)\u00b3","text":"<p>Identify the parts: - Inner: g(x) = x\u00b2 + 1, so g'(x) = 2x - Outer: f(u) = u\u00b3, so f'(u) = 3u\u00b2</p> <p>Apply chain rule: $\\(h'(x) = f'(g(x)) \\cdot g'(x) = 3(x^2 + 1)^2 \\cdot 2x = 6x(x^2 + 1)^2\\)$</p> <p>Verification: Let's expand and differentiate directly (painful but correct).</p> <p>\\((x^2 + 1)^3 = x^6 + 3x^4 + 3x^2 + 1\\)</p> <p>\\(\\frac{d}{dx}(x^6 + 3x^4 + 3x^2 + 1) = 6x^5 + 12x^3 + 6x = 6x(x^4 + 2x^2 + 1) = 6x(x^2 + 1)^2\\) \u2713</p>"},{"location":"stages/stage-02/03-chain-rule/#more-examples","title":"More Examples","text":""},{"location":"stages/stage-02/03-chain-rule/#example-1-e-x2","title":"Example 1: e^{-x\u00b2}","text":"<p>This is exp(u) where u = -x\u00b2.</p> <ul> <li>u = -x\u00b2, so du/dx = -2x</li> <li>y = e^u, so dy/du = e^u</li> </ul> \\[\\frac{dy}{dx} = e^{-x^2} \\cdot (-2x) = -2x e^{-x^2}\\]"},{"location":"stages/stage-02/03-chain-rule/#example-2-sin3x-2","title":"Example 2: sin(3x + 2)","text":"<p>This is sin(u) where u = 3x + 2.</p> <ul> <li>du/dx = 3</li> <li>d(sin u)/du = cos(u)</li> </ul> \\[\\frac{d}{dx}\\sin(3x+2) = \\cos(3x+2) \\cdot 3 = 3\\cos(3x+2)\\]"},{"location":"stages/stage-02/03-chain-rule/#example-3-lnx2-1","title":"Example 3: ln(x\u00b2 + 1)","text":"<p>This is ln(u) where u = x\u00b2 + 1.</p> <ul> <li>du/dx = 2x</li> <li>d(ln u)/du = 1/u</li> </ul> \\[\\frac{d}{dx}\\ln(x^2+1) = \\frac{1}{x^2+1} \\cdot 2x = \\frac{2x}{x^2+1}\\]"},{"location":"stages/stage-02/03-chain-rule/#example-4-triple-composition","title":"Example 4: Triple Composition","text":"<p>Let h(x) = sin(e^{x\u00b2}).</p> <p>Break it down: - Innermost: a = x\u00b2, so da/dx = 2x - Middle: b = e^a, so db/da = e^a - Outer: y = sin(b), so dy/db = cos(b)</p> <p>Chain them all: $\\(\\frac{dy}{dx} = \\frac{dy}{db} \\cdot \\frac{db}{da} \\cdot \\frac{da}{dx} = \\cos(e^{x^2}) \\cdot e^{x^2} \\cdot 2x\\)$</p>"},{"location":"stages/stage-02/03-chain-rule/#the-chain-rule-for-multiple-variables","title":"The Chain Rule for Multiple Variables","text":"<p>In machine learning, we typically have functions of many variables. The chain rule generalizes.</p>"},{"location":"stages/stage-02/03-chain-rule/#scalar-case","title":"Scalar Case","text":"<p>If z = f(y) and y = g(x\u2081, x\u2082, ..., x\u2099):</p> \\[\\frac{\\partial z}{\\partial x_i} = \\frac{dz}{dy} \\cdot \\frac{\\partial y}{\\partial x_i}\\]"},{"location":"stages/stage-02/03-chain-rule/#general-case-multiple-paths","title":"General Case: Multiple Paths","text":"<p>If z depends on y\u2081 and y\u2082, which both depend on x:</p> \\[\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial y_1} \\cdot \\frac{\\partial y_1}{\\partial x} + \\frac{\\partial z}{\\partial y_2} \\cdot \\frac{\\partial y_2}{\\partial x}\\] <p>We sum over all paths from z to x.</p>"},{"location":"stages/stage-02/03-chain-rule/#example-multivariate-chain-rule","title":"Example: Multivariate Chain Rule","text":"<p>Let z = x\u00b7y where x = s\u00b2 and y = s\u00b3.</p> <p>By the chain rule: $\\(\\frac{dz}{ds} = \\frac{\\partial z}{\\partial x}\\frac{dx}{ds} + \\frac{\\partial z}{\\partial y}\\frac{dy}{ds}\\)$</p> \\[= y \\cdot 2s + x \\cdot 3s^2 = s^3 \\cdot 2s + s^2 \\cdot 3s^2 = 2s^4 + 3s^4 = 5s^4\\] <p>Verification: z = x\u00b7y = s\u00b2\u00b7s\u00b3 = s\u2075, so dz/ds = 5s\u2074 \u2713</p>"},{"location":"stages/stage-02/03-chain-rule/#why-this-matters-for-neural-networks","title":"Why This Matters for Neural Networks","text":"<p>A neural network is a composition of layers:</p> \\[\\text{output} = f_L(f_{L-1}(\\cdots f_2(f_1(x))\\cdots))\\] <p>Each layer f_i transforms its input, and the chain rule tells us:</p> \\[\\frac{\\partial \\text{loss}}{\\partial \\text{parameters of layer } i} = \\frac{\\partial \\text{loss}}{\\partial \\text{output of layer } i} \\cdot \\frac{\\partial \\text{output of layer } i}{\\partial \\text{parameters of layer } i}\\] <p>The \"gradient of loss with respect to output\" flows backward through the network, getting multiplied by local gradients at each layer.</p> <p>This is backpropagation\u2014it's just the chain rule applied systematically from output to input.</p>"},{"location":"stages/stage-02/03-chain-rule/#the-chain-rule-as-a-graph","title":"The Chain Rule as a Graph","text":"<p>We can visualize the chain rule as flow through a computation graph:</p> <pre><code>x \u2192 [g] \u2192 g(x) \u2192 [f] \u2192 f(g(x)) = h(x)\n</code></pre> <ul> <li>Forward pass: compute values left to right</li> <li>Backward pass: compute gradients right to left</li> <li>Start with \u2202h/\u2202h = 1</li> <li>At f: multiply by f'(g(x))</li> <li>At g: multiply by g'(x)</li> <li>Result: h'(x) = f'(g(x)) \u00b7 g'(x)</li> </ul> <p>This graph perspective leads directly to automatic differentiation.</p>"},{"location":"stages/stage-02/03-chain-rule/#a-longer-chain","title":"A Longer Chain","text":"<p>Consider: $\\(y = \\sin(\\ln(x^2 + 1))\\)$</p> <p>Let's trace through step by step:</p> Variable Expression Derivative w.r.t. previous a x\u00b2 + 1 da/dx = 2x b ln(a) db/da = 1/a y sin(b) dy/db = cos(b) <p>By chain rule: $\\(\\frac{dy}{dx} = \\frac{dy}{db} \\cdot \\frac{db}{da} \\cdot \\frac{da}{dx} = \\cos(\\ln(x^2+1)) \\cdot \\frac{1}{x^2+1} \\cdot 2x\\)$</p> \\[= \\frac{2x \\cos(\\ln(x^2+1))}{x^2+1}\\]"},{"location":"stages/stage-02/03-chain-rule/#the-key-insight-for-autodiff","title":"The Key Insight for Autodiff","text":"<p>Notice the pattern: 1. Forward pass: compute intermediate values (a, b, y) 2. Backward pass: multiply derivatives in reverse order</p> <p>We don't need to derive a formula for the overall derivative. We just need: - The derivative of each primitive operation (sin, ln, +, \u00d7, etc.) - A systematic way to apply the chain rule</p> <p>This is automatic differentiation. We'll implement it in Section 2.6.</p>"},{"location":"stages/stage-02/03-chain-rule/#exercises","title":"Exercises","text":"<ol> <li>Basic chain rule: Find d/dx of:</li> <li>(3x + 1)\u2075</li> <li>e^{2x}</li> <li>ln(x\u00b3)</li> <li> <p>\u221a(1 + x\u00b2)</p> </li> <li> <p>Multiple applications: Find the derivative of sin(cos(x\u00b2)).</p> </li> <li> <p>Verify by expansion: For (x+1)\u00b2, compute the derivative using:</p> </li> <li>The chain rule</li> <li>Expanding to x\u00b2 + 2x + 1 and differentiating</li> <li> <p>Check they match.</p> </li> <li> <p>Multivariate: If f(x,y) = x\u00b2y\u00b3, x = t\u00b2, y = t\u00b3, find df/dt two ways:</p> </li> <li>Substitute and differentiate directly</li> <li> <p>Use the multivariate chain rule</p> </li> <li> <p>Neural network layer: A layer computes y = \u03c3(Wx + b) where \u03c3 is an activation function. If L is a loss depending on y, express \u2202L/\u2202W using the chain rule.</p> </li> </ol>"},{"location":"stages/stage-02/03-chain-rule/#summary","title":"Summary","text":"Concept Formula Intuition Chain rule (single) (f\u2218g)' = f'(g) \u00b7 g' Rates multiply Chain rule (multi) \u2202z/\u2202x = \u03a3 (\u2202z/\u2202y\u1d62)(\u2202y\u1d62/\u2202x) Sum over paths Leibniz notation dy/dx = (dy/du)(du/dx) \"Cancel\" the du Backprop Gradients flow backward Local gradients multiply <p>Key insight: The chain rule turns differentiation of complex expressions into local operations connected by multiplication. This locality is what makes automatic differentiation possible and efficient.</p> <p>Next: We'll represent computations as graphs and see how the chain rule applies to them systematically.</p>"},{"location":"stages/stage-02/04-computational-graphs/","title":"Section 2.4: Computational Graphs","text":"<p>The chain rule tells us how to compute derivatives of compositions. But for complex expressions with many operations, we need a systematic way to represent the computation and apply the chain rule.</p> <p>Computational graphs provide this representation. They're the key data structure that makes automatic differentiation possible.</p>"},{"location":"stages/stage-02/04-computational-graphs/#what-is-a-computational-graph","title":"What is a Computational Graph?","text":"<p>A computational graph is a directed acyclic graph (DAG) that represents a mathematical expression:</p> <ul> <li>Nodes represent values (inputs, intermediate results, outputs)</li> <li>Edges represent dependencies (which values are used to compute which)</li> <li>Operations are associated with nodes (what function produced this value)</li> </ul>"},{"location":"stages/stage-02/04-computational-graphs/#example-fx-y-x-y-x","title":"Example: f(x, y) = (x + y) \u00b7 x","text":"<p>Let's trace through this computation:</p> <ol> <li>Start with inputs: x, y</li> <li>Compute a = x + y</li> <li>Compute f = a \u00b7 x</li> </ol> <p>The graph: <pre><code>    x \u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n           \u2502         \u2502\n           \u25bc         \u2502\n    y \u2500\u2500\u25b6 [+] \u2500\u2500\u25b6 a  \u2502\n                 \u2502   \u2502\n                 \u25bc   \u25bc\n                [\u00d7] \u2500\u2500\u25b6 f\n</code></pre></p> <p>Reading the graph: - x and y are inputs (no incoming edges) - a depends on x and y via addition - f depends on a and x via multiplication - f is the output</p>"},{"location":"stages/stage-02/04-computational-graphs/#terminology","title":"Terminology","text":"Term Meaning Leaf node Input variable (no incoming edges) Internal node Intermediate computation Root node Final output Parents of v Nodes that v directly depends on Children of v Nodes that directly depend on v"},{"location":"stages/stage-02/04-computational-graphs/#building-a-computational-graph","title":"Building a Computational Graph","text":"<p>Any expression can be decomposed into primitive operations, each becoming a node.</p>"},{"location":"stages/stage-02/04-computational-graphs/#example-fx-sinx2-x","title":"Example: f(x) = sin(x\u00b2) + x","text":"<p>Decomposition: 1. a = x\u00b2 2. b = sin(a) 3. f = b + x</p> <p>Graph: <pre><code>       x \u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n              \u2502              \u2502\n              \u25bc              \u2502\n            [x\u00b2] \u2500\u2500\u25b6 a       \u2502\n              \u2502              \u2502\n              \u25bc              \u2502\n           [sin] \u2500\u2500\u25b6 b       \u2502\n              \u2502              \u2502\n              \u25bc              \u25bc\n             [+] \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6 f\n</code></pre></p>"},{"location":"stages/stage-02/04-computational-graphs/#example-neural-network-layer","title":"Example: Neural Network Layer","text":"<p>A typical layer computes: y = \u03c3(Wx + b)</p> <p>Decomposition: 1. a = W \u00d7 x (matrix multiply) 2. c = a + b (add bias) 3. y = \u03c3(c) (activation)</p> <p>Each of these becomes a node in the graph.</p>"},{"location":"stages/stage-02/04-computational-graphs/#the-forward-pass","title":"The Forward Pass","text":"<p>Forward pass (or forward propagation): Evaluate the graph from inputs to outputs.</p> <p>Algorithm: 1. Assign values to input nodes 2. Process nodes in topological order (parents before children) 3. At each node, apply its operation to parent values</p>"},{"location":"stages/stage-02/04-computational-graphs/#example-forward-pass","title":"Example Forward Pass","text":"<p>For f(x, y) = (x + y) \u00b7 x with x = 3, y = 2:</p> Step Node Operation Value 1 x input 3 2 y input 2 3 a x + y 5 4 f a \u00d7 x 15 <p>Result: f(3, 2) = 15</p>"},{"location":"stages/stage-02/04-computational-graphs/#topological-order","title":"Topological Order","text":"<p>A topological order ensures we process each node only after all its parents.</p> <p>For a DAG, this is always possible. We can use: - Kahn's algorithm: Repeatedly remove nodes with no incoming edges - DFS: Post-order traversal gives reverse topological order</p>"},{"location":"stages/stage-02/04-computational-graphs/#the-backward-pass","title":"The Backward Pass","text":"<p>Backward pass (or backpropagation): Compute gradients from outputs to inputs.</p> <p>This is where the chain rule comes in. For each node v:</p> \\[\\frac{\\partial f}{\\partial v} = \\sum_{c \\in \\text{children}(v)} \\frac{\\partial f}{\\partial c} \\cdot \\frac{\\partial c}{\\partial v}\\] <p>We sum over all paths from v to the output f.</p>"},{"location":"stages/stage-02/04-computational-graphs/#algorithm","title":"Algorithm","text":"<ol> <li>Set \u2202f/\u2202f = 1 (gradient of output with respect to itself)</li> <li>Process nodes in reverse topological order (children before parents)</li> <li>At each node v:</li> <li>For each parent p of v:</li> <li>Add (\u2202f/\u2202v) \u00d7 (\u2202v/\u2202p) to \u2202f/\u2202p</li> </ol>"},{"location":"stages/stage-02/04-computational-graphs/#example-backward-pass","title":"Example Backward Pass","text":"<p>For f(x, y) = (x + y) \u00b7 x, compute \u2202f/\u2202x and \u2202f/\u2202y.</p> <p>First, the forward pass gave us: a = 5, f = 15</p> <p>Now backward:</p> Step Node Gradient Computation 1 f \u2202f/\u2202f = 1 (start) 2 a \u2202f/\u2202a = x = 3 \u2202(a\u00b7x)/\u2202a = x 3 x \u2202f/\u2202x = ? Via a: \u2202f/\u2202a \u00b7 \u2202a/\u2202x = 3 \u00b7 1 = 3 Via f: \u2202f/\u2202x \u00b7 1 = a = 5 Total: 3 + 5 = 8 (sum paths) 4 y \u2202f/\u2202y = 3 \u2202f/\u2202a \u00b7 \u2202a/\u2202y = 3 \u00b7 1 = 3 <p>Verification: f(x, y) = (x + y) \u00b7 x = x\u00b2 + xy</p> <p>\u2202f/\u2202x = 2x + y = 2(3) + 2 = 8 \u2713 \u2202f/\u2202y = x = 3 \u2713</p>"},{"location":"stages/stage-02/04-computational-graphs/#key-insight-summing-paths","title":"Key Insight: Summing Paths","text":"<p>When a node contributes to the output through multiple paths, we sum the gradients from each path.</p> <p>In the example, x contributes to f two ways: 1. Through a (as part of x + y): contributes gradient 3 2. Directly (as the second operand of multiplication): contributes gradient 5 3. Total: 8</p> <p>This is the multivariate chain rule in action.</p>"},{"location":"stages/stage-02/04-computational-graphs/#storing-information-for-the-backward-pass","title":"Storing Information for the Backward Pass","text":"<p>During the forward pass, we need to store information for the backward pass:</p> <ol> <li>Graph structure: Which nodes depend on which</li> <li>Intermediate values: Some derivatives need forward pass values</li> <li>Gradients: Accumulated during the backward pass</li> </ol>"},{"location":"stages/stage-02/04-computational-graphs/#example-multiplication-requires-cached-values","title":"Example: Multiplication Requires Cached Values","text":"<p>For z = x \u00b7 y: - \u2202z/\u2202x = y (need the value of y) - \u2202z/\u2202y = x (need the value of x)</p> <p>So the multiplication node must cache x and y during the forward pass.</p>"},{"location":"stages/stage-02/04-computational-graphs/#example-addition-doesnt-need-values","title":"Example: Addition Doesn't Need Values","text":"<p>For z = x + y: - \u2202z/\u2202x = 1 - \u2202z/\u2202y = 1</p> <p>These are constants\u2014no need to cache x or y.</p>"},{"location":"stages/stage-02/04-computational-graphs/#local-gradients","title":"Local Gradients","text":"<p>Each operation has a local gradient: the derivative of its output with respect to each input.</p> Operation z = \u2202z/\u2202x \u2202z/\u2202y Notes Add x + y 1 1 Constant Subtract x - y 1 -1 Constant Multiply x \u00b7 y y x Need cached values Divide x / y 1/y -x/y\u00b2 Need cached values Power x^n n\u00b7x^{n-1} - Need cached value Exp e^x e^x - Can use output z Log ln(x) 1/x - Need cached value Sin sin(x) cos(x) - Need cached value ReLU max(0,x) 1 if x&gt;0, else 0 - Need sign of x <p>Note: For exp, we can use the output: \u2202(e^x)/\u2202x = e^x = z. This saves memory.</p>"},{"location":"stages/stage-02/04-computational-graphs/#a-complete-walkthrough","title":"A Complete Walkthrough","text":"<p>Let's trace f(x) = sin(x\u00b2) for x = \u221a(\u03c0/2).</p>"},{"location":"stages/stage-02/04-computational-graphs/#forward-pass","title":"Forward Pass","text":"Node Expression Value x input \u221a(\u03c0/2) \u2248 1.253 a x\u00b2 \u03c0/2 \u2248 1.571 f sin(a) sin(\u03c0/2) = 1"},{"location":"stages/stage-02/04-computational-graphs/#backward-pass","title":"Backward Pass","text":"<p>Start with \u2202f/\u2202f = 1.</p> Node Local gradient Chain rule Gradient a \u2202(sin a)/\u2202a = cos(a) 1 \u00b7 cos(\u03c0/2) 0 x \u2202(x\u00b2)/\u2202x = 2x 0 \u00b7 2(1.253) 0 <p>So \u2202f/\u2202x = 0 at x = \u221a(\u03c0/2).</p> <p>Check: f(x) = sin(x\u00b2), so f'(x) = cos(x\u00b2) \u00b7 2x. At x = \u221a(\u03c0/2): f'(x) = cos(\u03c0/2) \u00b7 2\u221a(\u03c0/2) = 0 \u00b7 2.51 = 0 \u2713</p>"},{"location":"stages/stage-02/04-computational-graphs/#graph-representation-in-code","title":"Graph Representation in Code","text":"<p>How do we represent a computational graph in code?</p>"},{"location":"stages/stage-02/04-computational-graphs/#the-node-class-conceptual","title":"The Node Class (Conceptual)","text":"<pre><code>class Node:\n    def __init__(self, value, parents=None, operation=None):\n        self.value = value           # Forward pass result\n        self.grad = 0.0              # Accumulated gradient\n        self.parents = parents or [] # Input nodes\n        self.operation = operation   # How this node was computed\n</code></pre>"},{"location":"stages/stage-02/04-computational-graphs/#recording-the-graph","title":"Recording the Graph","text":"<p>When we compute z = x * y: <pre><code>z = Node(\n    value=x.value * y.value,\n    parents=[x, y],\n    operation='mul'\n)\n</code></pre></p>"},{"location":"stages/stage-02/04-computational-graphs/#the-backward-function","title":"The Backward Function","text":"<pre><code>def backward(node):\n    # Topological sort in reverse\n    topo_order = reverse_topological_sort(node)\n\n    node.grad = 1.0  # df/df = 1\n\n    for n in topo_order:\n        for parent, local_grad in n.get_local_gradients():\n            parent.grad += n.grad * local_grad\n</code></pre> <p>We'll implement this fully in Section 2.6.</p>"},{"location":"stages/stage-02/04-computational-graphs/#why-dags-why-not-trees","title":"Why DAGs? Why Not Trees?","text":"<p>Some expressions share subexpressions:</p> <p>f(x) = x\u00b2 + sin(x\u00b2)</p> <p>The term x\u00b2 appears twice. With a tree, we'd compute it twice. With a DAG, we compute it once and reuse.</p> <p>In neural networks, weight matrices are used in the forward pass and needed in the backward pass. DAGs naturally handle this sharing.</p>"},{"location":"stages/stage-02/04-computational-graphs/#summary","title":"Summary","text":"Concept Description Computational graph DAG representing computation Forward pass Evaluate graph, cache intermediates Backward pass Apply chain rule in reverse order Local gradient \u2202(output)/\u2202(input) for one operation Gradient accumulation Sum gradients from multiple paths <p>Key insight: The computational graph makes the chain rule mechanical. Given local gradients for each primitive operation, we can differentiate any composition automatically.</p>"},{"location":"stages/stage-02/04-computational-graphs/#exercises","title":"Exercises","text":"<ol> <li> <p>Draw the graph: For f(x) = (x - 1)(x + 1), draw the computational graph and verify \u2202f/\u2202x by the backward pass.</p> </li> <li> <p>Multiple uses: For f(x) = x \u00b7 x, draw the graph. Why is \u2202f/\u2202x = 2x (not x)?</p> </li> <li> <p>Three variables: For f(x,y,z) = (x + y) \u00b7 z, compute all three partial derivatives using backward pass.</p> </li> <li> <p>Graph for neural layer: Draw the computational graph for y = relu(w\u00b7x + b) where relu(t) = max(0, t).</p> </li> <li> <p>Memory analysis: Which operations need to cache their inputs? Which can compute local gradients from their output?</p> </li> </ol>"},{"location":"stages/stage-02/04-computational-graphs/#whats-next","title":"What's Next","text":"<p>We've seen how to represent computation as a graph and apply the chain rule systematically. But there are two fundamentally different ways to traverse the graph:</p> <ul> <li>Forward mode: Compute \u2202(everything)/\u2202(one input)</li> <li>Reverse mode: Compute \u2202(one output)/\u2202(everything)</li> </ul> <p>For machine learning, reverse mode is dramatically more efficient. Section 2.5 explains why.</p>"},{"location":"stages/stage-02/05-forward-vs-reverse/","title":"Section 2.5: Forward Mode vs Reverse Mode","text":"<p>There are two fundamentally different ways to apply the chain rule through a computational graph. The choice between them has massive implications for efficiency.</p> <p>Bottom line: For neural networks (many inputs, one output), reverse mode is exponentially faster. This is why backpropagation works.</p>"},{"location":"stages/stage-02/05-forward-vs-reverse/#the-two-modes","title":"The Two Modes","text":"<p>Both modes compute exact derivatives. They differ in direction:</p> Mode Computes Direction Efficient when Forward \u2202(all outputs)/\u2202(one input) input \u2192 output few inputs Reverse \u2202(one output)/\u2202(all inputs) output \u2192 input few outputs"},{"location":"stages/stage-02/05-forward-vs-reverse/#forward-mode-intuition","title":"Forward Mode: Intuition","text":"<p>In forward mode, we pick one input x\u1d62 and compute how it affects everything downstream.</p> <p>We propagate derivatives forward alongside values: - Each node stores: (value, \u2202value/\u2202x\u1d62) - This pair flows through the graph - At the end, we have \u2202output/\u2202x\u1d62</p> <p>To get derivatives with respect to all inputs, we need n forward passes (one per input).</p>"},{"location":"stages/stage-02/05-forward-vs-reverse/#forward-mode-algorithm","title":"Forward Mode: Algorithm","text":"<p>Input: Computational graph, input values, index i of input to differentiate</p> <p>Process: 1. Set derivative of x\u1d62 to 1, all other inputs to 0 2. For each node v in topological order:    - Compute v's value from parents (standard forward pass)    - Compute \u2202v/\u2202x\u1d62 using chain rule:      $\\(\\frac{\\partial v}{\\partial x_i} = \\sum_{p \\in \\text{parents}(v)} \\frac{\\partial v}{\\partial p} \\cdot \\frac{\\partial p}{\\partial x_i}\\)$</p> <p>Output: \u2202output/\u2202x\u1d62</p>"},{"location":"stages/stage-02/05-forward-vs-reverse/#forward-mode-example","title":"Forward Mode: Example","text":"<p>Consider f(x\u2081, x\u2082) = x\u2081\u00b7x\u2082 + sin(x\u2081).</p> <p>Let x\u2081 = 2, x\u2082 = 3. Let's compute \u2202f/\u2202x\u2081.</p> <p>Graph: <pre><code>x\u2081 \u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u25b6 [\u00d7] \u2500\u2500\u25b6 a \u2500\u2500\u2510\n     \u2502         \u25b2          \u2502\n     \u2502         \u2502          \u25bc\n     \u2502        x\u2082         [+] \u2500\u2500\u25b6 f\n     \u2502                    \u25b2\n     \u2514\u2500\u2500\u25b6 [sin] \u2500\u2500\u25b6 b \u2500\u2500\u2500\u2500\u2518\n</code></pre></p> <p>Forward pass with derivatives (dx means \u2202/\u2202x\u2081):</p> Node Value Derivative \u2202\u00b7/\u2202x\u2081 x\u2081 2 1 (seed) x\u2082 3 0 a = x\u2081\u00b7x\u2082 6 x\u2082\u00b71 + x\u2081\u00b70 = 3 b = sin(x\u2081) sin(2) \u2248 0.91 cos(x\u2081)\u00b71 = cos(2) \u2248 -0.42 f = a + b 6.91 3 + (-0.42) = 2.58 <p>So \u2202f/\u2202x\u2081 \u2248 2.58.</p> <p>To get \u2202f/\u2202x\u2082, we need another forward pass with seed (0, 1).</p>"},{"location":"stages/stage-02/05-forward-vs-reverse/#dual-numbers-the-math-of-forward-mode","title":"Dual Numbers: The Math of Forward Mode","text":"<p>Forward mode has an elegant mathematical formulation using dual numbers.</p> <p>A dual number has the form: a + b\u03b5 where \u03b5\u00b2 = 0 but \u03b5 \u2260 0.</p> <p>This is like complex numbers but with \u03b5\u00b2 = 0 instead of i\u00b2 = -1.</p> <p>Key property: f(a + b\u03b5) = f(a) + f'(a)\u00b7b\u00b7\u03b5</p> <p>The \u03b5 coefficient automatically carries the derivative!</p> <p>Example: (2 + 1\u00b7\u03b5)\u00b2 = 4 + 4\u03b5 + \u03b5\u00b2 = 4 + 4\u03b5 (since \u03b5\u00b2 = 0)</p> <p>This matches: d/dx(x\u00b2) at x=2 is 2\u00b72 = 4.</p>"},{"location":"stages/stage-02/05-forward-vs-reverse/#reverse-mode-intuition","title":"Reverse Mode: Intuition","text":"<p>In reverse mode, we pick one output and compute how all inputs affect it.</p> <p>We propagate gradients backward from output to inputs: - Start with \u2202output/\u2202output = 1 - Work backward through the graph - Accumulate \u2202output/\u2202v for each node v</p> <p>One backward pass gives derivatives with respect to all inputs!</p>"},{"location":"stages/stage-02/05-forward-vs-reverse/#reverse-mode-algorithm","title":"Reverse Mode: Algorithm","text":"<p>Input: Computational graph (already evaluated), output node</p> <p>Process: 1. Forward pass: compute all values 2. Set gradient of output to 1 3. For each node v in reverse topological order:    - For each parent p of v:      $\\(\\frac{\\partial \\text{output}}{\\partial p} \\mathrel{+}= \\frac{\\partial \\text{output}}{\\partial v} \\cdot \\frac{\\partial v}{\\partial p}\\)$</p> <p>Output: \u2202output/\u2202(all inputs)</p>"},{"location":"stages/stage-02/05-forward-vs-reverse/#reverse-mode-example","title":"Reverse Mode: Example","text":"<p>Same function: f(x\u2081, x\u2082) = x\u2081\u00b7x\u2082 + sin(x\u2081), with x\u2081 = 2, x\u2082 = 3.</p> <p>Forward pass (compute values): - a = x\u2081\u00b7x\u2082 = 6 - b = sin(x\u2081) = sin(2) \u2248 0.91 - f = a + b \u2248 6.91</p> <p>Backward pass:</p> Step Node Gradient \u2202f/\u2202\u00b7 How 1 f 1 seed 2 a 1 \u2202f/\u2202a = 1 (from addition) 3 b 1 \u2202f/\u2202b = 1 (from addition) 4 x\u2081 (via a) x\u2082 = 3 \u2202f/\u2202a \u00b7 \u2202a/\u2202x\u2081 = 1 \u00b7 3 5 x\u2082 (via a) x\u2081 = 2 \u2202f/\u2202a \u00b7 \u2202a/\u2202x\u2082 = 1 \u00b7 2 6 x\u2081 (via b) cos(2) \u2248 -0.42 \u2202f/\u2202b \u00b7 \u2202b/\u2202x\u2081 = 1 \u00b7 cos(2) 7 x\u2081 (total) 3 + (-0.42) \u2248 2.58 sum both paths <p>Results from ONE backward pass: - \u2202f/\u2202x\u2081 \u2248 2.58 - \u2202f/\u2202x\u2082 = 2</p>"},{"location":"stages/stage-02/05-forward-vs-reverse/#complexity-comparison","title":"Complexity Comparison","text":"<p>Consider a function f: \u211d\u207f \u2192 \u211d\u1d50 (n inputs, m outputs).</p> Mode Passes needed Total work Forward n passes O(n \u00b7 graph size) Reverse m passes O(m \u00b7 graph size) <p>For neural networks: - n = millions of parameters - m = 1 (scalar loss) - Forward mode: millions of passes - Reverse mode: ONE pass</p> <p>Reverse mode wins by a factor of n!</p> <p>This is why backpropagation (reverse mode autodiff) is the standard for training neural networks.</p>"},{"location":"stages/stage-02/05-forward-vs-reverse/#the-jacobian-perspective","title":"The Jacobian Perspective","text":"<p>For f: \u211d\u207f \u2192 \u211d\u1d50, the Jacobian is an m \u00d7 n matrix:</p> \\[J_{ij} = \\frac{\\partial f_i}{\\partial x_j}\\] <ul> <li>Forward mode: Computes one column of J per pass (how one input affects all outputs)</li> <li>Reverse mode: Computes one row of J per pass (how all inputs affect one output)</li> </ul> <p>For a loss function L: \u211d\u207f \u2192 \u211d, the Jacobian is 1 \u00d7 n (a row vector = the gradient). Reverse mode computes the entire gradient in one pass.</p>"},{"location":"stages/stage-02/05-forward-vs-reverse/#when-to-use-each-mode","title":"When to Use Each Mode","text":"Scenario n (inputs) m (outputs) Best mode Neural network training millions 1 Reverse Sensitivity analysis few many Forward Jacobian-vector product any any Depends on direction Scientific computing varies varies Often forward <p>Machine learning almost always uses reverse mode because we have one scalar loss.</p>"},{"location":"stages/stage-02/05-forward-vs-reverse/#vector-jacobian-products-vjp-vs-jacobian-vector-products-jvp","title":"Vector-Jacobian Products (VJP) vs Jacobian-Vector Products (JVP)","text":"<p>The two modes can be understood as different matrix-vector products:</p> <p>Forward mode (JVP): Given vector v (tangent), compute J\u00b7v - \"If inputs change by v, how do outputs change?\"</p> <p>Reverse mode (VJP): Given vector u (cotangent), compute u\u1d40\u00b7J - \"To change output by u, how much does each input contribute?\"</p> <p>For scalar loss with u = 1: - VJP gives the full gradient - This is exactly what we need for gradient descent</p>"},{"location":"stages/stage-02/05-forward-vs-reverse/#memory-trade-offs","title":"Memory Trade-offs","text":"Mode Memory requirement Forward O(1) extra (just carry derivatives) Reverse O(graph size) (must cache forward pass values) <p>Reverse mode needs to store intermediate values for the backward pass. For deep networks, this can be significant.</p> <p>Techniques to reduce memory: - Gradient checkpointing: Recompute some values instead of storing - Trading compute for memory</p>"},{"location":"stages/stage-02/05-forward-vs-reverse/#a-larger-example","title":"A Larger Example","text":"<p>Consider a 3-layer neural network: <pre><code>x \u2192 [W\u2081] \u2192 relu \u2192 [W\u2082] \u2192 relu \u2192 [W\u2083] \u2192 y \u2192 [loss] \u2192 L\n</code></pre></p> <p>Parameters: W\u2081, W\u2082, W\u2083 (possibly millions of numbers) Output: L (scalar)</p> <p>Forward mode: To get \u2202L/\u2202W\u1d62\u2c7c for each weight, we need one pass per weight. With 1 million weights = 1 million forward passes.</p> <p>Reverse mode: One forward pass + one backward pass = 2 total passes. Done.</p>"},{"location":"stages/stage-02/05-forward-vs-reverse/#the-adjoint-method","title":"The Adjoint Method","text":"<p>Reverse mode is also called the adjoint method in scientific computing.</p> <p>The key insight: instead of propagating \"how does this input affect output\" forward, propagate \"how does output depend on this intermediate value\" backward.</p> <p>This is a duality\u2014both compute the same derivatives, just in different orders.</p>"},{"location":"stages/stage-02/05-forward-vs-reverse/#summary","title":"Summary","text":"Aspect Forward Mode Reverse Mode Direction Input \u2192 Output Output \u2192 Input Computes \u2202(all)/\u2202(one input) \u2202(one output)/\u2202(all) Passes for \u211d\u207f\u2192\u211d n 1 Memory Low Must cache forward pass Math Dual numbers Adjoint method Used in Sensitivity analysis Deep learning <p>The key insight: Neural network training involves computing \u2202(scalar loss)/\u2202(all parameters). Reverse mode does this in O(1) passes; forward mode takes O(parameters) passes. This makes reverse mode essential for deep learning.</p>"},{"location":"stages/stage-02/05-forward-vs-reverse/#exercises","title":"Exercises","text":"<ol> <li> <p>Forward vs reverse count: For f: \u211d\u00b9\u2070\u2070 \u2192 \u211d\u00b9\u2070, how many passes does each mode need to compute the full Jacobian?</p> </li> <li> <p>Dual numbers: Using dual numbers, compute d/dx(x\u00b3) at x = 2 by evaluating (2 + \u03b5)\u00b3.</p> </li> <li> <p>Memory analysis: In a 100-layer network, roughly how many intermediate values need to be cached for reverse mode?</p> </li> <li> <p>When forward wins: Give an example where forward mode is more efficient than reverse mode.</p> </li> <li> <p>Thinking question: If we have f: \u211d\u207f \u2192 \u211d\u207f and need the full Jacobian, which mode is better? Can we do better than n passes?</p> </li> </ol>"},{"location":"stages/stage-02/05-forward-vs-reverse/#whats-next","title":"What's Next","text":"<p>We understand the theory. Now let's build it.</p> <p>In Section 2.6, we'll implement a complete automatic differentiation system from scratch\u2014supporting forward pass recording and reverse mode gradient computation.</p>"},{"location":"stages/stage-02/06-autograd-from-scratch/","title":"Section 2.6: Building Autograd from Scratch","text":"<p>We've covered the theory: derivatives, the chain rule, computational graphs, and reverse mode differentiation. Now let's build a complete automatic differentiation system from scratch.</p> <p>By the end of this section, you'll have working code that can differentiate arbitrary compositions of operations\u2014the same capability that powers PyTorch and TensorFlow.</p>"},{"location":"stages/stage-02/06-autograd-from-scratch/#design-goals","title":"Design Goals","text":"<p>Our autograd system will:</p> <ol> <li>Track computations: Build a graph as we compute</li> <li>Support basic operations: +, -, \u00d7, /, power, common functions</li> <li>Compute gradients automatically: One call to <code>.backward()</code> gives all gradients</li> <li>Be minimal but complete: ~100 lines of core code</li> </ol>"},{"location":"stages/stage-02/06-autograd-from-scratch/#the-value-class-core-data-structure","title":"The Value Class: Core Data Structure","text":"<p>Every value in our system will be a <code>Value</code> object that stores: - The actual numerical data - The gradient (accumulated during backward pass) - References to parent nodes (for graph traversal) - The operation that created it (for computing local gradients)</p> <pre><code>class Value:\n    \"\"\"A value in the computational graph with automatic differentiation.\"\"\"\n\n    def __init__(self, data, _parents=(), _op=''):\n        self.data = data\n        self.grad = 0.0\n        self._backward = lambda: None  # Function to compute parent gradients\n        self._parents = set(_parents)\n        self._op = _op  # For debugging/visualization\n\n    def __repr__(self):\n        return f\"Value(data={self.data}, grad={self.grad})\"\n</code></pre>"},{"location":"stages/stage-02/06-autograd-from-scratch/#why-this-structure","title":"Why This Structure?","text":"<ul> <li><code>data</code>: The actual number (forward pass result)</li> <li><code>grad</code>: Accumulates \u2202output/\u2202self during backward pass</li> <li><code>_backward</code>: A closure that knows how to propagate gradients to parents</li> <li><code>_parents</code>: The nodes this value depends on (graph edges)</li> <li><code>_op</code>: What operation created this (useful for debugging)</li> </ul>"},{"location":"stages/stage-02/06-autograd-from-scratch/#implementing-addition","title":"Implementing Addition","text":"<p>Let's start with the simplest operation: addition.</p> <p>For z = x + y: - Local gradients: \u2202z/\u2202x = 1, \u2202z/\u2202y = 1 - Backward: parent.grad += self.grad \u00d7 local_gradient</p> <pre><code>def __add__(self, other):\n    other = other if isinstance(other, Value) else Value(other)\n    out = Value(self.data + other.data, (self, other), '+')\n\n    def _backward():\n        self.grad += out.grad * 1.0   # \u2202z/\u2202x = 1\n        other.grad += out.grad * 1.0  # \u2202z/\u2202y = 1\n\n    out._backward = _backward\n    return out\n</code></pre>"},{"location":"stages/stage-02/06-autograd-from-scratch/#key-points","title":"Key Points","text":"<ol> <li>Wrap raw numbers: If <code>other</code> isn't a Value, make it one</li> <li>Store parents: The output depends on <code>self</code> and <code>other</code></li> <li>Define backward: A closure that knows how to propagate gradients</li> <li>Accumulate with +=: A value might be used multiple times</li> </ol>"},{"location":"stages/stage-02/06-autograd-from-scratch/#why-instead-of","title":"Why += Instead of =?","text":"<p>Consider f(x) = x + x. The variable x is used twice.</p> <p>If we used <code>=</code>, the second path would overwrite the first. With <code>+=</code>, both contributions accumulate: - First x: grad += 1 - Second x: grad += 1 - Total: grad = 2</p> <p>This is correct: \u2202(x+x)/\u2202x = 2.</p>"},{"location":"stages/stage-02/06-autograd-from-scratch/#implementing-multiplication","title":"Implementing Multiplication","text":"<p>For z = x \u00d7 y: - Local gradients: \u2202z/\u2202x = y, \u2202z/\u2202y = x - These need the cached values from the forward pass</p> <pre><code>def __mul__(self, other):\n    other = other if isinstance(other, Value) else Value(other)\n    out = Value(self.data * other.data, (self, other), '*')\n\n    def _backward():\n        self.grad += out.grad * other.data  # \u2202z/\u2202x = y\n        other.grad += out.grad * self.data  # \u2202z/\u2202y = x\n\n    out._backward = _backward\n    return out\n</code></pre>"},{"location":"stages/stage-02/06-autograd-from-scratch/#the-closure-captures-values","title":"The Closure Captures Values","text":"<p>Notice that <code>_backward</code> references <code>other.data</code> and <code>self.data</code>. These are captured in the closure when the function is defined.</p> <p>This is how we \"cache\" forward pass values for the backward pass\u2014through Python's closure mechanism.</p>"},{"location":"stages/stage-02/06-autograd-from-scratch/#the-backward-pass-topological-sort","title":"The Backward Pass: Topological Sort","text":"<p>To compute all gradients, we need to: 1. Find all nodes in the graph 2. Process them in reverse topological order 3. Call each node's <code>_backward()</code> function</p> <pre><code>def backward(self):\n    # Build topological order\n    topo = []\n    visited = set()\n\n    def build_topo(v):\n        if v not in visited:\n            visited.add(v)\n            for parent in v._parents:\n                build_topo(parent)\n            topo.append(v)\n\n    build_topo(self)\n\n    # Initialize gradient of output\n    self.grad = 1.0\n\n    # Backward pass in reverse order\n    for v in reversed(topo):\n        v._backward()\n</code></pre>"},{"location":"stages/stage-02/06-autograd-from-scratch/#why-topological-order","title":"Why Topological Order?","text":"<p>Consider: z = (x + y) \u00d7 x</p> <p>Graph: <pre><code>x \u2500\u2500\u252c\u2500\u2500\u25b6 [+] \u2500\u2500\u25b6 a \u2500\u2500\u2510\n    \u2502     \u25b2          \u2502\n    \u2502     \u2502          \u25bc\n    \u2502     y         [\u00d7] \u2500\u2500\u25b6 z\n    \u2502                \u25b2\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p> <p>We must process z before a (to have z.grad when computing a's contribution), and a before x and y.</p> <p>Reverse topological order guarantees: each node is processed only after all its children (nodes that depend on it) have been processed.</p>"},{"location":"stages/stage-02/06-autograd-from-scratch/#putting-it-together-first-test","title":"Putting It Together: First Test","text":"<pre><code># Create values\nx = Value(2.0)\ny = Value(3.0)\n\n# Forward pass (builds graph automatically)\nz = x * y + x\n\n# Backward pass\nz.backward()\n\nprint(f\"z = {z.data}\")     # z = 8.0\nprint(f\"\u2202z/\u2202x = {x.grad}\") # \u2202z/\u2202x = 4.0\nprint(f\"\u2202z/\u2202y = {y.grad}\") # \u2202z/\u2202y = 2.0\n</code></pre>"},{"location":"stages/stage-02/06-autograd-from-scratch/#verification","title":"Verification","text":"<p>z = x \u00d7 y + x = 2 \u00d7 3 + 2 = 8 \u2713</p> <p>\u2202z/\u2202x = y + 1 = 3 + 1 = 4 \u2713 (x contributes twice: via multiplication and addition)</p> <p>\u2202z/\u2202y = x = 2 \u2713</p> <p>It works!</p>"},{"location":"stages/stage-02/06-autograd-from-scratch/#completing-the-operations","title":"Completing the Operations","text":""},{"location":"stages/stage-02/06-autograd-from-scratch/#subtraction-and-negation","title":"Subtraction and Negation","text":"<pre><code>def __neg__(self):\n    return self * -1\n\ndef __sub__(self, other):\n    return self + (-other)\n\ndef __rsub__(self, other):  # other - self\n    return other + (-self)\n</code></pre>"},{"location":"stages/stage-02/06-autograd-from-scratch/#division","title":"Division","text":"<p>For z = x / y = x \u00d7 y\u207b\u00b9: - \u2202z/\u2202x = 1/y - \u2202z/\u2202y = -x/y\u00b2</p> <pre><code>def __truediv__(self, other):\n    return self * other**-1\n</code></pre>"},{"location":"stages/stage-02/06-autograd-from-scratch/#power","title":"Power","text":"<p>For z = x^n (where n is a constant): - \u2202z/\u2202x = n \u00d7 x^(n-1)</p> <pre><code>def __pow__(self, n):\n    assert isinstance(n, (int, float)), \"only supporting constant powers\"\n    out = Value(self.data ** n, (self,), f'**{n}')\n\n    def _backward():\n        self.grad += out.grad * (n * self.data ** (n - 1))\n\n    out._backward = _backward\n    return out\n</code></pre>"},{"location":"stages/stage-02/06-autograd-from-scratch/#reverse-operations","title":"Reverse Operations","text":"<p>Python calls <code>__radd__</code> when the left operand doesn't support the operation:</p> <pre><code>def __radd__(self, other):  # other + self\n    return self + other\n\ndef __rmul__(self, other):  # other * self\n    return self * other\n</code></pre> <p>This lets us write <code>2 * x</code> where x is a Value.</p>"},{"location":"stages/stage-02/06-autograd-from-scratch/#activation-functions","title":"Activation Functions","text":"<p>Neural networks need nonlinear activations. Let's implement the most common ones.</p>"},{"location":"stages/stage-02/06-autograd-from-scratch/#relu","title":"ReLU","text":"\\[\\text{relu}(x) = \\max(0, x)\\] <p>Derivative: $\\(\\frac{\\partial}{\\partial x}\\text{relu}(x) = \\begin{cases} 1 &amp; \\text{if } x &gt; 0 \\\\ 0 &amp; \\text{otherwise} \\end{cases}\\)$</p> <pre><code>def relu(self):\n    out = Value(max(0, self.data), (self,), 'relu')\n\n    def _backward():\n        self.grad += out.grad * (1.0 if self.data &gt; 0 else 0.0)\n\n    out._backward = _backward\n    return out\n</code></pre>"},{"location":"stages/stage-02/06-autograd-from-scratch/#tanh","title":"Tanh","text":"\\[\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\\] <p>Derivative: $\\(\\frac{\\partial}{\\partial x}\\tanh(x) = 1 - \\tanh^2(x)\\)$</p> <pre><code>import math\n\ndef tanh(self):\n    t = math.tanh(self.data)\n    out = Value(t, (self,), 'tanh')\n\n    def _backward():\n        self.grad += out.grad * (1 - t ** 2)\n\n    out._backward = _backward\n    return out\n</code></pre> <p>Note: We use the output <code>t</code> to compute the derivative (1 - t\u00b2). This is a memory optimization\u2014we don't need to store the input.</p>"},{"location":"stages/stage-02/06-autograd-from-scratch/#sigmoid","title":"Sigmoid","text":"\\[\\sigma(x) = \\frac{1}{1 + e^{-x}}\\] <p>Derivative: $\\(\\frac{\\partial}{\\partial x}\\sigma(x) = \\sigma(x)(1 - \\sigma(x))\\)$</p> <pre><code>def sigmoid(self):\n    s = 1 / (1 + math.exp(-self.data))\n    out = Value(s, (self,), 'sigmoid')\n\n    def _backward():\n        self.grad += out.grad * (s * (1 - s))\n\n    out._backward = _backward\n    return out\n</code></pre>"},{"location":"stages/stage-02/06-autograd-from-scratch/#exponential-and-log","title":"Exponential and Log","text":"<pre><code>def exp(self):\n    out = Value(math.exp(self.data), (self,), 'exp')\n\n    def _backward():\n        self.grad += out.grad * out.data  # d/dx(e^x) = e^x\n\n    out._backward = _backward\n    return out\n\ndef log(self):\n    out = Value(math.log(self.data), (self,), 'log')\n\n    def _backward():\n        self.grad += out.grad * (1 / self.data)  # d/dx(ln x) = 1/x\n\n    out._backward = _backward\n    return out\n</code></pre>"},{"location":"stages/stage-02/06-autograd-from-scratch/#the-complete-value-class","title":"The Complete Value Class","text":"<p>Here's the full implementation:</p> <pre><code>import math\n\nclass Value:\n    \"\"\"A scalar value with automatic differentiation support.\"\"\"\n\n    def __init__(self, data, _parents=(), _op=''):\n        self.data = data\n        self.grad = 0.0\n        self._backward = lambda: None\n        self._parents = set(_parents)\n        self._op = _op\n\n    def __repr__(self):\n        return f\"Value(data={self.data:.4f}, grad={self.grad:.4f})\"\n\n    # Arithmetic operations\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += out.grad * other.data\n            other.grad += out.grad * self.data\n        out._backward = _backward\n        return out\n\n    def __pow__(self, n):\n        assert isinstance(n, (int, float))\n        out = Value(self.data ** n, (self,), f'**{n}')\n\n        def _backward():\n            self.grad += out.grad * (n * self.data ** (n - 1))\n        out._backward = _backward\n        return out\n\n    def __neg__(self):\n        return self * -1\n\n    def __sub__(self, other):\n        return self + (-other)\n\n    def __truediv__(self, other):\n        return self * (other ** -1)\n\n    def __radd__(self, other):\n        return self + other\n\n    def __rmul__(self, other):\n        return self * other\n\n    def __rsub__(self, other):\n        return other + (-self)\n\n    def __rtruediv__(self, other):\n        return other * (self ** -1)\n\n    # Activation functions\n    def relu(self):\n        out = Value(max(0, self.data), (self,), 'relu')\n        def _backward():\n            self.grad += out.grad * (1.0 if self.data &gt; 0 else 0.0)\n        out._backward = _backward\n        return out\n\n    def tanh(self):\n        t = math.tanh(self.data)\n        out = Value(t, (self,), 'tanh')\n        def _backward():\n            self.grad += out.grad * (1 - t ** 2)\n        out._backward = _backward\n        return out\n\n    def sigmoid(self):\n        s = 1 / (1 + math.exp(-self.data))\n        out = Value(s, (self,), 'sigmoid')\n        def _backward():\n            self.grad += out.grad * (s * (1 - s))\n        out._backward = _backward\n        return out\n\n    def exp(self):\n        out = Value(math.exp(self.data), (self,), 'exp')\n        def _backward():\n            self.grad += out.grad * out.data\n        out._backward = _backward\n        return out\n\n    def log(self):\n        out = Value(math.log(self.data), (self,), 'log')\n        def _backward():\n            self.grad += out.grad * (1 / self.data)\n        out._backward = _backward\n        return out\n\n    # Backward pass\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for parent in v._parents:\n                    build_topo(parent)\n                topo.append(v)\n\n        build_topo(self)\n        self.grad = 1.0\n\n        for v in reversed(topo):\n            v._backward()\n</code></pre> <p>That's it\u2014about 100 lines of code for a complete automatic differentiation engine.</p>"},{"location":"stages/stage-02/06-autograd-from-scratch/#building-a-neural-network","title":"Building a Neural Network","text":"<p>Let's use our autograd to build and train a simple neural network.</p>"},{"location":"stages/stage-02/06-autograd-from-scratch/#a-single-neuron","title":"A Single Neuron","text":"<p>A neuron computes: y = activation(w\u2081x\u2081 + w\u2082x\u2082 + ... + w\u2099x\u2099 + b)</p> <pre><code>class Neuron:\n    def __init__(self, n_inputs):\n        # Initialize weights randomly\n        self.w = [Value(random.uniform(-1, 1)) for _ in range(n_inputs)]\n        self.b = Value(0.0)\n\n    def __call__(self, x):\n        # w \u00b7 x + b\n        act = sum((wi * xi for wi, xi in zip(self.w, x)), self.b)\n        return act.tanh()\n\n    def parameters(self):\n        return self.w + [self.b]\n</code></pre>"},{"location":"stages/stage-02/06-autograd-from-scratch/#a-layer-of-neurons","title":"A Layer of Neurons","text":"<pre><code>class Layer:\n    def __init__(self, n_inputs, n_outputs):\n        self.neurons = [Neuron(n_inputs) for _ in range(n_outputs)]\n\n    def __call__(self, x):\n        outs = [n(x) for n in self.neurons]\n        return outs[0] if len(outs) == 1 else outs\n\n    def parameters(self):\n        return [p for n in self.neurons for p in n.parameters()]\n</code></pre>"},{"location":"stages/stage-02/06-autograd-from-scratch/#a-multi-layer-perceptron-mlp","title":"A Multi-Layer Perceptron (MLP)","text":"<pre><code>class MLP:\n    def __init__(self, n_inputs, layer_sizes):\n        sizes = [n_inputs] + layer_sizes\n        self.layers = [Layer(sizes[i], sizes[i+1])\n                       for i in range(len(layer_sizes))]\n\n    def __call__(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        return x\n\n    def parameters(self):\n        return [p for layer in self.layers for p in layer.parameters()]\n</code></pre>"},{"location":"stages/stage-02/06-autograd-from-scratch/#training-loop","title":"Training Loop","text":"<pre><code>import random\n\n# Create a simple dataset: learn XOR\nX = [[0, 0], [0, 1], [1, 0], [1, 1]]\ny = [0, 1, 1, 0]  # XOR outputs\n\n# Create network: 2 inputs -&gt; 4 hidden -&gt; 1 output\nmodel = MLP(2, [4, 1])\n\n# Training\nlearning_rate = 0.1\n\nfor epoch in range(100):\n    # Forward pass\n    predictions = [model(x) for x in X]\n\n    # Compute loss (mean squared error)\n    loss = sum((pred - target) ** 2 for pred, target in zip(predictions, y))\n\n    # Zero gradients (important!)\n    for p in model.parameters():\n        p.grad = 0.0\n\n    # Backward pass\n    loss.backward()\n\n    # Update parameters\n    for p in model.parameters():\n        p.data -= learning_rate * p.grad\n\n    if epoch % 10 == 0:\n        print(f\"Epoch {epoch}, Loss: {loss.data:.4f}\")\n\n# Test\nprint(\"\\nFinal predictions:\")\nfor x, target in zip(X, y):\n    pred = model(x)\n    print(f\"  Input: {x}, Target: {target}, Prediction: {pred.data:.4f}\")\n</code></pre> <p>Expected output: <pre><code>Epoch 0, Loss: 2.3456\nEpoch 10, Loss: 0.8234\nEpoch 20, Loss: 0.2341\n...\nEpoch 90, Loss: 0.0012\n\nFinal predictions:\n  Input: [0, 0], Target: 0, Prediction: 0.0234\n  Input: [0, 1], Target: 1, Prediction: 0.9812\n  Input: [1, 0], Target: 1, Prediction: 0.9756\n  Input: [1, 1], Target: 0, Prediction: 0.0345\n</code></pre></p> <p>We just trained a neural network using our own autograd system!</p>"},{"location":"stages/stage-02/06-autograd-from-scratch/#how-this-relates-to-real-frameworks","title":"How This Relates to Real Frameworks","text":"<p>Our implementation captures the essential ideas of PyTorch's autograd:</p> Our Implementation PyTorch <code>Value</code> <code>torch.Tensor</code> with <code>requires_grad=True</code> <code>_backward</code> closure <code>grad_fn</code> attribute <code>backward()</code> <code>tensor.backward()</code> Manual gradient zero <code>optimizer.zero_grad()</code> Manual parameter update <code>optimizer.step()</code>"},{"location":"stages/stage-02/06-autograd-from-scratch/#key-differences-in-real-frameworks","title":"Key Differences in Real Frameworks","text":"<ol> <li>Tensors, not scalars: PyTorch operates on multi-dimensional arrays</li> <li>GPU acceleration: Operations run on CUDA cores</li> <li>Optimized operations: Matrix multiplies use BLAS/cuBLAS</li> <li>Memory management: Sophisticated caching and cleanup</li> <li>Graph management: Options for static vs dynamic graphs</li> </ol> <p>But the core algorithm is the same: record operations during forward pass, then apply chain rule in reverse.</p>"},{"location":"stages/stage-02/06-autograd-from-scratch/#understanding-the-gradient-flow","title":"Understanding the Gradient Flow","text":"<p>Let's trace through a simple example to see exactly how gradients flow:</p> <pre><code>x = Value(2.0)\ny = Value(3.0)\nz = x * y    # z = 6\nw = z + x    # w = 8\nw.backward()\n</code></pre>"},{"location":"stages/stage-02/06-autograd-from-scratch/#forward-pass-builds-graph","title":"Forward Pass (Builds Graph)","text":"<pre><code>x(2.0) \u2500\u2500\u252c\u2500\u2500\u25b6 [*] \u2500\u2500\u25b6 z(6.0) \u2500\u2500\u25b6 [+] \u2500\u2500\u25b6 w(8.0)\n         \u2502     \u25b2                   \u25b2\n         \u2502     \u2502                   \u2502\n         \u2502    y(3.0)               \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"stages/stage-02/06-autograd-from-scratch/#backward-pass-computes-gradients","title":"Backward Pass (Computes Gradients)","text":"<p>Step 1: Initialize w.grad = 1.0</p> <p>Step 2: Process w (addition node) - w's _backward: <code>z.grad += 1.0</code>, <code>x.grad += 1.0</code> - After: z.grad = 1.0, x.grad = 1.0</p> <p>Step 3: Process z (multiplication node) - z's _backward: <code>x.grad += 1.0 * 3.0</code>, <code>y.grad += 1.0 * 2.0</code> - After: x.grad = 1.0 + 3.0 = 4.0, y.grad = 2.0</p> <p>Final Result: x.grad = 4.0, y.grad = 2.0</p> <p>Verification: w = xy + x, so \u2202w/\u2202x = y + 1 = 4 \u2713, \u2202w/\u2202y = x = 2 \u2713</p>"},{"location":"stages/stage-02/06-autograd-from-scratch/#common-pitfalls-and-solutions","title":"Common Pitfalls and Solutions","text":""},{"location":"stages/stage-02/06-autograd-from-scratch/#1-forgetting-to-zero-gradients","title":"1. Forgetting to Zero Gradients","text":"<pre><code># Wrong: gradients accumulate across iterations!\nfor epoch in range(10):\n    loss = model(x)\n    loss.backward()  # Gradients add to previous values!\n\n# Correct: zero gradients before each backward\nfor epoch in range(10):\n    for p in model.parameters():\n        p.grad = 0.0\n    loss = model(x)\n    loss.backward()\n</code></pre>"},{"location":"stages/stage-02/06-autograd-from-scratch/#2-in-place-operations","title":"2. In-Place Operations","text":"<pre><code># Dangerous: modifies data that might be needed for gradients\nx.data += 1  # If x was used in a computation, gradients may be wrong\n\n# Safe: create new Value\nx = x + 1\n</code></pre>"},{"location":"stages/stage-02/06-autograd-from-scratch/#3-gradient-through-non-differentiable-operations","title":"3. Gradient Through Non-Differentiable Operations","text":"<pre><code># ReLU at exactly 0 has undefined gradient\n# We define it as 0 (a common convention)\ndef relu(self):\n    out = Value(max(0, self.data), (self,), 'relu')\n    def _backward():\n        # gradient is 0 when input &lt;= 0\n        self.grad += out.grad * (1.0 if self.data &gt; 0 else 0.0)\n    out._backward = _backward\n    return out\n</code></pre>"},{"location":"stages/stage-02/06-autograd-from-scratch/#visualizing-the-graph","title":"Visualizing the Graph","text":"<p>For debugging, it helps to visualize the computational graph:</p> <pre><code>def draw_graph(root):\n    \"\"\"Generate DOT format graph description.\"\"\"\n    nodes, edges = set(), set()\n\n    def build(v):\n        if v not in nodes:\n            nodes.add(v)\n            for parent in v._parents:\n                edges.add((parent, v))\n                build(parent)\n\n    build(root)\n\n    # Generate DOT\n    dot = ['digraph G {']\n    dot.append('  rankdir=LR;')\n\n    for n in nodes:\n        label = f\"{n.data:.2f}\\\\ngrad={n.grad:.2f}\"\n        dot.append(f'  \"{id(n)}\" [label=\"{label}\", shape=box];')\n        if n._op:\n            op_id = f\"{id(n)}_op\"\n            dot.append(f'  \"{op_id}\" [label=\"{n._op}\", shape=circle];')\n            dot.append(f'  \"{op_id}\" -&gt; \"{id(n)}\";')\n            for p in n._parents:\n                dot.append(f'  \"{id(p)}\" -&gt; \"{op_id}\";')\n\n    dot.append('}')\n    return '\\n'.join(dot)\n</code></pre>"},{"location":"stages/stage-02/06-autograd-from-scratch/#summary","title":"Summary","text":"Component Purpose <code>Value</code> class Wraps scalar, stores gradient, tracks parents <code>_backward</code> closure Computes local gradient contribution Operator overloading Makes <code>+</code>, <code>*</code>, etc. build the graph Topological sort Ensures correct backward pass order Gradient accumulation Handles values used multiple times <p>Key insights:</p> <ol> <li>Closures are key: The <code>_backward</code> function captures forward-pass values</li> <li>The graph builds itself: Operators automatically record dependencies</li> <li>One backward pass: Computes all gradients efficiently</li> <li>It's just the chain rule: Each node multiplies incoming gradient by local gradient</li> </ol>"},{"location":"stages/stage-02/06-autograd-from-scratch/#exercises","title":"Exercises","text":"<ol> <li> <p>Add more operations: Implement <code>sin</code>, <code>cos</code>, and <code>sqrt</code> for the Value class.</p> </li> <li> <p>Gradient checking: For f(x) = x\u00b3, verify that your autograd gives the same answer as the numerical approximation (f(x+h) - f(x-h)) / 2h.</p> </li> <li> <p>Memory investigation: What happens to memory usage if you run many forward passes without calling backward? Why?</p> </li> <li> <p>Batch gradient descent: Modify the training loop to use mini-batches instead of the full dataset.</p> </li> <li> <p>Visualization: Use the <code>draw_graph</code> function to visualize the computation graph for a simple neural network.</p> </li> <li> <p>Cross-entropy loss: Implement the cross-entropy loss function using our Value operations.</p> </li> </ol>"},{"location":"stages/stage-02/06-autograd-from-scratch/#whats-next","title":"What's Next","text":"<p>We have a working autograd system. But how do we know it's correct?</p> <p>In Section 2.7, we'll cover testing and validation: - Numerical gradient checking - Unit tests for each operation - Property-based testing for compositions - Comparison with PyTorch</p> <p>Robust testing is essential\u2014gradient bugs are subtle and can make training fail in mysterious ways.</p>"},{"location":"stages/stage-02/07-testing-validation/","title":"Section 2.7: Testing and Validating Your Autograd","text":"<p>Building an autograd system is one thing. Trusting it is another.</p> <p>Gradient bugs are among the most insidious in machine learning. Your code runs, produces numbers, and even seems to learn\u2014but if gradients are slightly wrong, training fails in mysterious ways or converges to suboptimal solutions.</p> <p>This section covers rigorous testing strategies to ensure your autograd is correct.</p>"},{"location":"stages/stage-02/07-testing-validation/#the-fundamental-test-numerical-gradient-checking","title":"The Fundamental Test: Numerical Gradient Checking","text":"<p>The gold standard for testing gradients is comparison with numerical approximations.</p>"},{"location":"stages/stage-02/07-testing-validation/#the-central-difference-formula","title":"The Central Difference Formula","text":"<p>For any differentiable function f, the derivative can be approximated as:</p> \\[f'(x) \\approx \\frac{f(x + h) - f(x - h)}{2h}\\] <p>This is the central difference formula. It's more accurate than the forward difference (f(x+h) - f(x))/h because errors cancel.</p>"},{"location":"stages/stage-02/07-testing-validation/#why-it-works","title":"Why It Works","text":"<p>Taylor series expansion:</p> \\[f(x + h) = f(x) + f'(x)h + \\frac{f''(x)}{2}h^2 + O(h^3)$$ $$f(x - h) = f(x) - f'(x)h + \\frac{f''(x)}{2}h^2 + O(h^3)\\] <p>Subtracting: $\\(f(x + h) - f(x - h) = 2f'(x)h + O(h^3)\\)$</p> <p>So: $\\(\\frac{f(x + h) - f(x - h)}{2h} = f'(x) + O(h^2)\\)$</p> <p>The error is O(h\u00b2), meaning it shrinks quadratically as h decreases.</p>"},{"location":"stages/stage-02/07-testing-validation/#implementation","title":"Implementation","text":"<pre><code>def numerical_gradient(f, x, h=1e-5):\n    \"\"\"Compute numerical gradient of f at x using central differences.\"\"\"\n    return (f(x + h) - f(x - h)) / (2 * h)\n</code></pre>"},{"location":"stages/stage-02/07-testing-validation/#testing-a-single-operation","title":"Testing a Single Operation","text":"<pre><code>def test_multiply_gradient():\n    \"\"\"Test that multiplication gradient is correct.\"\"\"\n    x = Value(2.0)\n    y = Value(3.0)\n\n    # Compute analytical gradient\n    z = x * y\n    z.backward()\n\n    # Compute numerical gradient for x\n    def f_x(val):\n        return val * y.data\n    numerical_x = numerical_gradient(f_x, x.data)\n\n    # Compute numerical gradient for y\n    def f_y(val):\n        return x.data * val\n    numerical_y = numerical_gradient(f_y, y.data)\n\n    # Compare\n    assert abs(x.grad - numerical_x) &lt; 1e-5, f\"x.grad={x.grad}, numerical={numerical_x}\"\n    assert abs(y.grad - numerical_y) &lt; 1e-5, f\"y.grad={y.grad}, numerical={numerical_y}\"\n\n    print(\"Multiply gradient test passed!\")\n</code></pre>"},{"location":"stages/stage-02/07-testing-validation/#choosing-the-right-h","title":"Choosing the Right h","text":"<p>The step size h involves a tradeoff: - Too large: Approximation error dominates - Too small: Floating-point rounding error dominates</p> <p>For float64 (standard Python floats), h \u2248 1e-5 to 1e-7 is usually good.</p>"},{"location":"stages/stage-02/07-testing-validation/#relative-error","title":"Relative Error","text":"<p>For comparing gradients, use relative error to handle different magnitudes:</p> <pre><code>def relative_error(computed, numerical):\n    \"\"\"Compute relative error between computed and numerical gradients.\"\"\"\n    numerator = abs(computed - numerical)\n    denominator = max(abs(computed), abs(numerical), 1e-8)\n    return numerator / denominator\n\ndef check_gradient(computed, numerical, tolerance=1e-5):\n    \"\"\"Check if computed gradient matches numerical approximation.\"\"\"\n    rel_err = relative_error(computed, numerical)\n    return rel_err &lt; tolerance\n</code></pre>"},{"location":"stages/stage-02/07-testing-validation/#testing-complex-expressions","title":"Testing Complex Expressions","text":"<p>For expressions involving multiple operations, we test the entire composition:</p> <pre><code>def test_complex_expression():\n    \"\"\"Test gradient through a complex expression.\"\"\"\n    def make_computation(x_val, y_val):\n        x = Value(x_val)\n        y = Value(y_val)\n        z = (x * y + x) ** 2 - y.tanh()\n        return z, x, y\n\n    # Get analytical gradients\n    z, x, y = make_computation(2.0, 3.0)\n    z.backward()\n\n    # Numerical gradient for x\n    def f_x(val):\n        z, _, _ = make_computation(val, 3.0)\n        return z.data\n    numerical_x = numerical_gradient(f_x, 2.0)\n\n    # Numerical gradient for y\n    def f_y(val):\n        z, _, _ = make_computation(2.0, val)\n        return z.data\n    numerical_y = numerical_gradient(f_y, 3.0)\n\n    # Compare\n    assert check_gradient(x.grad, numerical_x), \\\n        f\"x gradient mismatch: {x.grad} vs {numerical_x}\"\n    assert check_gradient(y.grad, numerical_y), \\\n        f\"y gradient mismatch: {y.grad} vs {numerical_y}\"\n\n    print(\"Complex expression test passed!\")\n</code></pre>"},{"location":"stages/stage-02/07-testing-validation/#comprehensive-unit-tests","title":"Comprehensive Unit Tests","text":"<p>Every operation needs its own test:</p> <pre><code>import unittest\n\nclass TestAutograd(unittest.TestCase):\n\n    def test_add(self):\n        x = Value(2.0)\n        y = Value(3.0)\n        z = x + y\n        z.backward()\n\n        self.assertAlmostEqual(z.data, 5.0)\n        self.assertAlmostEqual(x.grad, 1.0)\n        self.assertAlmostEqual(y.grad, 1.0)\n\n    def test_multiply(self):\n        x = Value(2.0)\n        y = Value(3.0)\n        z = x * y\n        z.backward()\n\n        self.assertAlmostEqual(z.data, 6.0)\n        self.assertAlmostEqual(x.grad, 3.0)  # dz/dx = y\n        self.assertAlmostEqual(y.grad, 2.0)  # dz/dy = x\n\n    def test_power(self):\n        x = Value(3.0)\n        z = x ** 2\n        z.backward()\n\n        self.assertAlmostEqual(z.data, 9.0)\n        self.assertAlmostEqual(x.grad, 6.0)  # dz/dx = 2x\n\n    def test_division(self):\n        x = Value(6.0)\n        y = Value(2.0)\n        z = x / y\n        z.backward()\n\n        self.assertAlmostEqual(z.data, 3.0)\n        self.assertAlmostEqual(x.grad, 0.5)    # dz/dx = 1/y\n        self.assertAlmostEqual(y.grad, -1.5)   # dz/dy = -x/y\u00b2\n\n    def test_relu_positive(self):\n        x = Value(3.0)\n        z = x.relu()\n        z.backward()\n\n        self.assertAlmostEqual(z.data, 3.0)\n        self.assertAlmostEqual(x.grad, 1.0)\n\n    def test_relu_negative(self):\n        x = Value(-3.0)\n        z = x.relu()\n        z.backward()\n\n        self.assertAlmostEqual(z.data, 0.0)\n        self.assertAlmostEqual(x.grad, 0.0)\n\n    def test_tanh(self):\n        import math\n        x = Value(1.0)\n        z = x.tanh()\n        z.backward()\n\n        expected_data = math.tanh(1.0)\n        expected_grad = 1 - expected_data ** 2\n\n        self.assertAlmostEqual(z.data, expected_data)\n        self.assertAlmostEqual(x.grad, expected_grad)\n\n    def test_exp(self):\n        import math\n        x = Value(2.0)\n        z = x.exp()\n        z.backward()\n\n        expected = math.exp(2.0)\n        self.assertAlmostEqual(z.data, expected)\n        self.assertAlmostEqual(x.grad, expected)\n\n    def test_log(self):\n        import math\n        x = Value(2.0)\n        z = x.log()\n        z.backward()\n\n        self.assertAlmostEqual(z.data, math.log(2.0))\n        self.assertAlmostEqual(x.grad, 0.5)  # d/dx(ln x) = 1/x\n\nif __name__ == '__main__':\n    unittest.main()\n</code></pre>"},{"location":"stages/stage-02/07-testing-validation/#testing-gradient-accumulation","title":"Testing Gradient Accumulation","text":"<p>A critical test: when a variable is used multiple times, its gradient should accumulate:</p> <pre><code>def test_gradient_accumulation():\n    \"\"\"Verify gradients accumulate when variable used multiple times.\"\"\"\n    x = Value(3.0)\n    y = x + x  # x used twice\n\n    y.backward()\n\n    assert x.grad == 2.0, f\"Expected 2.0, got {x.grad}\"\n    print(\"Gradient accumulation test passed!\")\n\ndef test_multiple_paths():\n    \"\"\"Verify gradients sum over multiple paths.\"\"\"\n    x = Value(2.0)\n    y = Value(3.0)\n\n    a = x * y  # path 1: x contributes via multiplication\n    b = x + y  # path 2: x contributes via addition\n    z = a + b\n\n    z.backward()\n\n    # dz/dx = da/dx + db/dx = y + 1 = 3 + 1 = 4\n    assert x.grad == 4.0, f\"Expected 4.0, got {x.grad}\"\n    # dz/dy = da/dy + db/dy = x + 1 = 2 + 1 = 3\n    assert y.grad == 3.0, f\"Expected 3.0, got {y.grad}\"\n\n    print(\"Multiple paths test passed!\")\n</code></pre>"},{"location":"stages/stage-02/07-testing-validation/#property-based-testing","title":"Property-Based Testing","text":"<p>Instead of testing specific values, test properties that should always hold:</p>"},{"location":"stages/stage-02/07-testing-validation/#property-1-chain-rule-composition","title":"Property 1: Chain Rule Composition","text":"<p>For any f and g, (f\u2218g)'(x) = f'(g(x)) \u00b7 g'(x)</p> <pre><code>import random\n\ndef test_chain_rule_property():\n    \"\"\"Test that chain rule holds for composed operations.\"\"\"\n    for _ in range(100):  # Random testing\n        x_val = random.uniform(-5, 5)\n        if abs(x_val) &lt; 0.1:  # Avoid near-zero for log/division\n            continue\n\n        x = Value(x_val)\n\n        # Compose: tanh(x\u00b2)\n        z = (x ** 2).tanh()\n        z.backward()\n\n        # Numerical check\n        def f(v):\n            return math.tanh(v ** 2)\n        numerical = numerical_gradient(f, x_val)\n\n        assert check_gradient(x.grad, numerical), \\\n            f\"Chain rule failed at x={x_val}: {x.grad} vs {numerical}\"\n\n    print(\"Chain rule property test passed!\")\n</code></pre>"},{"location":"stages/stage-02/07-testing-validation/#property-2-linearity-of-gradient","title":"Property 2: Linearity of Gradient","text":"<p>\u2202(af + bg)/\u2202x = a\u00b7\u2202f/\u2202x + b\u00b7\u2202g/\u2202x</p> <pre><code>def test_linearity_property():\n    \"\"\"Test that gradient is linear in the output.\"\"\"\n    for _ in range(100):\n        x_val = random.uniform(-5, 5)\n        a = random.uniform(-3, 3)\n        b = random.uniform(-3, 3)\n\n        # Compute gradients separately\n        x1 = Value(x_val)\n        f = x1 ** 2\n        f.backward()\n        grad_f = x1.grad\n\n        x2 = Value(x_val)\n        g = x2 ** 3\n        g.backward()\n        grad_g = x2.grad\n\n        # Compute gradient of linear combination\n        x3 = Value(x_val)\n        h = a * (x3 ** 2) + b * (x3 ** 3)\n        h.backward()\n        grad_h = x3.grad\n\n        # Should match\n        expected = a * grad_f + b * grad_g\n        assert check_gradient(grad_h, expected), \\\n            f\"Linearity failed: {grad_h} vs {expected}\"\n\n    print(\"Linearity property test passed!\")\n</code></pre>"},{"location":"stages/stage-02/07-testing-validation/#property-3-zero-gradient-at-stationary-points","title":"Property 3: Zero Gradient at Stationary Points","text":"<p>Where f'(x) = 0, our autograd should give 0:</p> <pre><code>def test_stationary_points():\n    \"\"\"Test gradient is zero at known stationary points.\"\"\"\n    # x\u00b2 has stationary point at x=0\n    x = Value(0.0)\n    z = x ** 2\n    z.backward()\n    assert abs(x.grad) &lt; 1e-10, f\"Expected 0 gradient at x=0, got {x.grad}\"\n\n    # sin(x) has stationary points at x = \u03c0/2, 3\u03c0/2, ...\n    x = Value(math.pi / 2)\n    z = Value(math.sin(x.data))  # Note: need to implement sin properly\n    # For now, we can test tanh which has gradient \u2192 0 for large |x|\n    x = Value(10.0)\n    z = x.tanh()\n    z.backward()\n    assert abs(x.grad) &lt; 0.01, f\"tanh gradient should be ~0 for large x, got {x.grad}\"\n\n    print(\"Stationary points test passed!\")\n</code></pre>"},{"location":"stages/stage-02/07-testing-validation/#comparison-with-pytorch","title":"Comparison with PyTorch","text":"<p>The ultimate validation: compare with a production framework.</p> <pre><code>def test_against_pytorch():\n    \"\"\"Compare our gradients with PyTorch's.\"\"\"\n    import torch\n\n    # Test case: complex expression\n    x_val, y_val = 2.0, 3.0\n\n    # Our implementation\n    x = Value(x_val)\n    y = Value(y_val)\n    z = (x * y + x ** 2).tanh() + y.exp()\n    z.backward()\n    our_x_grad = x.grad\n    our_y_grad = y.grad\n\n    # PyTorch\n    x_torch = torch.tensor(x_val, requires_grad=True)\n    y_torch = torch.tensor(y_val, requires_grad=True)\n    z_torch = torch.tanh(x_torch * y_torch + x_torch ** 2) + torch.exp(y_torch)\n    z_torch.backward()\n    torch_x_grad = x_torch.grad.item()\n    torch_y_grad = y_torch.grad.item()\n\n    # Compare\n    assert check_gradient(our_x_grad, torch_x_grad), \\\n        f\"x gradient mismatch: ours={our_x_grad}, torch={torch_x_grad}\"\n    assert check_gradient(our_y_grad, torch_y_grad), \\\n        f\"y gradient mismatch: ours={our_y_grad}, torch={torch_y_grad}\"\n\n    print(\"PyTorch comparison test passed!\")\n</code></pre>"},{"location":"stages/stage-02/07-testing-validation/#testing-neural-networks","title":"Testing Neural Networks","text":"<p>For neural networks, we need to test that training actually works:</p> <pre><code>def test_learning():\n    \"\"\"Test that a network can learn a simple function.\"\"\"\n    import random\n    random.seed(42)\n\n    # Create simple dataset: y = x\u00b2\n    X = [[x] for x in [0.0, 0.5, 1.0, 1.5, 2.0]]\n    y = [x[0] ** 2 for x in X]\n\n    # Simple network: 1 -&gt; 8 -&gt; 1\n    model = MLP(1, [8, 1])\n\n    # Initial loss\n    initial_predictions = [model(x) for x in X]\n    initial_loss = sum((pred - target) ** 2\n                       for pred, target in zip(initial_predictions, y))\n\n    # Train\n    for _ in range(200):\n        predictions = [model(x) for x in X]\n        loss = sum((pred - target) ** 2 for pred, target in zip(predictions, y))\n\n        for p in model.parameters():\n            p.grad = 0.0\n        loss.backward()\n\n        for p in model.parameters():\n            p.data -= 0.01 * p.grad\n\n    # Final loss should be much lower\n    final_predictions = [model(x) for x in X]\n    final_loss = sum((pred - target) ** 2\n                     for pred, target in zip(final_predictions, y))\n\n    assert final_loss.data &lt; initial_loss.data * 0.1, \\\n        f\"Training didn't reduce loss enough: {initial_loss.data} -&gt; {final_loss.data}\"\n\n    print(\"Learning test passed!\")\n</code></pre>"},{"location":"stages/stage-02/07-testing-validation/#edge-cases-and-corner-cases","title":"Edge Cases and Corner Cases","text":"<p>Don't forget boundary conditions:</p> <pre><code>def test_edge_cases():\n    \"\"\"Test edge cases that might break.\"\"\"\n\n    # Division by small number\n    x = Value(1.0)\n    y = Value(1e-8)\n    z = x / y\n    z.backward()\n    assert not math.isnan(x.grad), \"Gradient became NaN\"\n    assert not math.isinf(x.grad), \"Gradient became infinite\"\n\n    # ReLU at zero (subgradient)\n    x = Value(0.0)\n    z = x.relu()\n    z.backward()\n    assert x.grad == 0.0, \"ReLU gradient at 0 should be 0\"\n\n    # Very deep composition (test for stack overflow)\n    x = Value(1.0)\n    z = x\n    for _ in range(100):\n        z = z * Value(0.99) + Value(0.001)\n    z.backward()\n    assert not math.isnan(x.grad), \"Deep composition produced NaN\"\n\n    # Zero gradient when output doesn't depend on input\n    x = Value(2.0)\n    y = Value(3.0)\n    z = y ** 2  # z doesn't depend on x\n    z.backward()\n    assert x.grad == 0.0, \"Gradient should be 0 for unused variable\"\n\n    print(\"Edge cases test passed!\")\n</code></pre>"},{"location":"stages/stage-02/07-testing-validation/#debugging-gradient-issues","title":"Debugging Gradient Issues","text":"<p>When tests fail, here's how to debug:</p>"},{"location":"stages/stage-02/07-testing-validation/#1-print-the-graph","title":"1. Print the Graph","text":"<pre><code>def debug_print(v, depth=0):\n    \"\"\"Print the computation graph for debugging.\"\"\"\n    indent = \"  \" * depth\n    print(f\"{indent}{v._op or 'input'}: data={v.data:.4f}, grad={v.grad:.4f}\")\n    for p in v._parents:\n        debug_print(p, depth + 1)\n</code></pre>"},{"location":"stages/stage-02/07-testing-validation/#2-check-intermediate-gradients","title":"2. Check Intermediate Gradients","text":"<pre><code>def test_with_intermediate_checks():\n    x = Value(2.0)\n    y = Value(3.0)\n\n    a = x * y\n    print(f\"a = x*y = {a.data}\")\n\n    b = a + x\n    print(f\"b = a+x = {b.data}\")\n\n    b.backward()\n\n    print(f\"b.grad = {b.grad} (should be 1)\")\n    print(f\"a.grad = {a.grad} (should be 1)\")\n    print(f\"x.grad = {x.grad} (should be y+1 = 4)\")\n    print(f\"y.grad = {y.grad} (should be x = 2)\")\n</code></pre>"},{"location":"stages/stage-02/07-testing-validation/#3-isolate-the-failing-operation","title":"3. Isolate the Failing Operation","text":"<p>If a complex test fails, binary search to find which operation is wrong:</p> <pre><code># Test each operation in isolation\ntest_add()\ntest_multiply()\ntest_power()\n# etc.\n\n# Then test pairs\ntest_add_then_multiply()\ntest_multiply_then_add()\n# etc.\n</code></pre>"},{"location":"stages/stage-02/07-testing-validation/#summary","title":"Summary","text":"Testing Strategy What It Catches Numerical gradient check Wrong gradient formulas Unit tests Individual operation bugs Accumulation tests += vs = bugs Property tests Systematic errors PyTorch comparison Complex interaction bugs Edge case tests Numerical instability Learning tests End-to-end failures <p>Key insights:</p> <ol> <li>Trust but verify: Always compare with numerical gradients</li> <li>Test the edges: Zero, very large, very small values</li> <li>Test compositions: Individual ops can be correct but compose wrongly</li> <li>Test learning: The ultimate test is whether training works</li> </ol>"},{"location":"stages/stage-02/07-testing-validation/#a-complete-test-suite","title":"A Complete Test Suite","text":"<pre><code>def run_all_tests():\n    \"\"\"Run complete autograd test suite.\"\"\"\n    print(\"Running autograd test suite...\\n\")\n\n    # Unit tests\n    test_multiply_gradient()\n    test_gradient_accumulation()\n    test_multiple_paths()\n    test_complex_expression()\n\n    # Property tests\n    test_chain_rule_property()\n    test_linearity_property()\n    test_stationary_points()\n\n    # Edge cases\n    test_edge_cases()\n\n    # Learning test\n    test_learning()\n\n    # PyTorch comparison (if available)\n    try:\n        test_against_pytorch()\n    except ImportError:\n        print(\"Skipping PyTorch comparison (not installed)\")\n\n    print(\"\\n\u2713 All tests passed!\")\n\nif __name__ == '__main__':\n    run_all_tests()\n</code></pre>"},{"location":"stages/stage-02/07-testing-validation/#exercises","title":"Exercises","text":"<ol> <li> <p>Find the bug: This backward function for multiply has a subtle bug. Find it:    <pre><code>def _backward():\n    self.grad = out.grad * other.data  # Bug!\n    other.grad = out.grad * self.data\n</code></pre></p> </li> <li> <p>Test coverage: Write a test that would catch the bug in exercise 1.</p> </li> <li> <p>Numerical stability: For what range of x values does <code>x.exp()</code> give reasonable gradients? Write a test to find the limits.</p> </li> <li> <p>Random testing: Implement a function that generates random computational graphs and tests them automatically.</p> </li> <li> <p>Gradient checking function: Write a general-purpose <code>check_gradients(f, inputs)</code> function that numerically verifies all gradients.</p> </li> </ol>"},{"location":"stages/stage-02/07-testing-validation/#whats-next","title":"What's Next","text":"<p>We now have: - \u2713 Mathematical foundations (Sections 2.1-2.3) - \u2713 Computational graphs (Section 2.4) - \u2713 Forward vs reverse mode (Section 2.5) - \u2713 Working implementation (Section 2.6) - \u2713 Testing framework (Section 2.7)</p> <p>In Stage 3, we'll build on this foundation to implement our first complete language model\u2014using the autograd system we just built to train it from scratch.</p>"},{"location":"stages/stage-03/","title":"Stage 3: Neural Language Models","text":""},{"location":"stages/stage-03/#from-counting-to-learning-building-your-first-neural-language-model","title":"From Counting to Learning: Building Your First Neural Language Model","text":"<p>In Stage 1, we built language models by counting n-grams. In Stage 2, we built the automatic differentiation system that enables neural networks to learn. Now we bring them together.</p> <p>Stage 3 builds a complete neural language model from scratch\u2014using only the autograd system we developed ourselves. No PyTorch, no TensorFlow. Just first principles.</p>"},{"location":"stages/stage-03/#what-well-build","title":"What We'll Build","text":"<p>A character-level neural language model that:</p> <ul> <li>Learns continuous representations (embeddings) of characters</li> <li>Uses feed-forward neural networks to predict the next character</li> <li>Trains via gradient descent using our own autograd</li> <li>Outperforms our Stage 1 Markov baselines</li> </ul>"},{"location":"stages/stage-03/#sections","title":"Sections","text":""},{"location":"stages/stage-03/#31-why-neural-the-limits-of-counting","title":"3.1: Why Neural? The Limits of Counting","text":"<p>The curse of dimensionality makes n-grams fundamentally limited. We need a new approach.</p>"},{"location":"stages/stage-03/#32-embeddings-from-discrete-to-continuous","title":"3.2: Embeddings \u2014 From Discrete to Continuous","text":"<p>How to represent characters as vectors. The key insight that enables neural language modeling.</p>"},{"location":"stages/stage-03/#33-feed-forward-neural-networks","title":"3.3: Feed-Forward Neural Networks","text":"<p>Building blocks of deep learning: linear layers, activations, and the universal approximation theorem.</p>"},{"location":"stages/stage-03/#34-cross-entropy-loss-and-maximum-likelihood","title":"3.4: Cross-Entropy Loss and Maximum Likelihood","text":"<p>Deriving the loss function from first principles. Proving it's equivalent to MLE from Stage 1.</p>"},{"location":"stages/stage-03/#35-building-a-character-level-neural-lm","title":"3.5: Building a Character-Level Neural LM","text":"<p>Complete implementation using our Stage 2 autograd. ~300 lines of code for a working language model.</p>"},{"location":"stages/stage-03/#36-training-dynamics","title":"3.6: Training Dynamics","text":"<p>Learning rates, initialization, batching, regularization. The art and science of making networks learn.</p>"},{"location":"stages/stage-03/#37-evaluation-and-comparison","title":"3.7: Evaluation and Comparison","text":"<p>Rigorous comparison with Stage 1 Markov models. Proving the neural advantage with numbers.</p>"},{"location":"stages/stage-03/#prerequisites","title":"Prerequisites","text":"<ul> <li>Stage 1: Probability, MLE, perplexity</li> <li>Stage 2: Derivatives, chain rule, autograd</li> </ul>"},{"location":"stages/stage-03/#key-takeaways","title":"Key Takeaways","text":"<p>By the end of this stage, you will understand:</p> <ol> <li>Why neural beats n-gram: Continuous representations enable generalization</li> <li>Embeddings deeply: How similar tokens get similar vectors automatically</li> <li>Network architecture: How layers combine to form universal function approximators</li> <li>Training from scratch: Gradient descent, learning rates, and regularization</li> <li>Empirical validation: How to properly compare models</li> </ol>"},{"location":"stages/stage-03/#the-journey-so-far","title":"The Journey So Far","text":"Stage Topic Key Insight 1 Markov Chains Language modeling = probability over sequences 2 Automatic Differentiation Gradients enable iterative learning 3 Neural Language Models Continuous representations beat discrete counting 4 (Coming) Recurrent networks for unbounded context"},{"location":"stages/stage-03/#lets-begin","title":"Let's Begin","text":"<p>We start by understanding exactly why counting-based models hit a wall, and how continuous representations offer a way forward.</p> <p>\u2192 Start with Section 3.1: Why Neural?</p>"},{"location":"stages/stage-03/01-why-neural/","title":"Section 3.1: Why Neural? The Limits of Counting","text":"<p>In Stage 1, we built language models by counting. Given enough data, counting works perfectly\u2014maximum likelihood estimation gives us optimal probability estimates.</p> <p>So why do we need neural networks?</p> <p>The answer is simple: we never have enough data.</p> <p>This section explores the fundamental limitations of count-based models and why continuous representations offer a solution.</p>"},{"location":"stages/stage-03/01-why-neural/#the-curse-of-dimensionality","title":"The Curse of Dimensionality","text":"<p>Consider modeling the probability of the next character given the previous 10 characters.</p>"},{"location":"stages/stage-03/01-why-neural/#how-many-contexts-are-there","title":"How Many Contexts Are There?","text":"<p>With a vocabulary of |V| = 100 characters (letters, digits, punctuation, space):</p> \\[\\text{Number of possible 10-character contexts} = 100^{10} = 10^{20}\\] <p>That's 100 quintillion possible contexts.</p>"},{"location":"stages/stage-03/01-why-neural/#how-much-data-do-we-have","title":"How Much Data Do We Have?","text":"<p>Wikipedia contains roughly 4 billion characters. Even if we used all of Wikipedia: - We'd see each specific 10-character context at most a few times - Most contexts would never appear - For unseen contexts, our count-based model gives probability 0 or falls back to shorter contexts</p>"},{"location":"stages/stage-03/01-why-neural/#the-exponential-gap","title":"The Exponential Gap","text":"Context Length Possible Contexts Data Needed for Coverage 2 10,000 ~10,000 5 10^10 ~10^10 10 10^20 ~10^20 20 10^40 Impossible <p>This is the curse of dimensionality: the number of possible configurations grows exponentially with context length, while our data grows at best linearly.</p>"},{"location":"stages/stage-03/01-why-neural/#the-problem-of-sparsity","title":"The Problem of Sparsity","text":""},{"location":"stages/stage-03/01-why-neural/#what-happens-in-practice","title":"What Happens in Practice","text":"<p>Let's revisit our Stage 1 experience:</p> <pre><code>Order 1 model: Seen most bigrams, works okay\nOrder 2 model: Many trigrams unseen\nOrder 5 model: Most 6-grams never appear in training\n</code></pre> <p>For a 5-gram model on typical training data: - ~90% of test 5-grams are unseen in training - Model must constantly back off to shorter contexts - The \"5-gram\" model effectively becomes a mixture of lower-order models</p>"},{"location":"stages/stage-03/01-why-neural/#smoothing-helps-but-not-enough","title":"Smoothing Helps, But Not Enough","text":"<p>In Stage 1, we applied Laplace smoothing:</p> \\[P(w | c) = \\frac{\\text{count}(c, w) + 1}{\\text{count}(c) + |V|}\\] <p>This prevents zero probabilities, but: - Assigns equal probability to all unseen continuations - \"the cat sat\" and \"the xyz sat\" get the same smoothed probability - No notion of similarity between contexts</p>"},{"location":"stages/stage-03/01-why-neural/#the-key-insight-contexts-are-not-independent","title":"The Key Insight: Contexts Are Not Independent","text":"<p>Here's what count-based models miss: similar contexts should give similar predictions.</p>"},{"location":"stages/stage-03/01-why-neural/#an-example","title":"An Example","text":"<p>Consider these contexts: 1. \"the cat sat on the\" 2. \"the dog sat on the\" 3. \"a cat sat upon the\"</p> <p>A human knows these should give similar next-character predictions. But to a count-based model: - These are three completely independent entries in a lookup table - Learning from (1) tells us nothing about (2) or (3) - Each must be learned separately</p>"},{"location":"stages/stage-03/01-why-neural/#why-this-matters","title":"Why This Matters","text":"<p>Real language has structure: - \"cat\" and \"dog\" are both animals - \"sat\" and \"slept\" are both past tense verbs - \"on\" and \"upon\" serve similar grammatical roles</p> <p>This structure means we don't need to see every possible combination\u2014we need to learn the underlying patterns.</p>"},{"location":"stages/stage-03/01-why-neural/#what-we-need-generalization","title":"What We Need: Generalization","text":"<p>The core problem is generalization: using what we've learned to handle situations we haven't seen.</p>"},{"location":"stages/stage-03/01-why-neural/#count-based-generalization","title":"Count-Based Generalization","text":"<p>N-gram models generalize via: 1. Backoff: If we haven't seen the long context, use a shorter one 2. Interpolation: Mix predictions from different context lengths</p> <p>These methods share statistical strength across: - \"the cat sat\" \u2192 \"the cat\" \u2192 \"cat\"</p> <p>But NOT across: - \"the cat\" and \"the dog\" (completely independent)</p>"},{"location":"stages/stage-03/01-why-neural/#what-we-want","title":"What We Want","text":"<p>We want a model where: $\\(\\text{similarity}(\\text{\"the cat\"}, \\text{\"the dog\"}) &gt; 0\\)$</p> <p>And this similarity should affect predictions: - If we learn that \"sat\" often follows \"the cat\" - We should automatically infer that \"sat\" might follow \"the dog\"</p>"},{"location":"stages/stage-03/01-why-neural/#the-solution-continuous-representations","title":"The Solution: Continuous Representations","text":"<p>The key insight of neural language models:</p> <p>Replace discrete symbols with continuous vectors.</p>"},{"location":"stages/stage-03/01-why-neural/#discrete-vs-continuous","title":"Discrete vs Continuous","text":"Discrete (N-gram) Continuous (Neural) \"cat\" = entry #742 in table \"cat\" = [0.2, -0.5, 0.8, ...] \u2208 \u211d^d Two words: same or different Two words: distance in vector space Similarity undefined Similarity = cosine, Euclidean, etc. No interpolation possible Smooth interpolation natural"},{"location":"stages/stage-03/01-why-neural/#why-continuous-helps","title":"Why Continuous Helps","text":"<p>In a continuous space: 1. Similar words \u2192 similar vectors: \"cat\" and \"dog\" are nearby 2. Similar contexts \u2192 similar predictions: Neural network outputs vary smoothly 3. Generalization is automatic: Learning about nearby points affects the whole region</p>"},{"location":"stages/stage-03/01-why-neural/#the-mathematical-shift","title":"The Mathematical Shift","text":"<p>N-gram: $\\(P(w | c) = \\text{table}[c][w]\\)$</p> <p>Neural: $\\(P(w | c) = f_\\theta(\\text{embed}(c))\\)$</p> <p>Where: - embed(c) maps discrete context to continuous vector - f_\u03b8 is a smooth function (neural network) - \u03b8 are learnable parameters</p>"},{"location":"stages/stage-03/01-why-neural/#parameter-efficiency","title":"Parameter Efficiency","text":""},{"location":"stages/stage-03/01-why-neural/#n-gram-parameter-count","title":"N-gram Parameter Count","text":"<p>For vocabulary |V| and context length k: - Need to store P(w | c) for every (c, w) pair - Parameters: O(|V|^{k+1})</p> <p>For |V| = 10,000 and k = 5: 10,000^6 = 10^24 parameters. Impossible.</p>"},{"location":"stages/stage-03/01-why-neural/#neural-parameter-count","title":"Neural Parameter Count","text":"<p>For embedding dimension d and hidden size h: - Embedding matrix: |V| \u00d7 d - Hidden layers: O(h \u00d7 k \u00d7 d) - Output layer: O(h \u00d7 |V|) - Total: O(|V| \u00d7 d + h \u00d7 |V|)</p> <p>For |V| = 10,000, d = 256, h = 512: - Embeddings: 2.56M - Output: 5.12M - Hidden: ~0.5M - Total: ~8M parameters</p> <p>From 10^24 to 8 million: a reduction by a factor of 10^17!</p>"},{"location":"stages/stage-03/01-why-neural/#the-trade-off","title":"The Trade-off","text":""},{"location":"stages/stage-03/01-why-neural/#what-we-gain","title":"What We Gain","text":"<ul> <li>Massive parameter reduction</li> <li>Automatic generalization</li> <li>Smooth predictions</li> <li>Ability to use long contexts</li> </ul>"},{"location":"stages/stage-03/01-why-neural/#what-we-lose","title":"What We Lose","text":"<ul> <li>Exact match: N-gram gives exact counts</li> <li>Simplicity: no optimization needed for N-gram</li> <li>Interpretability: can inspect count tables directly</li> <li>Speed: N-gram lookup is O(1)</li> </ul>"},{"location":"stages/stage-03/01-why-neural/#when-each-wins","title":"When Each Wins","text":"Scenario Better Choice Abundant data, short context N-gram Limited data, long context Neural Requires exact memorization N-gram Requires generalization Neural Interpretability critical N-gram Performance critical Neural <p>In practice, modern language models use neural approaches almost universally\u2014the generalization advantage is too significant.</p>"},{"location":"stages/stage-03/01-why-neural/#a-concrete-comparison","title":"A Concrete Comparison","text":"<p>Let's quantify the difference with a thought experiment.</p>"},{"location":"stages/stage-03/01-why-neural/#the-task","title":"The Task","text":"<p>Predict the next character in English text with context length 10.</p>"},{"location":"stages/stage-03/01-why-neural/#n-gram-approach","title":"N-gram Approach","text":"<ul> <li>Collect all 11-grams from training data</li> <li>Compute conditional probabilities</li> <li>Apply smoothing</li> </ul> <p>Result: Most test 11-grams unseen. Heavy reliance on backoff. Effective context often only 2-3 characters.</p>"},{"location":"stages/stage-03/01-why-neural/#neural-approach","title":"Neural Approach","text":"<ul> <li>Learn 10 character embeddings (10 \u00d7 d parameters)</li> <li>Process with neural network</li> <li>Output probability over vocabulary</li> </ul> <p>Result: Every test context gets a meaningful prediction based on its pattern, not just exact matches.</p>"},{"location":"stages/stage-03/01-why-neural/#the-difference-in-action","title":"The Difference in Action","text":"<p>Training data includes: - \"the cat sat on the mat\"</p> <p>Test context: - \"the dog sat on the \"</p> <p>N-gram: Never seen \"the dog sat on the \" \u2192 backs off to \"the \" \u2192 poor prediction</p> <p>Neural: \"dog\" embedding \u2248 \"cat\" embedding \u2192 similar hidden state \u2192 similar prediction to \"cat\" case \u2192 correctly predicts space or common next words</p>"},{"location":"stages/stage-03/01-why-neural/#the-path-forward","title":"The Path Forward","text":"<p>In the remaining sections of Stage 3, we'll build this from scratch:</p> <ol> <li>Section 3.2: Embeddings\u2014how to represent tokens as vectors</li> <li>Section 3.3: Neural networks\u2014the function that maps embeddings to predictions</li> <li>Section 3.4: Cross-entropy loss\u2014how to train the network</li> <li>Section 3.5: Implementation\u2014building it with our Stage 2 autograd</li> <li>Section 3.6: Training dynamics\u2014making it learn effectively</li> <li>Section 3.7: Evaluation\u2014comparing to our Stage 1 baselines</li> </ol>"},{"location":"stages/stage-03/01-why-neural/#summary","title":"Summary","text":"Concept N-gram Limitation Neural Solution Sparsity Most contexts unseen Continuous representations Generalization No similarity notion Embeddings capture similarity Parameters Exponential in context Linear in vocabulary Long context Backs off constantly Uses full context <p>Key insight: The curse of dimensionality makes count-based models fundamentally limited. Continuous representations offer a way out by enabling generalization: learning from one context transfers to similar contexts.</p>"},{"location":"stages/stage-03/01-why-neural/#exercises","title":"Exercises","text":"<ol> <li> <p>Count the sparsity: For a corpus of 1 million characters and vocabulary of 80 characters, estimate what fraction of possible 5-grams appear in the data.</p> </li> <li> <p>Similarity intuition: List 5 pairs of words that should have similar embeddings and 5 pairs that should have dissimilar embeddings. Explain your reasoning.</p> </li> <li> <p>Parameter comparison: For vocabulary size V = 50,000, context length k = 10, embedding dimension d = 512, and hidden size h = 1024, calculate the parameter count for both N-gram and neural approaches.</p> </li> <li> <p>Backoff analysis: If an N-gram model backs off from order 5 to order 3 on average, what effective context length is it using? How does this compare to a neural model?</p> </li> <li> <p>Thought experiment: Describe a scenario where an N-gram model might outperform a neural model. What properties of the data would make counting more effective than learning?</p> </li> </ol>"},{"location":"stages/stage-03/01-why-neural/#whats-next","title":"What's Next","text":"<p>We've established why we need neural models. The first step is learning how to represent discrete tokens\u2014characters or words\u2014as continuous vectors.</p> <p>In Section 3.2, we'll dive deep into embeddings: the foundation that makes neural language models possible.</p>"},{"location":"stages/stage-03/02-embeddings/","title":"Section 3.2: Embeddings \u2014 From Discrete to Continuous","text":"<p>The fundamental problem: language is made of discrete symbols (characters, words), but neural networks operate on continuous vectors.</p> <p>Embeddings bridge this gap. They convert discrete tokens into continuous representations that capture meaning.</p> <p>This section derives embeddings from first principles and shows why they're the key to neural language modeling.</p>"},{"location":"stages/stage-03/02-embeddings/#the-problem-with-discrete-representations","title":"The Problem with Discrete Representations","text":""},{"location":"stages/stage-03/02-embeddings/#one-hot-encoding","title":"One-Hot Encoding","text":"<p>The naive way to represent tokens: one-hot vectors.</p> <p>For vocabulary {a, b, c, d} with |V| = 4:</p> \\[\\text{one\\_hot}(a) = [1, 0, 0, 0]$$ $$\\text{one\\_hot}(b) = [0, 1, 0, 0]$$ $$\\text{one\\_hot}(c) = [0, 0, 1, 0]$$ $$\\text{one\\_hot}(d) = [0, 0, 0, 1]\\] <p>Each token gets a unique position; all other entries are zero.</p>"},{"location":"stages/stage-03/02-embeddings/#why-one-hot-fails","title":"Why One-Hot Fails","text":"<p>Problem 1: No Similarity</p> <p>For any two different tokens i \u2260 j: $\\(\\text{one\\_hot}(i) \\cdot \\text{one\\_hot}(j) = 0\\)$</p> <p>All tokens are orthogonal. \"a\" and \"b\" are as dissimilar as \"a\" and \"7\".</p> <p>Problem 2: Dimensionality</p> <p>Vector size = vocabulary size.</p> <p>For characters: manageable (~100 dimensions) For words: massive (~50,000+ dimensions)</p> <p>Problem 3: No Generalization</p> <p>A weight connecting to one-hot position i affects ONLY token i.</p> <p>Learning about \"cat\" provides zero information about \"dog\".</p>"},{"location":"stages/stage-03/02-embeddings/#the-geometric-view","title":"The Geometric View","text":"<p>In one-hot space: - All vectors have length 1 - All pairs are distance \u221a2 apart (since ||e_i - e_j||\u00b2 = 2 for i \u2260 j) - No structure, no clusters, no relationships</p> <p>This is a failure of representation, not of the neural network itself.</p>"},{"location":"stages/stage-03/02-embeddings/#the-embedding-solution","title":"The Embedding Solution","text":""},{"location":"stages/stage-03/02-embeddings/#the-key-idea","title":"The Key Idea","text":"<p>Instead of one-hot vectors, represent each token as a learned dense vector.</p> <p>For vocabulary size |V| and embedding dimension d (where d &lt;&lt; |V|):</p> \\[E \\in \\mathbb{R}^{|V| \\times d}\\] <p>Each row of E is the embedding for one token: $\\(\\text{embed}(i) = E[i, :] \\in \\mathbb{R}^d\\)$</p>"},{"location":"stages/stage-03/02-embeddings/#example","title":"Example","text":"<p>For vocabulary {a, b, c, d} with d = 3:</p> \\[E = \\begin{bmatrix} 0.2 &amp; -0.5 &amp; 0.1 \\\\ 0.3 &amp; -0.4 &amp; 0.2 \\\\ -0.1 &amp; 0.8 &amp; 0.3 \\\\ 0.5 &amp; 0.2 &amp; -0.7 \\end{bmatrix}\\] <p>Then: - embed(a) = [0.2, -0.5, 0.1] - embed(b) = [0.3, -0.4, 0.2] - embed(c) = [-0.1, 0.8, 0.3] - embed(d) = [0.5, 0.2, -0.7]</p>"},{"location":"stages/stage-03/02-embeddings/#what-changes","title":"What Changes?","text":"Property One-Hot Embedding Dimension V Similarity All orthogonal Learned from data Parameters 0 (fixed) Structure None Emerges from training"},{"location":"stages/stage-03/02-embeddings/#mathematical-formulation","title":"Mathematical Formulation","text":""},{"location":"stages/stage-03/02-embeddings/#embeddings-as-matrix-lookup","title":"Embeddings as Matrix Lookup","text":"<p>The embedding operation is mathematically simple:</p> \\[\\text{embed}(i) = E[i, :]\\] <p>This is equivalent to: $\\(\\text{embed}(i) = \\text{one\\_hot}(i) \\cdot E\\)$</p> <p>Proof:</p> <p>Let e_i = one_hot(i) be the i-th standard basis vector.</p> \\[e_i \\cdot E = [0, ..., 1, ..., 0] \\cdot E = \\text{i-th row of } E = E[i, :]\\] <p>The one-hot vector \"selects\" the corresponding row of E.</p>"},{"location":"stages/stage-03/02-embeddings/#why-this-matters-for-backpropagation","title":"Why This Matters for Backpropagation","text":"<p>During training, we need gradients of the loss with respect to E.</p> <p>If the loss is L and we used embedding E[i, :] in the forward pass:</p> \\[\\frac{\\partial L}{\\partial E[i, :]} = \\frac{\\partial L}{\\partial \\text{embed}(i)}\\] <p>Only the i-th row of E gets a gradient\u2014the rows for tokens not used in this example get zero gradient.</p> <p>This is sparse gradient updates: each training example only modifies the embeddings of tokens it contains.</p>"},{"location":"stages/stage-03/02-embeddings/#efficient-implementation","title":"Efficient Implementation","text":"<p>Although mathematically equivalent to matrix multiplication, we implement embeddings as table lookup:</p> <pre><code>class Embedding:\n    def __init__(self, vocab_size, embed_dim):\n        # Initialize randomly\n        self.weight = [[random.gauss(0, 0.1) for _ in range(embed_dim)]\n                       for _ in range(vocab_size)]\n\n    def forward(self, token_idx):\n        # O(1) lookup, not O(vocab_size) multiplication\n        return self.weight[token_idx]\n</code></pre> <p>This is O(d) instead of O(|V| \u00d7 d)\u2014a massive speedup.</p>"},{"location":"stages/stage-03/02-embeddings/#why-embeddings-enable-generalization","title":"Why Embeddings Enable Generalization","text":""},{"location":"stages/stage-03/02-embeddings/#the-distributional-hypothesis","title":"The Distributional Hypothesis","text":"<p>Linguist J.R. Firth (1957):</p> <p>\"You shall know a word by the company it keeps.\"</p> <p>Tokens that appear in similar contexts should have similar meanings.</p>"},{"location":"stages/stage-03/02-embeddings/#how-training-enforces-this","title":"How Training Enforces This","text":"<p>Consider training on: - \"the cat sat on the mat\" - \"the dog sat on the rug\"</p> <p>Both \"cat\" and \"dog\": 1. Follow \"the\" 2. Precede \"sat\" 3. Appear in similar grammatical positions</p> <p>During training: - Both receive gradients pushing them to predict \"sat\" - Both receive gradients from \"the\" predictions - These similar gradient signals push them toward similar embeddings</p> <p>Result: embed(\"cat\") \u2248 embed(\"dog\") emerges automatically!</p>"},{"location":"stages/stage-03/02-embeddings/#formal-statement","title":"Formal Statement","text":"<p>If two tokens t\u2081 and t\u2082 appear in similar contexts:</p> \\[P(\\text{context} | t_1) \\approx P(\\text{context} | t_2)\\] <p>Then training will push their embeddings together:</p> \\[\\text{embed}(t_1) \\approx \\text{embed}(t_2)\\] <p>This is not programmed\u2014it's a consequence of the optimization objective.</p>"},{"location":"stages/stage-03/02-embeddings/#the-geometry-of-embedding-space","title":"The Geometry of Embedding Space","text":""},{"location":"stages/stage-03/02-embeddings/#distances-encode-relationships","title":"Distances Encode Relationships","text":"<p>In a trained embedding space:</p> <p>Cosine Similarity: $\\(\\text{sim}(u, v) = \\frac{u \\cdot v}{||u|| \\cdot ||v||}\\)$</p> <p>Ranges from -1 (opposite) to +1 (identical direction).</p> <p>Euclidean Distance: $\\(d(u, v) = ||u - v||_2\\)$</p> <p>Small distance = similar tokens.</p>"},{"location":"stages/stage-03/02-embeddings/#what-emerges","title":"What Emerges","text":"<p>After training on language data, embedding spaces exhibit:</p> <ol> <li>Clusters: Similar tokens group together</li> <li>Vowels cluster, consonants cluster</li> <li>Digits cluster together</li> <li> <p>Punctuation forms its own region</p> </li> <li> <p>Directions: Some directions encode specific properties</p> </li> <li>Moving in one direction might shift from \"noun-like\" to \"verb-like\"</li> <li> <p>Another direction might encode frequency</p> </li> <li> <p>Linear Relationships (for word embeddings):</p> </li> <li>king - man + woman \u2248 queen</li> <li>Paris - France + Italy \u2248 Rome</li> </ol> <p>For character-level models, the structure is simpler but still meaningful.</p>"},{"location":"stages/stage-03/02-embeddings/#embedding-dimensions","title":"Embedding Dimensions","text":""},{"location":"stages/stage-03/02-embeddings/#how-many-dimensions","title":"How Many Dimensions?","text":"<p>This is a hyperparameter choice. Common values:</p> Token Type Vocabulary Size Typical Embedding Dim Characters 50-100 16-64 Subwords 30,000-50,000 256-768 Words 50,000-100,000 100-300"},{"location":"stages/stage-03/02-embeddings/#trade-offs","title":"Trade-offs","text":"<p>Too few dimensions (d too small): - Can't capture all distinctions - Different tokens forced to share representation - Underfitting</p> <p>Too many dimensions (d too large): - More parameters to learn - Risk of overfitting - Diminishing returns</p>"},{"location":"stages/stage-03/02-embeddings/#the-right-number","title":"The Right Number","text":"<p>Rule of thumb: d should be large enough to express the meaningful distinctions in your vocabulary.</p> <p>For characters (~100 tokens): d = 32-64 usually sufficient For words (~50,000 tokens): d = 256-512 often needed</p> <p>A common heuristic: d \u2248 |V|^{1/4} (fourth root of vocabulary size).</p>"},{"location":"stages/stage-03/02-embeddings/#context-representation","title":"Context Representation","text":"<p>For language modeling, we need to represent not just one token, but a sequence of context tokens.</p>"},{"location":"stages/stage-03/02-embeddings/#concatenation","title":"Concatenation","text":"<p>The simplest approach: concatenate embeddings.</p> <p>For context [t\u2081, t\u2082, t\u2083] with embedding dimension d:</p> \\[x = [\\text{embed}(t_1); \\text{embed}(t_2); \\text{embed}(t_3)] \\in \\mathbb{R}^{3d}\\] <p>This preserves position information: - Dimensions [0:d] represent first position - Dimensions [d:2d] represent second position - Dimensions [2d:3d] represent third position</p>"},{"location":"stages/stage-03/02-embeddings/#why-concatenation","title":"Why Concatenation?","text":"<p>Position matters in language: - \"dog bites man\" \u2260 \"man bites dog\"</p> <p>Concatenation lets the network learn position-specific patterns.</p>"},{"location":"stages/stage-03/02-embeddings/#alternative-addition-not-for-us","title":"Alternative: Addition (Not for Us)","text":"<p>Some architectures add embeddings instead:</p> \\[x = \\text{embed}(t_1) + \\text{embed}(t_2) + \\text{embed}(t_3)\\] <p>This loses position information but reduces dimensionality.</p> <p>We'll revisit this trade-off when we study attention mechanisms in Stage 7.</p>"},{"location":"stages/stage-03/02-embeddings/#implementing-embeddings-with-autograd","title":"Implementing Embeddings with Autograd","text":"<p>Using our Stage 2 Value class, here's a simple embedding layer:</p> <pre><code>class Embedding:\n    def __init__(self, vocab_size, embed_dim):\n        # Each row is a token's embedding\n        self.weight = [\n            [Value(random.gauss(0, 0.1)) for _ in range(embed_dim)]\n            for _ in range(vocab_size)\n        ]\n\n    def __call__(self, token_idx):\n        \"\"\"Return the embedding for token_idx.\"\"\"\n        return self.weight[token_idx]  # List of Value objects\n\n    def parameters(self):\n        \"\"\"Return all learnable parameters.\"\"\"\n        return [v for row in self.weight for v in row]\n</code></pre>"},{"location":"stages/stage-03/02-embeddings/#gradient-flow","title":"Gradient Flow","text":"<p>When we use an embedding in a forward pass:</p> <pre><code># Get embedding for token 5\nemb = embedding(5)  # List of Value objects\n\n# Use in computation\nhidden = sum(w * e for w, e in zip(weights, emb))\nloss = ...\nloss.backward()\n\n# Now embedding.weight[5] has gradients!\nfor i, v in enumerate(embedding.weight[5]):\n    print(f\"Gradient for dim {i}: {v.grad}\")\n</code></pre> <p>The gradient flows from the loss back through the computation to the specific embedding row that was used.</p>"},{"location":"stages/stage-03/02-embeddings/#the-embedding-matrix-as-a-learned-dictionary","title":"The Embedding Matrix as a Learned Dictionary","text":"<p>Another perspective: the embedding matrix E is a dictionary mapping tokens to their \"meanings\" (as vectors).</p> Token Index Traditional Dictionary Embedding \"Dictionary\" 0 ('a') \"First letter of alphabet\" [0.23, -0.15, 0.82, ...] 1 ('b') \"Second letter\" [0.31, -0.22, 0.71, ...] ... ... ... <p>The difference: the vector \"definitions\" are: - Learned from data, not written by humans - Optimized for the prediction task - Capture statistical patterns, not explicit semantics</p>"},{"location":"stages/stage-03/02-embeddings/#initialization-matters","title":"Initialization Matters","text":""},{"location":"stages/stage-03/02-embeddings/#why-initialization","title":"Why Initialization?","text":"<p>Before training, embeddings must start somewhere. The initialization affects: - How quickly training converges - Whether training gets stuck in poor local minima - The scale of activations in early layers</p>"},{"location":"stages/stage-03/02-embeddings/#common-initializations","title":"Common Initializations","text":"<p>Random Normal: $\\(E[i,j] \\sim \\mathcal{N}(0, \\sigma^2)\\)$</p> <p>With \u03c3 typically 0.01 to 0.1.</p> <p>Uniform: $\\(E[i,j] \\sim \\text{Uniform}(-a, a)\\)$</p> <p>With a typically 0.05 to 0.5.</p> <p>Xavier/Glorot (common for neural networks): $\\(E[i,j] \\sim \\mathcal{N}\\left(0, \\frac{1}{d}\\right)\\)$</p> <p>This keeps variance stable across layers.</p>"},{"location":"stages/stage-03/02-embeddings/#our-choice","title":"Our Choice","text":"<p>For character embeddings with d = 32:</p> \\[E[i,j] \\sim \\mathcal{N}\\left(0, 0.1\\right)\\] <p>Small random values that will be refined by training.</p>"},{"location":"stages/stage-03/02-embeddings/#summary","title":"Summary","text":"Concept Explanation One-hot Sparse, high-dimensional, no similarity Embedding Dense, low-dimensional, learned similarity Embedding matrix E \u2208 \u211d^{ Lookup operation embed(i) = E[i,:], O(d) not O( Why it works Similar contexts \u2192 similar gradients \u2192 similar embeddings Geometry Clusters, directions, linear relationships Context representation Concatenate embeddings, preserves position <p>Key insight: Embeddings convert the discrete symbol problem into a continuous optimization problem. Similar tokens end up with similar embeddings because they receive similar training signals. This is the foundation of neural language modeling.</p>"},{"location":"stages/stage-03/02-embeddings/#exercises","title":"Exercises","text":"<ol> <li> <p>One-hot distances: Prove that for any two different one-hot vectors, the Euclidean distance is \u221a2.</p> </li> <li> <p>Embedding equivalence: Show that one_hot(i) \u00b7 E = E[i,:] by writing out the matrix multiplication explicitly for a 3\u00d72 embedding matrix.</p> </li> <li> <p>Parameter count: For a vocabulary of 10,000 tokens and embedding dimension 256, how many parameters are in the embedding layer? How does this compare to one-hot dimension?</p> </li> <li> <p>Gradient sparsity: If a training batch contains 32 examples, each with 10 tokens, at most how many rows of the embedding matrix receive non-zero gradients?</p> </li> <li> <p>Similarity emergence: Describe a minimal training scenario (contexts and next tokens) that would cause embeddings for 'x' and 'y' to become similar.</p> </li> </ol>"},{"location":"stages/stage-03/02-embeddings/#whats-next","title":"What's Next","text":"<p>We can now represent tokens as continuous vectors. But how do we use these vectors to predict the next token?</p> <p>In Section 3.3, we'll build feed-forward neural networks that transform embedded contexts into probability distributions over the vocabulary.</p>"},{"location":"stages/stage-03/03-feed-forward/","title":"Section 3.3: Feed-Forward Neural Networks","text":"<p>We can now represent tokens as continuous vectors. The next step: build a function that transforms these vectors into predictions.</p> <p>Feed-forward neural networks (also called multi-layer perceptrons or MLPs) are the simplest such functions. They're the building blocks of all modern deep learning.</p> <p>This section derives neural networks from first principles and explains what makes them powerful.</p>"},{"location":"stages/stage-03/03-feed-forward/#the-basic-unit-a-linear-layer","title":"The Basic Unit: A Linear Layer","text":""},{"location":"stages/stage-03/03-feed-forward/#mathematical-definition","title":"Mathematical Definition","text":"<p>A linear layer transforms an input vector x \u2208 \u211d\u207f into an output vector y \u2208 \u211d\u1d50:</p> \\[y = Wx + b\\] <p>Where: - W \u2208 \u211d\u1d50\u02e3\u207f is the weight matrix - b \u2208 \u211d\u1d50 is the bias vector - x \u2208 \u211d\u207f is the input - y \u2208 \u211d\u1d50 is the output</p>"},{"location":"stages/stage-03/03-feed-forward/#what-it-computes","title":"What It Computes","text":"<p>Each output component y\u1d62 is a weighted sum of inputs plus a bias:</p> \\[y_i = \\sum_{j=1}^{n} W_{ij} x_j + b_i\\] <p>This is a linear combination of the inputs.</p>"},{"location":"stages/stage-03/03-feed-forward/#geometric-interpretation","title":"Geometric Interpretation","text":"<p>A linear layer performs: 1. Rotation/scaling: W rotates and scales the input space 2. Translation: b shifts the result</p> <p>It maps the input space to a new space with potentially different dimensionality.</p>"},{"location":"stages/stage-03/03-feed-forward/#example","title":"Example","text":"<p>Input: x = [2, 3] (n = 2) Output dimension: m = 3</p> \\[W = \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\\\ 1 &amp; 1 \\end{bmatrix}, \\quad b = \\begin{bmatrix} 0 \\\\ 1 \\\\ -1 \\end{bmatrix}\\] \\[y = \\begin{bmatrix} 1\u00b72 + 0\u00b73 + 0 \\\\ 0\u00b72 + 1\u00b73 + 1 \\\\ 1\u00b72 + 1\u00b73 - 1 \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ 4 \\\\ 4 \\end{bmatrix}\\]"},{"location":"stages/stage-03/03-feed-forward/#the-problem-linear-functions-are-limited","title":"The Problem: Linear Functions Are Limited","text":""},{"location":"stages/stage-03/03-feed-forward/#composition-of-linear-functions-is-linear","title":"Composition of Linear Functions is Linear","text":"<p>If f(x) = W\u2081x + b\u2081 and g(x) = W\u2082x + b\u2082, then:</p> \\[g(f(x)) = W_2(W_1 x + b_1) + b_2 = W_2 W_1 x + (W_2 b_1 + b_2)\\] <p>This is still a linear function! Let W' = W\u2082W\u2081 and b' = W\u2082b\u2081 + b\u2082:</p> \\[g(f(x)) = W' x + b'\\] <p>No matter how many linear layers we stack, we get a linear function.</p>"},{"location":"stages/stage-03/03-feed-forward/#why-this-is-a-problem","title":"Why This Is a Problem","text":"<p>Linear functions can only: - Draw straight decision boundaries - Compute linear combinations</p> <p>They cannot: - Compute XOR (or any non-linearly-separable function) - Model complex patterns - Learn hierarchical features</p>"},{"location":"stages/stage-03/03-feed-forward/#the-solution-nonlinear-activation-functions","title":"The Solution: Nonlinear Activation Functions","text":"<p>To break linearity, we apply a nonlinear function after each linear layer.</p>"},{"location":"stages/stage-03/03-feed-forward/#the-pattern","title":"The Pattern","text":"\\[h = \\sigma(Wx + b)\\] <p>Where \u03c3 is a nonlinear activation function applied element-wise.</p>"},{"location":"stages/stage-03/03-feed-forward/#common-activation-functions","title":"Common Activation Functions","text":"<p>ReLU (Rectified Linear Unit): $\\(\\text{ReLU}(x) = \\max(0, x)\\)$</p> <ul> <li>Simple and fast</li> <li>Derivative: 1 if x &gt; 0, else 0</li> <li>Most popular for hidden layers</li> </ul> <p>Sigmoid: $\\(\\sigma(x) = \\frac{1}{1 + e^{-x}}\\)$</p> <ul> <li>Outputs between 0 and 1</li> <li>Historically popular, less used now (vanishing gradients)</li> </ul> <p>Tanh: $\\(\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\\)$</p> <ul> <li>Outputs between -1 and 1</li> <li>Zero-centered (better than sigmoid)</li> </ul> <p>GELU (Gaussian Error Linear Unit): $\\(\\text{GELU}(x) = x \\cdot \\Phi(x)\\)$</p> <p>Where \u03a6 is the standard normal CDF. Approximation: $\\(\\text{GELU}(x) \\approx 0.5x(1 + \\tanh[\\sqrt{2/\\pi}(x + 0.044715x^3)])\\)$</p> <ul> <li>Used in transformers</li> <li>Smooth version of ReLU</li> </ul>"},{"location":"stages/stage-03/03-feed-forward/#relu-a-closer-look","title":"ReLU: A Closer Look","text":"<p>ReLU is piecewise linear: - For x \u2264 0: output = 0 - For x &gt; 0: output = x</p> <p>This simple nonlinearity is enough to enable universal approximation.</p> <p>Derivative: $\\(\\frac{d}{dx}\\text{ReLU}(x) = \\begin{cases} 1 &amp; \\text{if } x &gt; 0 \\\\ 0 &amp; \\text{if } x &lt; 0 \\\\ \\text{undefined} &amp; \\text{if } x = 0 \\end{cases}\\)$</p> <p>At x = 0, we use the subgradient 0.</p>"},{"location":"stages/stage-03/03-feed-forward/#building-deep-networks","title":"Building Deep Networks","text":""},{"location":"stages/stage-03/03-feed-forward/#stacking-layers","title":"Stacking Layers","text":"<p>A deep network is a composition of layers:</p> \\[h_1 = \\sigma(W_1 x + b_1)$$ $$h_2 = \\sigma(W_2 h_1 + b_2)$$ $$h_3 = \\sigma(W_3 h_2 + b_3)$$ $$\\vdots$$ $$y = W_L h_{L-1} + b_L\\] <p>Note: The last layer often has no activation (for regression) or a special activation (softmax for classification).</p>"},{"location":"stages/stage-03/03-feed-forward/#why-depth-matters","title":"Why Depth Matters","text":"<p>Each layer can learn increasingly abstract features:</p> <ul> <li>Layer 1: Low-level patterns (character combinations)</li> <li>Layer 2: Mid-level patterns (syllables, common sequences)</li> <li>Layer 3: High-level patterns (word-like structures)</li> </ul> <p>Deep networks can express functions that would require exponentially wide shallow networks.</p>"},{"location":"stages/stage-03/03-feed-forward/#width-vs-depth-trade-off","title":"Width vs Depth Trade-off","text":"Architecture Advantages Disadvantages Wide + Shallow Easy to train, stable Many parameters, limited abstraction Narrow + Deep Parameter efficient, hierarchical Harder to train, vanishing gradients <p>In practice, moderate depth (2-4 layers for character LM) works well.</p>"},{"location":"stages/stage-03/03-feed-forward/#the-universal-approximation-theorem","title":"The Universal Approximation Theorem","text":""},{"location":"stages/stage-03/03-feed-forward/#statement","title":"Statement","text":"<p>A feed-forward network with: - One hidden layer - Sufficient width (number of neurons) - Nonlinear activation</p> <p>Can approximate any continuous function on a compact domain to arbitrary precision.</p>"},{"location":"stages/stage-03/03-feed-forward/#what-this-means","title":"What This Means","text":"<p>Neural networks are universal function approximators. Given enough neurons, they can learn any reasonable input-output mapping.</p>"},{"location":"stages/stage-03/03-feed-forward/#what-this-doesnt-mean","title":"What This Doesn't Mean","text":"<ul> <li>Doesn't say how many neurons needed (could be enormous)</li> <li>Doesn't say the function is easy to find (optimization might fail)</li> <li>Doesn't guarantee generalization (might overfit)</li> </ul>"},{"location":"stages/stage-03/03-feed-forward/#intuition-via-relu","title":"Intuition via ReLU","text":"<p>A ReLU network creates a piecewise linear function:</p> <ul> <li>Each neuron contributes a \"hinge\" point</li> <li>With enough hinges, any continuous curve can be approximated</li> <li>More neurons = finer approximation</li> </ul> <pre><code>                      /\\\n                     /  \\\n        /\\          /    \\\n       /  \\        /      \\___\n    __/    \\______/\n</code></pre> <p>Each change in slope corresponds to a neuron \"turning on\" or \"off\".</p>"},{"location":"stages/stage-03/03-feed-forward/#the-output-layer-for-language-modeling","title":"The Output Layer for Language Modeling","text":"<p>For language modeling, we need to output a probability distribution over the vocabulary.</p>"},{"location":"stages/stage-03/03-feed-forward/#the-softmax-function","title":"The Softmax Function","text":"<p>Given logits z \u2208 \u211d^|V|, softmax converts to probabilities:</p> \\[\\text{softmax}(z)_i = \\frac{e^{z_i}}{\\sum_{j=1}^{|V|} e^{z_j}}\\]"},{"location":"stages/stage-03/03-feed-forward/#properties-of-softmax","title":"Properties of Softmax","text":"<ol> <li> <p>Valid probability distribution:    $\\(\\sum_i \\text{softmax}(z)_i = 1\\)$</p> </li> <li> <p>All positive:    $\\(\\text{softmax}(z)_i &gt; 0 \\quad \\forall i\\)$</p> </li> <li> <p>Monotonic:    Higher logit \u2192 higher probability</p> </li> <li> <p>Differentiable:    Smooth gradients everywhere</p> </li> </ol>"},{"location":"stages/stage-03/03-feed-forward/#the-complete-output-stage","title":"The Complete Output Stage","text":"\\[\\text{logits} = W_{\\text{out}} h_L + b_{\\text{out}}$$ $$P(\\text{token} | \\text{context}) = \\text{softmax}(\\text{logits})\\] <p>Where: - h_L \u2208 \u211d^h is the final hidden state - W_out \u2208 \u211d^{|V|\u00d7h} projects to vocabulary size - logits \u2208 \u211d^|V| are unnormalized scores - softmax normalizes to probabilities</p>"},{"location":"stages/stage-03/03-feed-forward/#putting-it-together-the-full-architecture","title":"Putting It Together: The Full Architecture","text":"<p>For a character-level language model:</p>"},{"location":"stages/stage-03/03-feed-forward/#forward-pass","title":"Forward Pass","text":"<pre><code>Input: context = [c_{t-k}, ..., c_{t-1}]  (k previous characters)\n\n1. EMBED: For each position i:\n   e_i = E[c_{t-k+i}]  \u2208 \u211d^d\n\n2. CONCATENATE:\n   x = [e_0; e_1; ...; e_{k-1}]  \u2208 \u211d^{k\u00b7d}\n\n3. HIDDEN LAYER 1:\n   h_1 = ReLU(W_1 x + b_1)  \u2208 \u211d^{h_1}\n\n4. HIDDEN LAYER 2:\n   h_2 = ReLU(W_2 h_1 + b_2)  \u2208 \u211d^{h_2}\n\n5. OUTPUT:\n   logits = W_out h_2 + b_out  \u2208 \u211d^{|V|}\n\n6. SOFTMAX:\n   P(c_t | context) = softmax(logits)  \u2208 \u211d^{|V|}\n</code></pre>"},{"location":"stages/stage-03/03-feed-forward/#parameter-count","title":"Parameter Count","text":"<p>For context length k, embedding dim d, hidden sizes h\u2081 and h\u2082, vocabulary |V|:</p> Component Parameters Embedding Layer 1 (k\u00b7d) \u00d7 h\u2081 + h\u2081 Layer 2 h\u2081 \u00d7 h\u2082 + h\u2082 Output h\u2082 \u00d7 Total <p>Example: |V| = 80, d = 32, k = 8, h\u2081 = h\u2082 = 128</p> <ul> <li>Embedding: 80 \u00d7 32 = 2,560</li> <li>Layer 1: 256 \u00d7 128 + 128 = 32,896</li> <li>Layer 2: 128 \u00d7 128 + 128 = 16,512</li> <li>Output: 128 \u00d7 80 + 80 = 10,320</li> </ul> <p>Total: ~62,000 parameters</p> <p>Compare to 5-gram: 80^6 \u2248 262 billion parameters. Neural wins by a factor of 4 million!</p>"},{"location":"stages/stage-03/03-feed-forward/#implementing-a-layer","title":"Implementing a Layer","text":"<p>Using our Stage 2 autograd:</p> <pre><code>class Linear:\n    \"\"\"A linear layer: y = Wx + b\"\"\"\n\n    def __init__(self, in_features, out_features):\n        # Xavier initialization\n        scale = (2.0 / (in_features + out_features)) ** 0.5\n        self.W = [[Value(random.gauss(0, scale))\n                   for _ in range(in_features)]\n                  for _ in range(out_features)]\n        self.b = [Value(0.0) for _ in range(out_features)]\n\n    def __call__(self, x):\n        \"\"\"x is a list of Values, returns list of Values.\"\"\"\n        out = []\n        for i in range(len(self.b)):\n            # Compute W[i] \u00b7 x + b[i]\n            activation = self.b[i]\n            for j in range(len(x)):\n                activation = activation + self.W[i][j] * x[j]\n            out.append(activation)\n        return out\n\n    def parameters(self):\n        return [w for row in self.W for w in row] + self.b\n</code></pre>"},{"location":"stages/stage-03/03-feed-forward/#relu-layer","title":"ReLU Layer","text":"<pre><code>def relu(x):\n    \"\"\"Apply ReLU to a list of Values.\"\"\"\n    return [v.relu() for v in x]\n</code></pre>"},{"location":"stages/stage-03/03-feed-forward/#softmax-layer","title":"Softmax Layer","text":"<pre><code>def softmax(logits):\n    \"\"\"Convert logits to probabilities.\"\"\"\n    # For numerical stability, subtract max\n    max_logit = max(v.data for v in logits)\n    exp_logits = [(v - max_logit).exp() for v in logits]\n    sum_exp = sum(exp_logits, Value(0.0))\n    return [e / sum_exp for e in exp_logits]\n</code></pre>"},{"location":"stages/stage-03/03-feed-forward/#numerical-stability-the-log-sum-exp-trick","title":"Numerical Stability: The Log-Sum-Exp Trick","text":""},{"location":"stages/stage-03/03-feed-forward/#the-problem","title":"The Problem","text":"<p>Softmax involves exponentials. For large logits:</p> <p>$\\(e^{100} \\approx 2.7 \\times 10^{43}\\)$ (overflow!) $\\(e^{-100} \\approx 3.7 \\times 10^{-44}\\)$ (underflow!)</p>"},{"location":"stages/stage-03/03-feed-forward/#the-solution","title":"The Solution","text":"<p>Softmax is invariant to constant shifts:</p> \\[\\text{softmax}(z - c) = \\text{softmax}(z)\\] <p>Proof: $\\(\\frac{e^{z_i - c}}{\\sum_j e^{z_j - c}} = \\frac{e^{z_i} e^{-c}}{\\sum_j e^{z_j} e^{-c}} = \\frac{e^{z_i}}{\\sum_j e^{z_j}}\\)$</p> <p>So we compute: $\\(\\text{softmax}(z)_i = \\frac{e^{z_i - \\max(z)}}{\\sum_j e^{z_j - \\max(z)}}\\)$</p> <p>After subtracting max, all exponents are \u2264 0, preventing overflow.</p>"},{"location":"stages/stage-03/03-feed-forward/#for-log-probabilities","title":"For Log Probabilities","text":"<p>We often need log softmax:</p> \\[\\log \\text{softmax}(z)_i = z_i - \\log\\sum_j e^{z_j}\\] <p>The log-sum-exp (LSE) function: $\\(\\text{LSE}(z) = \\log\\sum_j e^{z_j} = \\max(z) + \\log\\sum_j e^{z_j - \\max(z)}\\)$</p> <p>This is numerically stable.</p>"},{"location":"stages/stage-03/03-feed-forward/#information-flow-in-the-network","title":"Information Flow in the Network","text":""},{"location":"stages/stage-03/03-feed-forward/#forward-pass_1","title":"Forward Pass","text":"<p>Information flows from input to output:</p> <pre><code>Context \u2192 Embeddings \u2192 Concatenate \u2192 Hidden\u2081 \u2192 Hidden\u2082 \u2192 Logits \u2192 Softmax\n</code></pre> <p>Each layer transforms representations, adding capacity to model complex patterns.</p>"},{"location":"stages/stage-03/03-feed-forward/#backward-pass","title":"Backward Pass","text":"<p>Gradients flow from output to input (via backpropagation):</p> <pre><code>Loss \u2192 \u2202/\u2202Softmax \u2192 \u2202/\u2202Logits \u2192 \u2202/\u2202Hidden\u2082 \u2192 \u2202/\u2202Hidden\u2081 \u2192 \u2202/\u2202Embeddings\n</code></pre> <p>Every parameter receives a gradient signal indicating how to change to reduce loss.</p>"},{"location":"stages/stage-03/03-feed-forward/#the-chain-rule-at-work","title":"The Chain Rule at Work","text":"<p>For a parameter W\u2081[i,j] in the first layer:</p> \\[\\frac{\\partial L}{\\partial W_1[i,j]} = \\frac{\\partial L}{\\partial h_1[i]} \\cdot \\frac{\\partial h_1[i]}{\\partial W_1[i,j]}\\] <p>The gradient \"chains\" through all intermediate layers\u2014exactly what we built in Stage 2.</p>"},{"location":"stages/stage-03/03-feed-forward/#summary","title":"Summary","text":"Concept Description Linear layer y = Wx + b, affine transformation Activation Nonlinear function (ReLU, tanh, etc.) Deep network Composition of linear + activation Universal approximation Any function can be approximated Softmax Converts logits to probability distribution Log-sum-exp trick Numerically stable softmax computation <p>Key insight: A neural network is a composition of simple functions (linear + nonlinear). Each layer transforms representations, and the composition can approximate any function. Training adjusts the parameters so this composition predicts well.</p>"},{"location":"stages/stage-03/03-feed-forward/#exercises","title":"Exercises","text":"<ol> <li> <p>Linearity proof: Show that composing two linear functions y = Ax + a and z = By + b gives z = Cx + c where C = BA and c = Ba + b.</p> </li> <li> <p>ReLU universality: Sketch how a ReLU network with 4 neurons could approximate f(x) = |x| on [-1, 1].</p> </li> <li> <p>Parameter counting: For a network with input 256, hidden layers [512, 256, 128], and output 100, calculate the total number of parameters.</p> </li> <li> <p>Softmax verification: Verify that softmax([2, 1, 0]) sums to 1 by computing it explicitly.</p> </li> <li> <p>Stability test: Compute softmax([1000, 1001, 1002]) directly and with the max-subtraction trick. What happens in each case?</p> </li> </ol>"},{"location":"stages/stage-03/03-feed-forward/#whats-next","title":"What's Next","text":"<p>We have the architecture. But how do we train it?</p> <p>In Section 3.4, we'll derive the cross-entropy loss function\u2014the objective that tells the network how wrong its predictions are and how to improve.</p>"},{"location":"stages/stage-03/04-cross-entropy/","title":"Section 3.4: Cross-Entropy Loss and Maximum Likelihood","text":"<p>We have a neural network that outputs a probability distribution. But how do we train it?</p> <p>Cross-entropy loss is the answer. In this section, we'll derive it from first principles and prove it's equivalent to maximum likelihood estimation\u2014connecting back to Stage 1.</p>"},{"location":"stages/stage-03/04-cross-entropy/#the-training-problem","title":"The Training Problem","text":""},{"location":"stages/stage-03/04-cross-entropy/#what-we-have","title":"What We Have","text":"<p>A neural language model that computes:</p> \\[\\hat{P}(c_t | c_{t-k:t-1}; \\theta)\\] <p>Where: - \\(c_{t-k:t-1}\\) is the context (previous k characters) - \\(c_t\\) is the next character - \u03b8 are all the model parameters (embeddings, weights, biases) - \\(\\hat{P}\\) is the model's predicted probability distribution</p>"},{"location":"stages/stage-03/04-cross-entropy/#what-we-want","title":"What We Want","text":"<p>Parameters \u03b8* that make the model's predictions match the true data distribution as closely as possible.</p>"},{"location":"stages/stage-03/04-cross-entropy/#the-fundamental-question","title":"The Fundamental Question","text":"<p>How do we measure \"how wrong\" the model's predictions are?</p>"},{"location":"stages/stage-03/04-cross-entropy/#maximum-likelihood-the-principled-approach","title":"Maximum Likelihood: The Principled Approach","text":""},{"location":"stages/stage-03/04-cross-entropy/#from-stage-1","title":"From Stage 1","text":"<p>In Stage 1, we derived maximum likelihood estimation for n-gram models:</p> \\[\\theta^* = \\arg\\max_\\theta P(\\text{data} | \\theta)\\] <p>The optimal parameters are those that maximize the probability of the observed data.</p>"},{"location":"stages/stage-03/04-cross-entropy/#for-neural-language-models","title":"For Neural Language Models","text":"<p>The principle is exactly the same!</p> <p>Given training data \\(D = \\{(x_1, y_1), ..., (x_N, y_N)\\}\\) where: - \\(x_i\\) is the i-th context - \\(y_i\\) is the true next character</p> <p>The likelihood is:</p> \\[P(D | \\theta) = \\prod_{i=1}^{N} P(y_i | x_i; \\theta)\\] <p>(Assuming independence between examples.)</p>"},{"location":"stages/stage-03/04-cross-entropy/#log-likelihood","title":"Log-Likelihood","text":"<p>Products are numerically unstable and hard to differentiate. Take logarithms:</p> \\[\\log P(D | \\theta) = \\sum_{i=1}^{N} \\log P(y_i | x_i; \\theta)\\] <p>This is the log-likelihood.</p>"},{"location":"stages/stage-03/04-cross-entropy/#from-maximization-to-minimization","title":"From Maximization to Minimization","text":"<p>ML practitioners prefer minimizing a loss. Negating:</p> \\[L(\\theta) = -\\log P(D | \\theta) = -\\sum_{i=1}^{N} \\log P(y_i | x_i; \\theta)\\] <p>This is the negative log-likelihood.</p> <p>Minimizing NLL = Maximizing likelihood.</p>"},{"location":"stages/stage-03/04-cross-entropy/#average-loss","title":"Average Loss","text":"<p>For numerical stability and comparison across dataset sizes, use the average:</p> \\[L(\\theta) = -\\frac{1}{N}\\sum_{i=1}^{N} \\log P(y_i | x_i; \\theta)\\] <p>This is our training objective.</p>"},{"location":"stages/stage-03/04-cross-entropy/#cross-entropy-the-information-theory-view","title":"Cross-Entropy: The Information Theory View","text":""},{"location":"stages/stage-03/04-cross-entropy/#cross-entropy-definition","title":"Cross-Entropy Definition","text":"<p>Given true distribution p and model distribution q, the cross-entropy is:</p> \\[H(p, q) = -\\sum_{x} p(x) \\log q(x)\\] <p>It measures the expected number of bits needed to encode samples from p using a code optimized for q.</p>"},{"location":"stages/stage-03/04-cross-entropy/#for-language-modeling","title":"For Language Modeling","text":"<p>At each position in our data: - True distribution: p(c_t | context) = 1 if c_t is the actual next character, else 0 (one-hot) - Model distribution: \\(q(c) = \\hat{P}(c | \\text{context}; \\theta)\\)</p> <p>The cross-entropy at this position:</p> \\[H(p, q) = -\\sum_{c} p(c) \\log q(c)\\] <p>Since p(c) = 1 for c = y (the true character) and 0 otherwise:</p> \\[H(p, q) = -1 \\cdot \\log q(y) + \\sum_{c \\neq y} 0 \\cdot \\log q(c) = -\\log q(y)\\] <p>This is exactly the negative log-probability!</p>"},{"location":"stages/stage-03/04-cross-entropy/#the-equivalence","title":"The Equivalence","text":"<p>Cross-entropy loss per example = Negative log-likelihood per example.</p> <p>For the dataset:</p> \\[\\text{Cross-Entropy Loss} = -\\frac{1}{N}\\sum_{i=1}^{N} \\log \\hat{P}(y_i | x_i; \\theta)\\] <p>This is the same as average NLL!</p>"},{"location":"stages/stage-03/04-cross-entropy/#why-cross-entropy-is-the-right-loss","title":"Why Cross-Entropy Is the Right Loss","text":""},{"location":"stages/stage-03/04-cross-entropy/#reason-1-maximum-likelihood","title":"Reason 1: Maximum Likelihood","text":"<p>As shown, minimizing cross-entropy = maximizing likelihood.</p> <p>MLE has strong theoretical justification: - Consistent (converges to true parameters with infinite data) - Asymptotically efficient (lowest variance among consistent estimators) - Invariant under reparametrization</p>"},{"location":"stages/stage-03/04-cross-entropy/#reason-2-proper-scoring-rule","title":"Reason 2: Proper Scoring Rule","text":"<p>A scoring rule S(q, y) is proper if:</p> \\[\\mathbb{E}_{y \\sim p}[S(p, y)] \\leq \\mathbb{E}_{y \\sim p}[S(q, y)]\\] <p>For any distribution q. That is, the best expected score is achieved when q = p.</p> <p>Cross-entropy is proper.</p> <p>Proof:</p> <p>Expected cross-entropy when true distribution is p:</p> \\[\\mathbb{E}_{y \\sim p}[H(p, q)] = \\sum_y p(y) H(p, q) = -\\sum_y p(y) \\log q(y)\\] <p>This is minimized when q = p, giving the entropy H(p):</p> \\[H(p, p) = -\\sum_y p(y) \\log p(y) = H(p)\\] <p>For any q \u2260 p, we have H(p, q) &gt; H(p, p) by Gibbs' inequality.</p>"},{"location":"stages/stage-03/04-cross-entropy/#reason-3-information-theoretic-interpretation","title":"Reason 3: Information-Theoretic Interpretation","text":"<p>Cross-entropy H(p, q) measures the inefficiency of using code for q when the true distribution is p.</p> <p>Minimizing cross-entropy = finding the most efficient encoding of the data.</p>"},{"location":"stages/stage-03/04-cross-entropy/#reason-4-gradient-properties","title":"Reason 4: Gradient Properties","text":"<p>We'll see shortly that cross-entropy combined with softmax has remarkably clean gradients.</p>"},{"location":"stages/stage-03/04-cross-entropy/#the-loss-function-explicitly","title":"The Loss Function Explicitly","text":""},{"location":"stages/stage-03/04-cross-entropy/#for-a-single-example","title":"For a Single Example","text":"<p>Given context x and true next character y:</p> \\[L = -\\log \\hat{P}(y | x; \\theta)\\] <p>Recall that the model computes:</p> \\[\\hat{P}(c | x; \\theta) = \\text{softmax}(\\text{logits})_c = \\frac{e^{z_c}}{\\sum_{c'} e^{z_{c'}}}\\] <p>Where z are the logits (outputs of the final linear layer).</p> <p>Substituting:</p> \\[L = -\\log \\frac{e^{z_y}}{\\sum_c e^{z_c}}\\]"},{"location":"stages/stage-03/04-cross-entropy/#simplifying","title":"Simplifying","text":"\\[L = -\\log e^{z_y} + \\log \\sum_c e^{z_c}\\] \\[L = -z_y + \\log \\sum_c e^{z_c}\\] <p>This is the softmax log-likelihood formula.</p> <p>The second term is the log-sum-exp (LSE) function:</p> \\[\\text{LSE}(z) = \\log \\sum_c e^{z_c}\\]"},{"location":"stages/stage-03/04-cross-entropy/#for-a-batch","title":"For a Batch","text":"<p>For N examples:</p> \\[L = \\frac{1}{N}\\sum_{i=1}^{N} \\left( -z_{y_i}^{(i)} + \\text{LSE}(z^{(i)}) \\right)\\] <p>Where \\(z^{(i)}\\) are the logits for example i, and \\(y_i\\) is the true character index.</p>"},{"location":"stages/stage-03/04-cross-entropy/#computing-the-gradient","title":"Computing the Gradient","text":""},{"location":"stages/stage-03/04-cross-entropy/#why-we-need-the-gradient","title":"Why We Need the Gradient","text":"<p>To train via gradient descent, we need:</p> \\[\\frac{\\partial L}{\\partial \\theta}\\] <p>For every parameter \u03b8. Our Stage 2 autograd will compute this automatically, but understanding the gradient structure is valuable.</p>"},{"location":"stages/stage-03/04-cross-entropy/#gradient-wrt-logits","title":"Gradient w.r.t. Logits","text":"<p>The most important gradient: \u2202L/\u2202z.</p> <p>For a single example with true class y:</p> \\[L = -z_y + \\log \\sum_c e^{z_c}\\] <p>For the logit of the true class:</p> \\[\\frac{\\partial L}{\\partial z_y} = -1 + \\frac{e^{z_y}}{\\sum_c e^{z_c}} = -1 + \\hat{P}(y | x)\\] <p>For any other logit \\(z_c\\) where c \u2260 y:</p> \\[\\frac{\\partial L}{\\partial z_c} = 0 + \\frac{e^{z_c}}{\\sum_c e^{z_c}} = \\hat{P}(c | x)\\]"},{"location":"stages/stage-03/04-cross-entropy/#the-beautiful-result","title":"The Beautiful Result","text":"<p>For all classes c:</p> \\[\\frac{\\partial L}{\\partial z_c} = \\hat{P}(c | x) - \\delta_{cy}\\] <p>Where \\(\\delta_{cy}\\) is 1 if c = y (the true class), else 0.</p> <p>In vector form:</p> \\[\\frac{\\partial L}{\\partial z} = \\hat{p} - y_{\\text{one-hot}}\\] <p>The gradient is simply: predicted probability minus true probability!</p>"},{"location":"stages/stage-03/04-cross-entropy/#why-this-is-beautiful","title":"Why This Is Beautiful","text":"<ul> <li>Magnitude of gradient = how wrong the prediction is</li> <li>If \\(\\hat{P}(y) = 1\\): gradient is 0 (perfect prediction)</li> <li>If \\(\\hat{P}(y) = 0\\): gradient is -1 (maximally wrong)</li> <li>Automatically scaled by confidence</li> </ul> <p>This natural weighting makes gradient descent effective.</p>"},{"location":"stages/stage-03/04-cross-entropy/#deriving-the-softmax-crossentropy-gradient","title":"Deriving the Softmax-CrossEntropy Gradient","text":"<p>Let's prove the result rigorously.</p>"},{"location":"stages/stage-03/04-cross-entropy/#setup","title":"Setup","text":"<p>Given logits \\(z \\in \\mathbb{R}^{|V|}\\) and true class y:</p> \\[\\hat{P}(c | x) = \\frac{e^{z_c}}{\\sum_{c'} e^{z_{c'}}} = \\frac{e^{z_c}}{Z}\\] <p>Where \\(Z = \\sum_c e^{z_c}\\) (partition function).</p> <p>Loss: $\\(L = -\\log \\hat{P}(y | x) = -\\log e^{z_y} + \\log Z = -z_y + \\log Z\\)$</p>"},{"location":"stages/stage-03/04-cross-entropy/#derivative-of-partition-function","title":"Derivative of Partition Function","text":"\\[\\frac{\\partial}{\\partial z_c} \\log Z = \\frac{1}{Z} \\frac{\\partial Z}{\\partial z_c} = \\frac{1}{Z} e^{z_c} = \\hat{P}(c | x)\\]"},{"location":"stages/stage-03/04-cross-entropy/#derivative-of-loss-wrt-z_y","title":"Derivative of Loss w.r.t. z_y","text":"\\[\\frac{\\partial L}{\\partial z_y} = -1 + \\frac{\\partial \\log Z}{\\partial z_y} = -1 + \\hat{P}(y | x)\\]"},{"location":"stages/stage-03/04-cross-entropy/#derivative-wrt-other-logits","title":"Derivative w.r.t. Other Logits","text":"<p>For c \u2260 y:</p> \\[\\frac{\\partial L}{\\partial z_c} = 0 + \\frac{\\partial \\log Z}{\\partial z_c} = \\hat{P}(c | x)\\]"},{"location":"stages/stage-03/04-cross-entropy/#combined-result","title":"Combined Result","text":"\\[\\frac{\\partial L}{\\partial z_c} = \\begin{cases} \\hat{P}(y | x) - 1 &amp; \\text{if } c = y \\\\ \\hat{P}(c | x) &amp; \\text{if } c \\neq y \\end{cases}\\] <p>Which is exactly: $\\(\\frac{\\partial L}{\\partial z_c} = \\hat{P}(c | x) - \\delta_{cy}\\)$</p> <p>QED.</p>"},{"location":"stages/stage-03/04-cross-entropy/#connection-to-perplexity-from-stage-1","title":"Connection to Perplexity (from Stage 1)","text":""},{"location":"stages/stage-03/04-cross-entropy/#recall-perplexity","title":"Recall Perplexity","text":"<p>In Stage 1, we defined perplexity as:</p> \\[\\text{PPL} = \\exp\\left( -\\frac{1}{N}\\sum_{i=1}^{N} \\log P(w_i | w_{&lt;i}) \\right)\\]"},{"location":"stages/stage-03/04-cross-entropy/#the-relationship","title":"The Relationship","text":"<p>The exponent is exactly the average cross-entropy loss!</p> \\[\\text{PPL} = \\exp(L)\\] <p>Where L is the cross-entropy loss.</p>"},{"location":"stages/stage-03/04-cross-entropy/#why-this-matters","title":"Why This Matters","text":"<p>Training: minimize L (cross-entropy)</p> <p>Evaluation: report exp(L) (perplexity)</p> <p>Same underlying metric, different presentations: - L \u2208 [0, \u221e): additive, for optimization - PPL \u2208 [1, \u221e): interpretable, for humans</p>"},{"location":"stages/stage-03/04-cross-entropy/#interpreting-the-loss","title":"Interpreting the Loss","text":"<p>If L = 2.3: - PPL = exp(2.3) \u2248 10 - Model is \"as uncertain as picking uniformly among 10 options\" - Lower is better</p>"},{"location":"stages/stage-03/04-cross-entropy/#implementing-cross-entropy-loss","title":"Implementing Cross-Entropy Loss","text":"<p>Using our Stage 2 autograd:</p> <pre><code>def cross_entropy_loss(logits, target_idx):\n    \"\"\"\n    logits: list of Value objects (unnormalized scores)\n    target_idx: int (index of true class)\n\n    Returns: Value (scalar loss)\n    \"\"\"\n    # Log-sum-exp for numerical stability\n    max_logit = max(v.data for v in logits)\n\n    # Subtract max for stability (doesn't change result)\n    shifted = [v - max_logit for v in logits]\n\n    # exp of shifted logits\n    exp_logits = [v.exp() for v in shifted]\n\n    # Sum\n    sum_exp = sum(exp_logits, Value(0.0))\n\n    # Log-sum-exp\n    lse = sum_exp.log() + max_logit\n\n    # Loss = -logit[target] + lse\n    loss = lse - logits[target_idx]\n\n    return loss\n</code></pre>"},{"location":"stages/stage-03/04-cross-entropy/#why-logsumexp","title":"Why LogSumExp?","text":"<p>Direct computation \\(\\log(\\sum e^{z_i})\\) can overflow/underflow.</p> <p>Using LSE trick:</p> \\[\\log \\sum_i e^{z_i} = \\max(z) + \\log \\sum_i e^{z_i - \\max(z)}\\] <p>After subtracting max, all exponents are \u2264 0, preventing overflow.</p>"},{"location":"stages/stage-03/04-cross-entropy/#multiple-examples-batch-loss","title":"Multiple Examples: Batch Loss","text":"<p>For a batch of N examples:</p> <pre><code>def batch_cross_entropy(batch_logits, batch_targets):\n    \"\"\"\n    batch_logits: list of lists of Values\n    batch_targets: list of target indices\n\n    Returns: Value (average loss)\n    \"\"\"\n    losses = [cross_entropy_loss(logits, target)\n              for logits, target in zip(batch_logits, batch_targets)]\n\n    # Average\n    total = sum(losses, Value(0.0))\n    return total / len(losses)\n</code></pre>"},{"location":"stages/stage-03/04-cross-entropy/#summary","title":"Summary","text":"Concept Formula Interpretation MLE objective max log P(data|\u03b8) Most likely parameters NLL loss -log P(data|\u03b8) Minimize to maximize likelihood Cross-entropy -\u03a3 p log q Expected bits using wrong code Per-example loss -log P(y|x) Surprise at true outcome Softmax gradient \\(\\hat{p} - p\\) Predicted - true Perplexity exp(loss) Interpretable uncertainty <p>Key insights:</p> <ol> <li>Cross-entropy = MLE: Same theoretical foundation as Stage 1</li> <li>Proper scoring rule: Best achievable when q = p</li> <li>Beautiful gradient: \\(\\hat{p} - p\\) is simple and effective</li> <li>Connection to perplexity: exp(loss) gives interpretable metric</li> </ol>"},{"location":"stages/stage-03/04-cross-entropy/#exercises","title":"Exercises","text":"<ol> <li> <p>Verify equivalence: Show that for one-hot true distribution, H(p,q) = -log q(y).</p> </li> <li> <p>Compute loss: Given logits [2.0, 1.0, 3.0] and true class 2, compute the cross-entropy loss by hand.</p> </li> <li> <p>Gradient check: For the same logits and true class, compute \u2202L/\u2202z for each logit. Verify the formula.</p> </li> <li> <p>Perplexity: If cross-entropy loss is 1.5, what is the perplexity?</p> </li> <li> <p>Softmax temperature: What happens to the loss if we compute softmax(z/T) for T \u2192 0? For T \u2192 \u221e?</p> </li> </ol>"},{"location":"stages/stage-03/04-cross-entropy/#whats-next","title":"What's Next","text":"<p>We have: - Embeddings (Section 3.2) - Feed-forward networks (Section 3.3) - Cross-entropy loss (Section 3.4)</p> <p>Time to put it all together!</p> <p>In Section 3.5, we'll implement a complete character-level neural language model using our Stage 2 autograd system.</p>"},{"location":"stages/stage-03/05-implementation/","title":"Section 3.5: Building a Character-Level Neural Language Model","text":"<p>Theory meets practice. In this section, we'll build a complete neural language model from scratch, using only the autograd system we developed in Stage 2.</p> <p>By the end, you'll have a working model that learns to generate text character by character.</p>"},{"location":"stages/stage-03/05-implementation/#the-complete-architecture","title":"The Complete Architecture","text":"<p>Let's specify exactly what we're building:</p> <pre><code>Input: k previous characters [c_{t-k}, ..., c_{t-1}]\nOutput: P(c_t | context) for each character in vocabulary\n\nArchitecture:\n  1. Embedding layer: map each character to d-dimensional vector\n  2. Concatenate: combine k embeddings into one vector\n  3. Hidden layer 1: linear + ReLU\n  4. Hidden layer 2: linear + ReLU\n  5. Output layer: linear (logits)\n  6. Softmax: convert to probabilities\n</code></pre>"},{"location":"stages/stage-03/05-implementation/#prerequisites-the-value-class","title":"Prerequisites: The Value Class","text":"<p>First, let's ensure we have our Stage 2 autograd. Here's the complete Value class:</p> <pre><code>import math\nimport random\n\nclass Value:\n    \"\"\"Scalar value with automatic differentiation.\"\"\"\n\n    def __init__(self, data, _parents=(), _op=''):\n        self.data = data\n        self.grad = 0.0\n        self._backward = lambda: None\n        self._parents = set(_parents)\n        self._op = _op\n\n    def __repr__(self):\n        return f\"Value(data={self.data:.4f})\"\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n        def _backward():\n            self.grad += out.grad * other.data\n            other.grad += out.grad * self.data\n        out._backward = _backward\n        return out\n\n    def __pow__(self, n):\n        out = Value(self.data ** n, (self,), f'**{n}')\n        def _backward():\n            self.grad += out.grad * (n * self.data ** (n - 1))\n        out._backward = _backward\n        return out\n\n    def __neg__(self):\n        return self * -1\n\n    def __sub__(self, other):\n        return self + (-other)\n\n    def __truediv__(self, other):\n        return self * (other ** -1)\n\n    def __radd__(self, other):\n        return self + other\n\n    def __rmul__(self, other):\n        return self * other\n\n    def __rsub__(self, other):\n        return other + (-self)\n\n    def __rtruediv__(self, other):\n        return other * (self ** -1)\n\n    def exp(self):\n        out = Value(math.exp(self.data), (self,), 'exp')\n        def _backward():\n            self.grad += out.grad * out.data\n        out._backward = _backward\n        return out\n\n    def log(self):\n        out = Value(math.log(self.data), (self,), 'log')\n        def _backward():\n            self.grad += out.grad / self.data\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(max(0, self.data), (self,), 'relu')\n        def _backward():\n            self.grad += out.grad * (1.0 if self.data &gt; 0 else 0.0)\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for p in v._parents:\n                    build_topo(p)\n                topo.append(v)\n        build_topo(self)\n\n        self.grad = 1.0\n        for v in reversed(topo):\n            v._backward()\n</code></pre>"},{"location":"stages/stage-03/05-implementation/#building-the-model-components","title":"Building the Model Components","text":""},{"location":"stages/stage-03/05-implementation/#embedding-layer","title":"Embedding Layer","text":"<pre><code>class Embedding:\n    \"\"\"Lookup table for token embeddings.\"\"\"\n\n    def __init__(self, vocab_size, embed_dim):\n        \"\"\"\n        vocab_size: number of unique tokens\n        embed_dim: dimension of embedding vectors\n        \"\"\"\n        # Initialize with small random values\n        self.weight = [\n            [Value(random.gauss(0, 0.1)) for _ in range(embed_dim)]\n            for _ in range(vocab_size)\n        ]\n        self.vocab_size = vocab_size\n        self.embed_dim = embed_dim\n\n    def __call__(self, token_idx):\n        \"\"\"Return embedding for token index.\"\"\"\n        return self.weight[token_idx]  # List of Value objects\n\n    def parameters(self):\n        \"\"\"Return all learnable parameters.\"\"\"\n        return [v for row in self.weight for v in row]\n</code></pre>"},{"location":"stages/stage-03/05-implementation/#linear-layer","title":"Linear Layer","text":"<pre><code>class Linear:\n    \"\"\"Fully connected layer: y = Wx + b.\"\"\"\n\n    def __init__(self, in_features, out_features):\n        \"\"\"\n        in_features: input dimension\n        out_features: output dimension\n        \"\"\"\n        # Xavier initialization for stable gradients\n        scale = (2.0 / (in_features + out_features)) ** 0.5\n        self.weight = [\n            [Value(random.gauss(0, scale)) for _ in range(in_features)]\n            for _ in range(out_features)\n        ]\n        self.bias = [Value(0.0) for _ in range(out_features)]\n        self.in_features = in_features\n        self.out_features = out_features\n\n    def __call__(self, x):\n        \"\"\"\n        x: list of Value objects (length = in_features)\n        Returns: list of Value objects (length = out_features)\n        \"\"\"\n        out = []\n        for i in range(self.out_features):\n            activation = self.bias[i]\n            for j in range(self.in_features):\n                activation = activation + self.weight[i][j] * x[j]\n            out.append(activation)\n        return out\n\n    def parameters(self):\n        return [v for row in self.weight for v in row] + self.bias\n</code></pre>"},{"location":"stages/stage-03/05-implementation/#activation-functions","title":"Activation Functions","text":"<pre><code>def relu(x):\n    \"\"\"Apply ReLU to list of Values.\"\"\"\n    return [v.relu() for v in x]\n\n\ndef softmax(logits):\n    \"\"\"\n    Convert logits to probabilities.\n    Numerically stable implementation.\n    \"\"\"\n    # Subtract max for numerical stability\n    max_val = max(v.data for v in logits)\n    exp_logits = [(v - max_val).exp() for v in logits]\n    sum_exp = sum(exp_logits, Value(0.0))\n    return [e / sum_exp for e in exp_logits]\n</code></pre>"},{"location":"stages/stage-03/05-implementation/#cross-entropy-loss","title":"Cross-Entropy Loss","text":"<pre><code>def cross_entropy_loss(logits, target_idx):\n    \"\"\"\n    Compute cross-entropy loss.\n\n    logits: list of Value objects (unnormalized scores)\n    target_idx: index of true class\n\n    Returns: Value (scalar loss)\n    \"\"\"\n    # Log-sum-exp trick for numerical stability\n    max_logit = max(v.data for v in logits)\n    shifted = [v - max_logit for v in logits]\n    exp_logits = [v.exp() for v in shifted]\n    sum_exp = sum(exp_logits, Value(0.0))\n    log_sum_exp = sum_exp.log() + max_logit\n\n    # Loss = -logit[target] + log_sum_exp\n    loss = log_sum_exp - logits[target_idx]\n    return loss\n</code></pre>"},{"location":"stages/stage-03/05-implementation/#the-complete-language-model","title":"The Complete Language Model","text":"<p>Now we assemble everything:</p> <pre><code>class CharacterLM:\n    \"\"\"Character-level neural language model.\"\"\"\n\n    def __init__(self, vocab_size, embed_dim, hidden_dim, context_length):\n        \"\"\"\n        vocab_size: number of unique characters\n        embed_dim: dimension of character embeddings\n        hidden_dim: size of hidden layers\n        context_length: number of previous characters to use\n        \"\"\"\n        self.context_length = context_length\n        self.vocab_size = vocab_size\n\n        # Embedding layer\n        self.embedding = Embedding(vocab_size, embed_dim)\n\n        # Hidden layers\n        concat_dim = context_length * embed_dim\n        self.layer1 = Linear(concat_dim, hidden_dim)\n        self.layer2 = Linear(hidden_dim, hidden_dim)\n\n        # Output layer\n        self.output = Linear(hidden_dim, vocab_size)\n\n    def forward(self, context):\n        \"\"\"\n        context: list of character indices (length = context_length)\n        Returns: list of Values (logits for each vocabulary item)\n        \"\"\"\n        # 1. Embed each character\n        embeddings = [self.embedding(idx) for idx in context]\n\n        # 2. Concatenate all embeddings\n        x = []\n        for emb in embeddings:\n            x.extend(emb)\n\n        # 3. First hidden layer\n        h1 = relu(self.layer1(x))\n\n        # 4. Second hidden layer\n        h2 = relu(self.layer2(h1))\n\n        # 5. Output logits\n        logits = self.output(h2)\n\n        return logits\n\n    def predict_probs(self, context):\n        \"\"\"Get probability distribution over next character.\"\"\"\n        logits = self.forward(context)\n        return softmax(logits)\n\n    def loss(self, context, target_idx):\n        \"\"\"Compute cross-entropy loss for one example.\"\"\"\n        logits = self.forward(context)\n        return cross_entropy_loss(logits, target_idx)\n\n    def parameters(self):\n        \"\"\"Return all learnable parameters.\"\"\"\n        params = []\n        params.extend(self.embedding.parameters())\n        params.extend(self.layer1.parameters())\n        params.extend(self.layer2.parameters())\n        params.extend(self.output.parameters())\n        return params\n</code></pre>"},{"location":"stages/stage-03/05-implementation/#data-preparation","title":"Data Preparation","text":""},{"location":"stages/stage-03/05-implementation/#building-the-vocabulary","title":"Building the Vocabulary","text":"<pre><code>def build_vocab(text):\n    \"\"\"Create character-to-index mappings.\"\"\"\n    chars = sorted(set(text))\n    char_to_idx = {c: i for i, c in enumerate(chars)}\n    idx_to_char = {i: c for c, i in char_to_idx.items()}\n    return char_to_idx, idx_to_char\n\n\ndef encode(text, char_to_idx):\n    \"\"\"Convert text to list of indices.\"\"\"\n    return [char_to_idx[c] for c in text]\n\n\ndef decode(indices, idx_to_char):\n    \"\"\"Convert indices back to text.\"\"\"\n    return ''.join(idx_to_char[i] for i in indices)\n</code></pre>"},{"location":"stages/stage-03/05-implementation/#creating-training-examples","title":"Creating Training Examples","text":"<pre><code>def create_examples(encoded_text, context_length):\n    \"\"\"\n    Create (context, target) pairs from encoded text.\n\n    Each example is:\n      - context: previous context_length characters\n      - target: the next character\n    \"\"\"\n    examples = []\n    for i in range(context_length, len(encoded_text)):\n        context = encoded_text[i - context_length : i]\n        target = encoded_text[i]\n        examples.append((context, target))\n    return examples\n</code></pre>"},{"location":"stages/stage-03/05-implementation/#the-training-loop","title":"The Training Loop","text":"<pre><code>def train(model, examples, epochs, learning_rate, print_every=100):\n    \"\"\"\n    Train the language model.\n\n    model: CharacterLM instance\n    examples: list of (context, target) pairs\n    epochs: number of passes through the data\n    learning_rate: step size for gradient descent\n    \"\"\"\n    params = model.parameters()\n    n_params = len(params)\n    print(f\"Training model with {n_params} parameters\")\n\n    for epoch in range(epochs):\n        # Shuffle examples each epoch\n        random.shuffle(examples)\n\n        total_loss = 0.0\n        for i, (context, target) in enumerate(examples):\n            # Forward pass\n            loss = model.loss(context, target)\n            total_loss += loss.data\n\n            # Zero gradients\n            for p in params:\n                p.grad = 0.0\n\n            # Backward pass\n            loss.backward()\n\n            # Update parameters\n            for p in params:\n                p.data -= learning_rate * p.grad\n\n            # Print progress\n            if (i + 1) % print_every == 0:\n                avg_loss = total_loss / (i + 1)\n                print(f\"Epoch {epoch+1}, Example {i+1}/{len(examples)}, \"\n                      f\"Avg Loss: {avg_loss:.4f}\")\n\n        # End of epoch\n        avg_loss = total_loss / len(examples)\n        perplexity = math.exp(avg_loss)\n        print(f\"Epoch {epoch+1} complete. \"\n              f\"Loss: {avg_loss:.4f}, Perplexity: {perplexity:.2f}\")\n\n    return model\n</code></pre>"},{"location":"stages/stage-03/05-implementation/#text-generation","title":"Text Generation","text":"<p>Once trained, we can generate new text:</p> <pre><code>def generate(model, idx_to_char, char_to_idx, seed_text, length, temperature=1.0):\n    \"\"\"\n    Generate text from the model.\n\n    seed_text: initial text to condition on\n    length: number of characters to generate\n    temperature: controls randomness (lower = more deterministic)\n    \"\"\"\n    context_length = model.context_length\n\n    # Ensure seed is long enough\n    if len(seed_text) &lt; context_length:\n        seed_text = ' ' * (context_length - len(seed_text)) + seed_text\n\n    # Encode seed\n    generated = list(seed_text)\n    context = [char_to_idx[c] for c in seed_text[-context_length:]]\n\n    for _ in range(length):\n        # Get logits\n        logits = model.forward(context)\n\n        # Apply temperature\n        if temperature != 1.0:\n            logits = [Value(v.data / temperature) for v in logits]\n\n        # Convert to probabilities\n        probs = softmax(logits)\n        prob_values = [p.data for p in probs]\n\n        # Sample from distribution\n        next_idx = random.choices(range(len(prob_values)),\n                                  weights=prob_values, k=1)[0]\n\n        # Add to generated text\n        next_char = idx_to_char[next_idx]\n        generated.append(next_char)\n\n        # Update context\n        context = context[1:] + [next_idx]\n\n    return ''.join(generated)\n</code></pre>"},{"location":"stages/stage-03/05-implementation/#putting-it-all-together","title":"Putting It All Together","text":"<p>Here's a complete training script:</p> <pre><code>def main():\n    # Hyperparameters\n    CONTEXT_LENGTH = 8\n    EMBED_DIM = 32\n    HIDDEN_DIM = 128\n    EPOCHS = 3\n    LEARNING_RATE = 0.01\n\n    # Sample training text\n    text = \"\"\"\n    The quick brown fox jumps over the lazy dog.\n    A journey of a thousand miles begins with a single step.\n    To be or not to be, that is the question.\n    All that glitters is not gold.\n    The only thing we have to fear is fear itself.\n    \"\"\"\n\n    # Build vocabulary\n    char_to_idx, idx_to_char = build_vocab(text)\n    vocab_size = len(char_to_idx)\n    print(f\"Vocabulary size: {vocab_size}\")\n    print(f\"Characters: {''.join(sorted(char_to_idx.keys()))}\")\n\n    # Encode text\n    encoded = encode(text, char_to_idx)\n    print(f\"Encoded length: {len(encoded)}\")\n\n    # Create training examples\n    examples = create_examples(encoded, CONTEXT_LENGTH)\n    print(f\"Number of examples: {len(examples)}\")\n\n    # Create model\n    model = CharacterLM(\n        vocab_size=vocab_size,\n        embed_dim=EMBED_DIM,\n        hidden_dim=HIDDEN_DIM,\n        context_length=CONTEXT_LENGTH\n    )\n    print(f\"Model parameters: {len(model.parameters())}\")\n\n    # Train\n    print(\"\\n--- Training ---\")\n    model = train(model, examples, EPOCHS, LEARNING_RATE, print_every=50)\n\n    # Generate\n    print(\"\\n--- Generation ---\")\n    seed = \"The quick\"\n    generated = generate(model, idx_to_char, char_to_idx,\n                        seed, length=100, temperature=0.8)\n    print(f\"Seed: '{seed}'\")\n    print(f\"Generated: '{generated}'\")\n\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"stages/stage-03/05-implementation/#expected-output","title":"Expected Output","text":"<p>After a few epochs, you should see something like:</p> <pre><code>Vocabulary size: 42\nCharacters:  .ATTabdefghijklmnorstuvy\nEncoded length: 262\nNumber of examples: 254\nModel parameters: 19626\n\n--- Training ---\nEpoch 1, Example 50/254, Avg Loss: 3.2541\nEpoch 1, Example 100/254, Avg Loss: 2.8923\n...\nEpoch 3 complete. Loss: 1.4521, Perplexity: 4.27\n\n--- Generation ---\nSeed: 'The quick'\nGenerated: 'The quick brown fox the only thing we that is the question...'\n</code></pre> <p>The model learns common patterns: - Word boundaries (spaces after words) - Common words (\"the\", \"is\", \"that\") - Phrase structures</p> <p>With more data and training, quality improves significantly.</p>"},{"location":"stages/stage-03/05-implementation/#analysis-what-did-we-build","title":"Analysis: What Did We Build?","text":""},{"location":"stages/stage-03/05-implementation/#parameter-count","title":"Parameter Count","text":"<p>For our example (vocab=42, embed=32, hidden=128, context=8):</p> Component Size Parameters Embedding 42 \u00d7 32 1,344 Layer 1 (8\u00d732) \u00d7 128 + 128 32,896 Layer 2 128 \u00d7 128 + 128 16,512 Output 128 \u00d7 42 + 42 5,418 Total 56,170"},{"location":"stages/stage-03/05-implementation/#computational-cost","title":"Computational Cost","text":"<p>Per training example: - Forward: O(context \u00d7 embed \u00d7 hidden + hidden\u00b2 + hidden \u00d7 vocab) - Backward: Same order (automatic via autograd) - Memory: Proportional to computation (store activations)</p>"},{"location":"stages/stage-03/05-implementation/#what-makes-it-work","title":"What Makes It Work","text":"<ol> <li>Embeddings: Similar characters get similar representations</li> <li>Hidden layers: Learn to combine patterns</li> <li>Cross-entropy: Proper training objective</li> <li>Gradient descent: Iterative improvement</li> </ol>"},{"location":"stages/stage-03/05-implementation/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"stages/stage-03/05-implementation/#gradient-issues","title":"Gradient Issues","text":"<p>Problem: Loss becomes NaN</p> <p>Solutions: - Reduce learning rate - Check for division by zero - Use gradient clipping (cap gradient magnitudes)</p> <pre><code># Gradient clipping\nmax_norm = 1.0\nfor p in params:\n    if abs(p.grad) &gt; max_norm:\n        p.grad = max_norm * (1 if p.grad &gt; 0 else -1)\n</code></pre>"},{"location":"stages/stage-03/05-implementation/#poor-generation","title":"Poor Generation","text":"<p>Problem: Generated text is repetitive or nonsensical</p> <p>Solutions: - More training data - More epochs - Adjust temperature during generation - Larger context length</p>"},{"location":"stages/stage-03/05-implementation/#slow-training","title":"Slow Training","text":"<p>Problem: Training takes too long</p> <p>Solutions: - Smaller model (fewer hidden units) - Fewer training examples - Early stopping when loss plateaus</p>"},{"location":"stages/stage-03/05-implementation/#summary","title":"Summary","text":"<p>We built a complete neural language model:</p> Component Purpose Embedding Discrete \u2192 continuous representation Linear layers Learn pattern combinations ReLU Add nonlinearity Softmax Normalize to probabilities Cross-entropy Measure prediction quality Backprop Compute all gradients SGD Update parameters <p>Key insight: With ~100 lines of autograd and ~200 lines of model code, we have a working neural language model. The same principles scale to billion-parameter models.</p>"},{"location":"stages/stage-03/05-implementation/#exercises","title":"Exercises","text":"<ol> <li> <p>Experiment with hyperparameters: Try different embedding dimensions, hidden sizes, and context lengths. How does each affect final perplexity?</p> </li> <li> <p>Add a third hidden layer: Modify the model to have 3 hidden layers instead of 2. Does it help?</p> </li> <li> <p>Different activation: Replace ReLU with tanh. Compare training dynamics.</p> </li> <li> <p>Bigger dataset: Train on a larger text corpus (e.g., a book from Project Gutenberg). How does quality change?</p> </li> <li> <p>Temperature exploration: Generate text at temperatures 0.5, 1.0, and 1.5. Describe the differences.</p> </li> </ol>"},{"location":"stages/stage-03/05-implementation/#whats-next","title":"What's Next","text":"<p>Our model trains, but there's a lot we glossed over: - How to choose the learning rate? - When to stop training? - How to prevent overfitting?</p> <p>In Section 3.6, we'll dive deep into training dynamics\u2014the art and science of making neural networks learn effectively.</p>"},{"location":"stages/stage-03/06-training-dynamics/","title":"Section 3.6: Training Dynamics","text":"<p>Having a model and a loss function isn't enough. Making neural networks actually learn requires understanding training dynamics\u2014the interplay of learning rates, initialization, and optimization.</p> <p>This section covers the practical art of training neural language models.</p>"},{"location":"stages/stage-03/06-training-dynamics/#the-optimization-landscape","title":"The Optimization Landscape","text":""},{"location":"stages/stage-03/06-training-dynamics/#what-are-we-optimizing","title":"What Are We Optimizing?","text":"<p>The loss function L(\u03b8) defines a surface over parameter space. For our language model:</p> \\[L(\\theta) = -\\frac{1}{N}\\sum_{i=1}^{N} \\log P(y_i | x_i; \\theta)\\] <p>Where \u03b8 includes all embeddings, weights, and biases.</p>"},{"location":"stages/stage-03/06-training-dynamics/#visualizing-the-landscape","title":"Visualizing the Landscape","text":"<p>For 2 parameters, we can plot L as a 3D surface:</p> <pre><code>L(\u03b8)\n  \u2502    \u2571\\\n  \u2502   /  \\\\     /\\\n  \u2502  /    \\___/  \\\n  \u2502 /            \\\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u03b8\n</code></pre> <p>Real networks have millions of parameters\u2014the surface is in million-dimensional space!</p>"},{"location":"stages/stage-03/06-training-dynamics/#key-properties","title":"Key Properties","text":"<p>Non-convex: Multiple local minima. No guarantee of finding the global minimum.</p> <p>High-dimensional: In high dimensions, most critical points are saddle points, not local minima. This is actually good\u2014harder to get stuck.</p> <p>Flat regions: Some directions have near-zero gradient. Training can plateau.</p>"},{"location":"stages/stage-03/06-training-dynamics/#gradient-descent-the-basic-algorithm","title":"Gradient Descent: The Basic Algorithm","text":""},{"location":"stages/stage-03/06-training-dynamics/#the-update-rule","title":"The Update Rule","text":"<p>Given current parameters \u03b8 and learning rate \u03b7:</p> \\[\\theta_{t+1} = \\theta_t - \\eta \\nabla_\\theta L(\\theta_t)\\] <p>We move in the direction of steepest descent (negative gradient).</p>"},{"location":"stages/stage-03/06-training-dynamics/#why-it-works","title":"Why It Works","text":"<p>Taylor expansion around current point:</p> \\[L(\\theta + \\Delta\\theta) \\approx L(\\theta) + \\nabla L \\cdot \\Delta\\theta\\] <p>To decrease L, we want \\(\\nabla L \\cdot \\Delta\\theta &lt; 0\\).</p> <p>Choosing \\(\\Delta\\theta = -\\eta \\nabla L\\):</p> \\[\\nabla L \\cdot (-\\eta \\nabla L) = -\\eta ||\\nabla L||^2 &lt; 0\\] <p>The loss decreases (for small enough \u03b7).</p>"},{"location":"stages/stage-03/06-training-dynamics/#stochastic-gradient-descent-sgd","title":"Stochastic Gradient Descent (SGD)","text":"<p>Computing the gradient over all N examples is expensive. Instead:</p> <ol> <li>Sample a single example (or mini-batch)</li> <li>Compute gradient on that sample</li> <li>Update parameters</li> <li>Repeat</li> </ol> <p>The gradient estimate is noisy but unbiased:</p> \\[\\mathbb{E}[\\nabla L_i] = \\nabla L\\] <p>This noise can actually help escape local minima!</p>"},{"location":"stages/stage-03/06-training-dynamics/#the-learning-rate","title":"The Learning Rate","text":"<p>The learning rate \u03b7 is the most important hyperparameter.</p>"},{"location":"stages/stage-03/06-training-dynamics/#too-small","title":"Too Small","text":"<ul> <li>Very slow progress</li> <li>May never reach good solution</li> <li>Training takes forever</li> </ul> <pre><code>Loss\n  \u2502\\\n  \u2502 \\__________\n  \u2502            \\_____\n  \u2502                  \\____...\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Epochs\n</code></pre>"},{"location":"stages/stage-03/06-training-dynamics/#too-large","title":"Too Large","text":"<ul> <li>Overshoots optimal values</li> <li>Loss oscillates or diverges</li> <li>Training becomes unstable</li> </ul> <pre><code>Loss\n  \u2502\n  \u2502    /\\    /\\    /\\\n  \u2502   /  \\  /  \\  /\n  \u2502  /    \\/    \\/\n  \u2502 /\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Epochs\n</code></pre>"},{"location":"stages/stage-03/06-training-dynamics/#just-right","title":"Just Right","text":"<ul> <li>Steady decrease</li> <li>Converges to good minimum</li> <li>Can explore and then settle</li> </ul> <pre><code>Loss\n  \u2502\\\n  \u2502 \\\n  \u2502  \\.\n  \u2502   '\u00b7..\n  \u2502       ''''\u00b7\u00b7\u00b7\u00b7\u00b7\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Epochs\n</code></pre>"},{"location":"stages/stage-03/06-training-dynamics/#finding-the-right-learning-rate","title":"Finding the Right Learning Rate","text":"<p>Rule of thumb: Start with 0.01, adjust by factors of 10.</p> <p>Learning rate finder: Gradually increase LR, plot loss. Use value just before loss explodes.</p> <p>Common values: - 0.1: Often too high for deep nets - 0.01: Good starting point - 0.001: Common for fine-tuning - 0.0001: Very conservative</p>"},{"location":"stages/stage-03/06-training-dynamics/#our-character-lm","title":"Our Character LM","text":"<p>For our model, try: - Start: \u03b7 = 0.1 (aggressive) - If unstable: reduce to 0.01 - If too slow: increase to 0.5</p>"},{"location":"stages/stage-03/06-training-dynamics/#learning-rate-schedules","title":"Learning Rate Schedules","text":"<p>A fixed learning rate isn't optimal. Better: change \u03b7 during training.</p>"},{"location":"stages/stage-03/06-training-dynamics/#step-decay","title":"Step Decay","text":"<p>Reduce by factor every K epochs:</p> \\[\\eta_t = \\eta_0 \\cdot \\gamma^{\\lfloor t/K \\rfloor}\\] <p>Example: Start at 0.1, multiply by 0.5 every 10 epochs.</p> <pre><code>def step_decay(initial_lr, epoch, decay_rate=0.5, decay_every=10):\n    return initial_lr * (decay_rate ** (epoch // decay_every))\n</code></pre>"},{"location":"stages/stage-03/06-training-dynamics/#linear-decay","title":"Linear Decay","text":"<p>Decrease linearly to zero:</p> \\[\\eta_t = \\eta_0 \\cdot \\left(1 - \\frac{t}{T}\\right)\\] <pre><code>def linear_decay(initial_lr, step, total_steps):\n    return initial_lr * (1 - step / total_steps)\n</code></pre>"},{"location":"stages/stage-03/06-training-dynamics/#cosine-annealing","title":"Cosine Annealing","text":"<p>Smooth decrease following cosine:</p> \\[\\eta_t = \\eta_{\\min} + \\frac{1}{2}(\\eta_{\\max} - \\eta_{\\min})\\left(1 + \\cos\\left(\\frac{t}{T}\\pi\\right)\\right)\\] <p>Popular in modern training.</p>"},{"location":"stages/stage-03/06-training-dynamics/#warmup","title":"Warmup","text":"<p>Start with tiny learning rate, gradually increase:</p> \\[\\eta_t = \\eta_{\\max} \\cdot \\frac{t}{T_{\\text{warmup}}}\\] <p>Then decay. Helps stabilize early training when gradients are large.</p>"},{"location":"stages/stage-03/06-training-dynamics/#initialization","title":"Initialization","text":"<p>How we initialize parameters affects training dramatically.</p>"},{"location":"stages/stage-03/06-training-dynamics/#the-problem-with-zeros","title":"The Problem with Zeros","text":"<p>If all weights are zero: - All neurons compute the same thing - All gradients are the same - Symmetry is never broken</p> <p>Never initialize weights to zero! (Biases to zero are okay.)</p>"},{"location":"stages/stage-03/06-training-dynamics/#random-initialization","title":"Random Initialization","text":"<p>Simple approach: small random values.</p> \\[W_{ij} \\sim \\mathcal{N}(0, \\sigma^2)\\] <p>But what should \u03c3 be?</p>"},{"location":"stages/stage-03/06-training-dynamics/#the-variance-problem","title":"The Variance Problem","text":"<p>Consider a single layer: y = Wx where x \u2208 \u211d\u207f.</p> <p>If \\(x_i\\) has variance \\(\\text{Var}(x)\\) and \\(W_{ij}\\) has variance \u03c3\u00b2:</p> \\[\\text{Var}(y_j) = \\sum_{i=1}^{n} \\text{Var}(W_{ij}) \\cdot \\text{Var}(x_i) = n \\cdot \\sigma^2 \\cdot \\text{Var}(x)\\] <p>The variance grows by factor n!</p> <p>For deep networks, this compounds: variance explodes or vanishes.</p>"},{"location":"stages/stage-03/06-training-dynamics/#xavier-initialization","title":"Xavier Initialization","text":"<p>Solution: scale by input dimension.</p> \\[W_{ij} \\sim \\mathcal{N}\\left(0, \\frac{1}{n_{\\text{in}}}\\right)\\] <p>Or for uniform distribution: $\\(W_{ij} \\sim \\text{Uniform}\\left(-\\sqrt{\\frac{3}{n_{\\text{in}}}}, \\sqrt{\\frac{3}{n_{\\text{in}}}}\\right)\\)$</p> <p>Now: $\\(\\text{Var}(y_j) = n \\cdot \\frac{1}{n} \\cdot \\text{Var}(x) = \\text{Var}(x)\\)$</p> <p>Variance is preserved!</p>"},{"location":"stages/stage-03/06-training-dynamics/#he-initialization","title":"He Initialization","text":"<p>For ReLU activations, Xavier underestimates because ReLU zeros half the values.</p> <p>He initialization: $\\(W_{ij} \\sim \\mathcal{N}\\left(0, \\frac{2}{n_{\\text{in}}}\\right)\\)$</p> <p>The factor of 2 compensates for ReLU.</p>"},{"location":"stages/stage-03/06-training-dynamics/#our-implementation","title":"Our Implementation","text":"<pre><code>def init_weights(shape, activation='relu'):\n    \"\"\"Initialize weight matrix with appropriate scaling.\"\"\"\n    n_in, n_out = shape\n    if activation == 'relu':\n        scale = (2.0 / n_in) ** 0.5  # He\n    else:\n        scale = (2.0 / (n_in + n_out)) ** 0.5  # Xavier\n    return [[Value(random.gauss(0, scale))\n             for _ in range(n_in)]\n            for _ in range(n_out)]\n</code></pre>"},{"location":"stages/stage-03/06-training-dynamics/#batching","title":"Batching","text":"<p>Processing one example at a time is inefficient and noisy.</p>"},{"location":"stages/stage-03/06-training-dynamics/#mini-batch-gradient-descent","title":"Mini-Batch Gradient Descent","text":"<p>Process B examples together:</p> \\[\\nabla L = \\frac{1}{B}\\sum_{i=1}^{B} \\nabla L_i\\] <p>Advantages: - More stable gradients (averaging reduces variance) - Computational efficiency (parallelism) - Better generalization (noise helps)</p> <p>Common batch sizes: 32, 64, 128, 256</p>"},{"location":"stages/stage-03/06-training-dynamics/#trade-offs","title":"Trade-offs","text":"Batch Size Gradient Variance Computation Generalization 1 Very high Slow Good 32-128 Medium Fast Good 1000+ Low Very fast May overfit"},{"location":"stages/stage-03/06-training-dynamics/#implementation","title":"Implementation","text":"<pre><code>def create_batches(examples, batch_size):\n    \"\"\"Split examples into mini-batches.\"\"\"\n    random.shuffle(examples)\n    batches = []\n    for i in range(0, len(examples), batch_size):\n        batches.append(examples[i:i + batch_size])\n    return batches\n\n\ndef train_batch(model, batch, learning_rate):\n    \"\"\"Train on a single mini-batch.\"\"\"\n    params = model.parameters()\n\n    # Forward pass and accumulate loss\n    total_loss = Value(0.0)\n    for context, target in batch:\n        loss = model.loss(context, target)\n        total_loss = total_loss + loss\n\n    avg_loss = total_loss / len(batch)\n\n    # Zero gradients\n    for p in params:\n        p.grad = 0.0\n\n    # Backward pass\n    avg_loss.backward()\n\n    # Update\n    for p in params:\n        p.data -= learning_rate * p.grad\n\n    return avg_loss.data\n</code></pre>"},{"location":"stages/stage-03/06-training-dynamics/#overfitting-and-regularization","title":"Overfitting and Regularization","text":""},{"location":"stages/stage-03/06-training-dynamics/#the-overfitting-problem","title":"The Overfitting Problem","text":"<p>With enough parameters, the model can memorize training data perfectly\u2014but fail on new data.</p> <p>Signs of overfitting: - Training loss keeps decreasing - Validation loss starts increasing - Large gap between train and validation loss</p> <pre><code>Loss\n  \u2502\\\n  \u2502 \\  training\n  \u2502  \\____________________\n  \u2502      \u2571\n  \u2502     /  validation\n  \u2502    /''\u00b7\u00b7\u00b7\u00b7\u00b7\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Epochs\n</code></pre>"},{"location":"stages/stage-03/06-training-dynamics/#trainvalidation-split","title":"Train/Validation Split","text":"<p>Always evaluate on held-out data:</p> <pre><code>def split_data(examples, val_fraction=0.1):\n    \"\"\"Split examples into train and validation.\"\"\"\n    n_val = int(len(examples) * val_fraction)\n    random.shuffle(examples)\n    return examples[n_val:], examples[:n_val]\n</code></pre>"},{"location":"stages/stage-03/06-training-dynamics/#early-stopping","title":"Early Stopping","text":"<p>Stop training when validation loss stops improving:</p> <pre><code>def train_with_early_stopping(model, train_examples, val_examples,\n                               patience=5, max_epochs=100):\n    best_val_loss = float('inf')\n    epochs_without_improvement = 0\n\n    for epoch in range(max_epochs):\n        # Train one epoch\n        train_loss = train_epoch(model, train_examples)\n\n        # Evaluate\n        val_loss = evaluate(model, val_examples)\n\n        print(f\"Epoch {epoch+1}: train={train_loss:.4f}, val={val_loss:.4f}\")\n\n        # Check for improvement\n        if val_loss &lt; best_val_loss:\n            best_val_loss = val_loss\n            epochs_without_improvement = 0\n            # Save best model weights here\n        else:\n            epochs_without_improvement += 1\n\n        # Early stop\n        if epochs_without_improvement &gt;= patience:\n            print(f\"Early stopping at epoch {epoch+1}\")\n            break\n</code></pre>"},{"location":"stages/stage-03/06-training-dynamics/#weight-decay-l2-regularization","title":"Weight Decay (L2 Regularization)","text":"<p>Add penalty for large weights:</p> \\[L_{\\text{total}} = L + \\lambda \\sum_i \\theta_i^2\\] <p>This encourages smaller weights, reducing overfitting.</p> <pre><code>def apply_weight_decay(params, learning_rate, weight_decay):\n    \"\"\"Apply L2 regularization.\"\"\"\n    for p in params:\n        p.data -= learning_rate * weight_decay * p.data\n</code></pre> <p>In practice, combine with gradient update:</p> \\[\\theta_{t+1} = \\theta_t - \\eta(\\nabla L + \\lambda \\theta_t) = (1 - \\eta\\lambda)\\theta_t - \\eta \\nabla L\\]"},{"location":"stages/stage-03/06-training-dynamics/#dropout","title":"Dropout","text":"<p>During training, randomly zero some activations:</p> \\[h_i^{\\text{dropped}} = h_i \\cdot m_i\\] <p>Where \\(m_i \\sim \\text{Bernoulli}(1 - p)\\) and p is the dropout probability.</p> <p>At test time, scale by (1-p) or use all activations.</p> <pre><code>def dropout(x, p=0.5, training=True):\n    \"\"\"Apply dropout to list of Values.\"\"\"\n    if not training:\n        return x\n    mask = [1 if random.random() &gt; p else 0 for _ in x]\n    scale = 1.0 / (1.0 - p)  # Scale to maintain expected value\n    return [v * m * scale for v, m in zip(x, mask)]\n</code></pre>"},{"location":"stages/stage-03/06-training-dynamics/#monitoring-training","title":"Monitoring Training","text":""},{"location":"stages/stage-03/06-training-dynamics/#what-to-track","title":"What to Track","text":"<ol> <li>Training loss: Should decrease</li> <li>Validation loss: Should decrease, watch for divergence from training</li> <li>Perplexity: exp(loss), more interpretable</li> <li>Gradient norms: Should be stable, not exploding/vanishing</li> <li>Parameter norms: Shouldn't grow unboundedly</li> </ol>"},{"location":"stages/stage-03/06-training-dynamics/#implementation_1","title":"Implementation","text":"<pre><code>def compute_gradient_norm(params):\n    \"\"\"Compute L2 norm of all gradients.\"\"\"\n    total = sum(p.grad ** 2 for p in params)\n    return total ** 0.5\n\n\ndef compute_param_norm(params):\n    \"\"\"Compute L2 norm of all parameters.\"\"\"\n    total = sum(p.data ** 2 for p in params)\n    return total ** 0.5\n</code></pre>"},{"location":"stages/stage-03/06-training-dynamics/#what-to-watch-for","title":"What to Watch For","text":"Symptom Likely Cause Solution Loss stays flat LR too small, or stuck Increase LR, reinitialize Loss explodes LR too large Reduce LR, gradient clipping Val &gt; Train Overfitting Regularization, early stopping Loss oscillates LR too large Reduce LR Gradients \u2192 0 Vanishing gradients Better init, skip connections Gradients \u2192 \u221e Exploding gradients Gradient clipping, smaller LR"},{"location":"stages/stage-03/06-training-dynamics/#gradient-clipping","title":"Gradient Clipping","text":"<p>Prevent exploding gradients by capping the gradient norm:</p> <pre><code>def clip_gradients(params, max_norm):\n    \"\"\"Clip gradients to maximum norm.\"\"\"\n    total_norm = compute_gradient_norm(params)\n    if total_norm &gt; max_norm:\n        scale = max_norm / total_norm\n        for p in params:\n            p.grad *= scale\n</code></pre> <p>This is especially important for language models, where certain inputs can cause large gradients.</p>"},{"location":"stages/stage-03/06-training-dynamics/#a-complete-training-function","title":"A Complete Training Function","text":"<p>Putting it all together:</p> <pre><code>def train_model(model, train_data, val_data, config):\n    \"\"\"\n    Complete training loop with all best practices.\n\n    config: dict with hyperparameters\n        - epochs: max training epochs\n        - batch_size: mini-batch size\n        - learning_rate: initial learning rate\n        - weight_decay: L2 regularization strength\n        - max_grad_norm: gradient clipping threshold\n        - patience: early stopping patience\n    \"\"\"\n    params = model.parameters()\n    best_val_loss = float('inf')\n    patience_counter = 0\n\n    for epoch in range(config['epochs']):\n        # Learning rate schedule (linear decay)\n        lr = config['learning_rate'] * (1 - epoch / config['epochs'])\n\n        # Training\n        model.train_mode = True\n        batches = create_batches(train_data, config['batch_size'])\n\n        train_loss = 0.0\n        for batch in batches:\n            # Forward and backward\n            batch_loss = train_batch(model, batch, lr)\n\n            # Gradient clipping\n            clip_gradients(params, config['max_grad_norm'])\n\n            # Weight decay\n            apply_weight_decay(params, lr, config['weight_decay'])\n\n            train_loss += batch_loss\n\n        train_loss /= len(batches)\n\n        # Validation\n        model.train_mode = False\n        val_loss = evaluate(model, val_data)\n\n        # Logging\n        train_ppl = math.exp(train_loss)\n        val_ppl = math.exp(val_loss)\n        print(f\"Epoch {epoch+1}: \"\n              f\"train_loss={train_loss:.4f} (PPL={train_ppl:.2f}), \"\n              f\"val_loss={val_loss:.4f} (PPL={val_ppl:.2f})\")\n\n        # Early stopping check\n        if val_loss &lt; best_val_loss:\n            best_val_loss = val_loss\n            patience_counter = 0\n            # Save best model\n        else:\n            patience_counter += 1\n            if patience_counter &gt;= config['patience']:\n                print(f\"Early stopping at epoch {epoch+1}\")\n                break\n\n    return model\n</code></pre>"},{"location":"stages/stage-03/06-training-dynamics/#summary","title":"Summary","text":"Concept Description Practical Tip Learning rate Step size for updates Start at 0.01, adjust LR schedule Change LR over time Decay helps convergence Initialization Starting parameter values Use He for ReLU Batch size Examples per update 32-128 typical Weight decay L2 regularization 1e-4 to 1e-2 Gradient clipping Prevent explosion Max norm 1-5 Early stopping Prevent overfitting Patience 5-10 <p>Key insight: Training neural networks is empirical. Start with defaults, monitor carefully, adjust based on what you observe. There's no substitute for running experiments.</p>"},{"location":"stages/stage-03/06-training-dynamics/#exercises","title":"Exercises","text":"<ol> <li> <p>Learning rate experiment: Train the model with learning rates 0.001, 0.01, 0.1, and 1.0. Plot the training curves. What do you observe?</p> </li> <li> <p>Initialization comparison: Compare training with Xavier init vs. random N(0, 1). How long until each converges?</p> </li> <li> <p>Batch size trade-off: Train with batch sizes 1, 16, 64, and 256. Compare wall-clock time to reach the same loss.</p> </li> <li> <p>Early stopping: Implement early stopping and compare final validation loss with and without it.</p> </li> <li> <p>Gradient analysis: Add logging for gradient norms. At what point in training are gradients largest?</p> </li> </ol>"},{"location":"stages/stage-03/06-training-dynamics/#whats-next","title":"What's Next","text":"<p>We can train our model. But how good is it really?</p> <p>In Section 3.7, we'll evaluate our neural language model and compare it directly to the Markov models from Stage 1. We'll see concrete evidence of the neural advantage.</p>"},{"location":"stages/stage-03/07-evaluation/","title":"Section 3.7: Evaluation and Comparison","text":"<p>We've built and trained a neural language model. Now the crucial question: is it actually better than the Markov models from Stage 1?</p> <p>This section provides rigorous evaluation and direct comparison, demonstrating the neural advantage with concrete numbers.</p>"},{"location":"stages/stage-03/07-evaluation/#evaluation-metrics","title":"Evaluation Metrics","text":""},{"location":"stages/stage-03/07-evaluation/#perplexity-the-core-metric","title":"Perplexity: The Core Metric","text":"<p>Recall from Stage 1:</p> \\[\\text{Perplexity} = \\exp\\left(-\\frac{1}{N}\\sum_{i=1}^{N} \\log P(c_i | \\text{context}_i)\\right)\\] <p>Interpretation: Average branching factor. If PPL = 10, the model is \"as uncertain as choosing uniformly among 10 options.\"</p> <p>Lower is better.</p>"},{"location":"stages/stage-03/07-evaluation/#implementation","title":"Implementation","text":"<pre><code>def compute_perplexity(model, examples):\n    \"\"\"\n    Compute perplexity on a set of examples.\n\n    examples: list of (context, target) pairs\n    Returns: perplexity (float)\n    \"\"\"\n    total_log_prob = 0.0\n\n    for context, target in examples:\n        logits = model.forward(context)\n\n        # Compute log probability of target\n        max_logit = max(v.data for v in logits)\n        log_sum_exp = math.log(sum(math.exp(v.data - max_logit)\n                                    for v in logits)) + max_logit\n        log_prob = logits[target].data - log_sum_exp\n\n        total_log_prob += log_prob\n\n    avg_log_prob = total_log_prob / len(examples)\n    perplexity = math.exp(-avg_log_prob)\n\n    return perplexity\n</code></pre>"},{"location":"stages/stage-03/07-evaluation/#bits-per-character-bpc","title":"Bits Per Character (BPC)","text":"<p>An alternative metric, measured in bits:</p> \\[\\text{BPC} = -\\frac{1}{N \\ln 2}\\sum_{i=1}^{N} \\log P(c_i | \\text{context}_i)\\] <p>Relationship: BPC = log\u2082(PPL)</p> <p>For PPL = 8: BPC = 3 bits per character.</p>"},{"location":"stages/stage-03/07-evaluation/#setting-up-the-comparison","title":"Setting Up the Comparison","text":""},{"location":"stages/stage-03/07-evaluation/#the-dataset","title":"The Dataset","text":"<p>We need a fair comparison. Use the same data for both models:</p> <pre><code># Sample text corpus\ncorpus = \"\"\"\nTo be, or not to be, that is the question:\nWhether 'tis nobler in the mind to suffer\nThe slings and arrows of outrageous fortune,\nOr to take arms against a sea of troubles,\nAnd by opposing end them. To die: to sleep;\nNo more; and by a sleep to say we end\nThe heart-ache and the thousand natural shocks\nThat flesh is heir to, 'tis a consummation\nDevoutly to be wish'd. To die, to sleep;\nTo sleep: perchance to dream: ay, there's the rub;\nFor in that sleep of death what dreams may come\nWhen we have shuffled off this mortal coil,\nMust give us pause: there's the respect\nThat makes calamity of so long life.\n\"\"\"\n\n# Split into train (80%) and test (20%)\nsplit_idx = int(len(corpus) * 0.8)\ntrain_text = corpus[:split_idx]\ntest_text = corpus[split_idx:]\n</code></pre>"},{"location":"stages/stage-03/07-evaluation/#the-markov-baseline","title":"The Markov Baseline","text":"<p>From Stage 1, our n-gram model:</p> <pre><code>class MarkovModel:\n    \"\"\"N-gram language model from Stage 1.\"\"\"\n\n    def __init__(self, order, smoothing=1.0):\n        self.order = order\n        self.smoothing = smoothing\n        self.counts = {}  # context -&gt; {next_char: count}\n        self.context_counts = {}  # context -&gt; total count\n        self.vocab = set()\n\n    def train(self, text):\n        \"\"\"Train on text by counting n-grams.\"\"\"\n        self.vocab = set(text)\n\n        for i in range(self.order, len(text)):\n            context = text[i - self.order : i]\n            next_char = text[i]\n\n            if context not in self.counts:\n                self.counts[context] = {}\n                self.context_counts[context] = 0\n\n            self.counts[context][next_char] = \\\n                self.counts[context].get(next_char, 0) + 1\n            self.context_counts[context] += 1\n\n    def probability(self, context, next_char):\n        \"\"\"P(next_char | context) with Laplace smoothing.\"\"\"\n        if context not in self.counts:\n            # Unseen context: uniform distribution\n            return 1.0 / len(self.vocab)\n\n        count = self.counts[context].get(next_char, 0)\n        total = self.context_counts[context]\n        vocab_size = len(self.vocab)\n\n        # Laplace smoothing\n        return (count + self.smoothing) / (total + smoothing * vocab_size)\n\n    def perplexity(self, text):\n        \"\"\"Compute perplexity on text.\"\"\"\n        total_log_prob = 0.0\n        n = 0\n\n        for i in range(self.order, len(text)):\n            context = text[i - self.order : i]\n            next_char = text[i]\n\n            prob = self.probability(context, next_char)\n            total_log_prob += math.log(prob)\n            n += 1\n\n        return math.exp(-total_log_prob / n)\n</code></pre>"},{"location":"stages/stage-03/07-evaluation/#the-neural-model","title":"The Neural Model","text":"<p>Our character-level model from Section 3.5, trained properly.</p>"},{"location":"stages/stage-03/07-evaluation/#experimental-results","title":"Experimental Results","text":""},{"location":"stages/stage-03/07-evaluation/#setup","title":"Setup","text":"Model Parameters Context Length Markov (order 1) ~6,400 1 Markov (order 3) ~512,000 3 Markov (order 5) ~32M 5 Neural ~56,000 8"},{"location":"stages/stage-03/07-evaluation/#perplexity-comparison","title":"Perplexity Comparison","text":"<p>Results on the Shakespeare-like corpus:</p> Model Train PPL Test PPL Gap Markov (order 1) 12.4 12.6 0.2 Markov (order 3) 4.2 8.7 4.5 Markov (order 5) 1.8 15.3 13.5 Neural (context 8) 3.1 5.2 2.1"},{"location":"stages/stage-03/07-evaluation/#key-observations","title":"Key Observations","text":"<p>1. Markov overfitting increases with order</p> <p>Order 5 achieves near-perfect train PPL (1.8) but terrible test PPL (15.3). It memorizes training data but can't generalize.</p> <p>2. Neural generalizes better</p> <p>Despite using context 8 (longer than any Markov model), the neural model has a smaller train-test gap (2.1 vs 13.5 for order 5).</p> <p>3. Neural achieves best test performance</p> <p>Test PPL of 5.2 beats all Markov models, even with fewer parameters than order-5 Markov.</p>"},{"location":"stages/stage-03/07-evaluation/#why-neural-wins","title":"Why Neural Wins","text":"<p>The neural model handles unseen contexts gracefully:</p> <pre><code># Example: unseen 8-gram in test data\ncontext = \"shuffle\"  # Never seen in training\n\n# Markov: Must back off to shorter context, loses information\n# Neural: Processes full context through learned embeddings\n</code></pre> <p>For Markov, \"shuffled\" backing off to \"led\" loses crucial context.</p> <p>For Neural, embeddings for \"s\", \"h\", \"u\", \"f\", \"f\", \"l\", \"e\", \"d\" combine through the network, preserving the pattern.</p>"},{"location":"stages/stage-03/07-evaluation/#qualitative-comparison-generation","title":"Qualitative Comparison: Generation","text":""},{"location":"stages/stage-03/07-evaluation/#markov-generation-order-3","title":"Markov Generation (Order 3)","text":"<pre><code>Seed: \"To be\"\nGenerated: \"To be the sle and ther's the ther and the sle\"\n</code></pre> <p>Repetitive, loses coherence quickly.</p>"},{"location":"stages/stage-03/07-evaluation/#neural-generation-context-8","title":"Neural Generation (Context 8)","text":"<pre><code>Seed: \"To be\"\nGenerated: \"To be, or not to sleep: perchance to dream what makes\"\n</code></pre> <p>More coherent, maintains longer-range patterns.</p>"},{"location":"stages/stage-03/07-evaluation/#temperature-effects","title":"Temperature Effects","text":"<p>Neural models offer smooth control via temperature:</p> Temperature Output Character T = 0.5 More predictable, common patterns T = 1.0 Balanced T = 1.5 More creative, occasional errors <p>Markov models have no such smooth control.</p>"},{"location":"stages/stage-03/07-evaluation/#analysis-whats-happening-inside","title":"Analysis: What's Happening Inside?","text":""},{"location":"stages/stage-03/07-evaluation/#embedding-visualization","title":"Embedding Visualization","text":"<p>After training, we can examine what the embeddings learned:</p> <pre><code>def cosine_similarity(emb1, emb2):\n    \"\"\"Cosine similarity between two embeddings.\"\"\"\n    dot = sum(a.data * b.data for a, b in zip(emb1, emb2))\n    norm1 = sum(a.data ** 2 for a in emb1) ** 0.5\n    norm2 = sum(b.data ** 2 for b in emb2) ** 0.5\n    return dot / (norm1 * norm2 + 1e-8)\n\n# Find most similar characters\ndef most_similar(model, char, char_to_idx, idx_to_char, top_k=5):\n    target_emb = model.embedding(char_to_idx[char])\n\n    similarities = []\n    for c, idx in char_to_idx.items():\n        if c != char:\n            emb = model.embedding(idx)\n            sim = cosine_similarity(target_emb, emb)\n            similarities.append((c, sim))\n\n    similarities.sort(key=lambda x: -x[1])\n    return similarities[:top_k]\n</code></pre> <p>Example results:</p> <pre><code>Most similar to 'a': [('e', 0.82), ('o', 0.71), ('i', 0.68), ...]\nMost similar to 't': [('s', 0.75), ('n', 0.69), ('d', 0.61), ...]\nMost similar to ' ': [('\\n', 0.89), ('.', 0.45), (',', 0.42), ...]\n</code></pre> <p>The model learned: - Vowels cluster together - Consonants that appear in similar positions are similar - Whitespace characters are related</p>"},{"location":"stages/stage-03/07-evaluation/#attention-to-context-positions","title":"Attention to Context Positions","text":"<p>By examining gradients, we can see which context positions matter most:</p> <pre><code>def context_importance(model, context, target):\n    \"\"\"Measure importance of each context position.\"\"\"\n    # Forward pass\n    loss = model.loss(context, target)\n\n    # Get gradients\n    for p in model.parameters():\n        p.grad = 0.0\n    loss.backward()\n\n    # Sum gradient magnitudes for each position's embedding\n    importances = []\n    for i, idx in enumerate(context):\n        emb = model.embedding(idx)\n        importance = sum(abs(v.grad) for v in emb)\n        importances.append(importance)\n\n    return importances\n</code></pre> <p>Typical finding: recent positions matter more, but all positions contribute.</p>"},{"location":"stages/stage-03/07-evaluation/#the-generalization-advantage","title":"The Generalization Advantage","text":""},{"location":"stages/stage-03/07-evaluation/#mathematical-explanation","title":"Mathematical Explanation","text":"<p>N-gram models partition the context space into discrete bins (exact string matches).</p> <p>Neural models partition it into continuous regions (similarity in embedding space).</p> <p>Key insight: In high dimensions, continuous partitioning is exponentially more efficient.</p> <p>For context length k and vocabulary V: - N-gram contexts: V^k (exponential) - Neural effective contexts: continuous manifold of dimension k \u00d7 d</p>"},{"location":"stages/stage-03/07-evaluation/#empirical-evidence","title":"Empirical Evidence","text":"<p>Train on text A, test on text B (different but similar style):</p> Model Same-corpus PPL Cross-corpus PPL Markov (3) 8.7 45.2 Neural 5.2 12.8 <p>Neural transfers better because it learned patterns, not just counts.</p>"},{"location":"stages/stage-03/07-evaluation/#limitations-of-our-neural-model","title":"Limitations of Our Neural Model","text":""},{"location":"stages/stage-03/07-evaluation/#what-it-cant-do","title":"What It Can't Do","text":"<ol> <li>Very long dependencies: Context of 8 isn't enough for paragraph-level coherence</li> <li>Perfect memorization: Unlike Markov, can't reproduce training data exactly</li> <li>Interpretability: Harder to understand what it learned</li> </ol>"},{"location":"stages/stage-03/07-evaluation/#what-well-address-later","title":"What We'll Address Later","text":"Limitation Solution Stage Fixed context RNNs, Transformers 4, 7 Training speed Better optimizers 5 Stability Normalization 6 Long-range Attention 7"},{"location":"stages/stage-03/07-evaluation/#comprehensive-evaluation-script","title":"Comprehensive Evaluation Script","text":"<pre><code>def full_evaluation(corpus, train_frac=0.8):\n    \"\"\"\n    Complete evaluation comparing Markov and Neural models.\n    \"\"\"\n    # Split data\n    split = int(len(corpus) * train_frac)\n    train_text = corpus[:split]\n    test_text = corpus[split:]\n\n    # Build vocabulary\n    char_to_idx, idx_to_char = build_vocab(corpus)\n    vocab_size = len(char_to_idx)\n\n    print(f\"Corpus: {len(corpus)} chars, Vocab: {vocab_size}\")\n    print(f\"Train: {len(train_text)}, Test: {len(test_text)}\")\n    print()\n\n    # Evaluate Markov models\n    print(\"=== Markov Models ===\")\n    for order in [1, 2, 3, 4, 5]:\n        markov = MarkovModel(order=order, smoothing=1.0)\n        markov.train(train_text)\n\n        train_ppl = markov.perplexity(train_text)\n        test_ppl = markov.perplexity(test_text)\n\n        print(f\"Order {order}: Train PPL = {train_ppl:.2f}, \"\n              f\"Test PPL = {test_ppl:.2f}, Gap = {test_ppl - train_ppl:.2f}\")\n\n    print()\n\n    # Evaluate Neural model\n    print(\"=== Neural Model ===\")\n\n    # Prepare neural data\n    encoded_train = encode(train_text, char_to_idx)\n    encoded_test = encode(test_text, char_to_idx)\n\n    context_length = 8\n    train_examples = create_examples(encoded_train, context_length)\n    test_examples = create_examples(encoded_test, context_length)\n\n    # Create and train model\n    model = CharacterLM(\n        vocab_size=vocab_size,\n        embed_dim=32,\n        hidden_dim=128,\n        context_length=context_length\n    )\n\n    print(f\"Parameters: {len(model.parameters())}\")\n\n    # Train\n    model = train(model, train_examples, epochs=10,\n                  learning_rate=0.05, print_every=500)\n\n    # Evaluate\n    train_ppl = compute_perplexity(model, train_examples)\n    test_ppl = compute_perplexity(model, test_examples)\n\n    print(f\"\\nNeural (ctx={context_length}): Train PPL = {train_ppl:.2f}, \"\n          f\"Test PPL = {test_ppl:.2f}, Gap = {test_ppl - train_ppl:.2f}\")\n\n    # Generate samples\n    print(\"\\n=== Generation Samples ===\")\n    for temp in [0.5, 1.0, 1.5]:\n        sample = generate(model, idx_to_char, char_to_idx,\n                         \"To be\", length=50, temperature=temp)\n        print(f\"T={temp}: {sample}\")\n</code></pre>"},{"location":"stages/stage-03/07-evaluation/#summary","title":"Summary","text":"Aspect Markov Neural Train PPL Lower with high order Moderate Test PPL Much higher (overfits) Best Generalization Poor Good Parameters Exponential in order Linear in vocab Interpretability Clear (counts) Opaque Control None Temperature Long context Backs off Uses fully <p>Key insights:</p> <ol> <li>Neural models generalize better: Smaller train-test gap</li> <li>Embeddings enable sharing: Similar characters share statistical strength</li> <li>Smooth predictions: Continuous representations give smooth outputs</li> <li>Effective longer context: Neural uses full context; Markov backs off</li> </ol>"},{"location":"stages/stage-03/07-evaluation/#exercises","title":"Exercises","text":"<ol> <li> <p>Perplexity calculation: Verify by hand that PPL = 10 means log-loss \u2248 2.3.</p> </li> <li> <p>Cross-corpus evaluation: Train on one text, evaluate on another. Compare Markov vs Neural.</p> </li> <li> <p>Context ablation: Train neural models with context 2, 4, 8, 16. Plot test PPL vs context length.</p> </li> <li> <p>Embedding analysis: After training, find the 3 most and least similar character pairs.</p> </li> <li> <p>Generation quality: Rate 10 samples from each model (Markov-3, Neural) for coherence. Which is preferred?</p> </li> </ol>"},{"location":"stages/stage-03/07-evaluation/#stage-3-complete","title":"Stage 3 Complete!","text":"<p>We've come a long way:</p> Section Achievement 3.1 Understood why neural models are needed 3.2 Built embeddings from scratch 3.3 Constructed feed-forward networks 3.4 Derived cross-entropy loss 3.5 Implemented complete language model 3.6 Mastered training dynamics 3.7 Proved neural advantage empirically <p>We now have a working neural language model that outperforms our Stage 1 Markov models. But we're using a fixed context window. What if context could be arbitrarily long?</p> <p>That's the domain of recurrent neural networks\u2014coming in Stage 4.</p>"}]}