
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="A rigorous, bottom-up approach to understanding language models">
      
      
        <meta name="author" content="Taras Tsugrii">
      
      
        <link rel="canonical" href="https://ttsugriy.github.io/llm-first-principles/stages/stage-03/06-training-dynamics/">
      
      
        <link rel="prev" href="../05-implementation/">
      
      
        <link rel="next" href="../07-evaluation/">
      
      
        
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.1">
    
    
      
        <title>3.6 Training Dynamics - Building LLMs from First Principles</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.484c7ddc.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.ab4e12ef.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#section-36-training-dynamics" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="Building LLMs from First Principles" class="md-header__button md-logo" aria-label="Building LLMs from First Principles" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Building LLMs from First Principles
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              3.6 Training Dynamics
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/ttsugriy/llm-first-principles" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    ttsugriy/llm-first-principles
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



  

<nav class="md-nav md-nav--primary md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="Building LLMs from First Principles" class="md-nav__button md-logo" aria-label="Building LLMs from First Principles" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Building LLMs from First Principles
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/ttsugriy/llm-first-principles" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    ttsugriy/llm-first-principles
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Home
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    
  
    Stage 1 - Markov Chains
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    Stage 1 - Markov Chains
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../stage-01/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Overview
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../stage-01/01-probability-foundations/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    1.1 Probability Foundations
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../stage-01/02-language-modeling-problem/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    1.2 Language Modeling Problem
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../stage-01/03-mle-derivation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    1.3 MLE Derivation
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../stage-01/04-information-theory/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    1.4 Information Theory
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../stage-01/05-perplexity/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    1.5 Perplexity
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../stage-01/06-temperature-sampling/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    1.6 Temperature Sampling
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../stage-01/07-implementation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    1.7 Implementation
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../stage-01/08-trade-offs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    1.8 Trade-offs
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    
  
    Stage 2 - Automatic Differentiation
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            
  
    Stage 2 - Automatic Differentiation
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../stage-02/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Overview
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../stage-02/01-what-is-derivative/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    2.1 What is a Derivative?
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../stage-02/02-derivative-rules/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    2.2 Derivative Rules
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../stage-02/03-chain-rule/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    2.3 The Chain Rule
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../stage-02/04-computational-graphs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    2.4 Computational Graphs
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../stage-02/05-forward-vs-reverse/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    2.5 Forward vs Reverse Mode
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../stage-02/06-autograd-from-scratch/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    2.6 Building Autograd
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../stage-02/07-testing-validation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    2.7 Testing and Validation
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" checked>
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    
  
    Stage 3 - Neural Language Models
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            
  
    Stage 3 - Neural Language Models
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Overview
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../01-why-neural/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    3.1 Why Neural?
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../02-embeddings/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    3.2 Embeddings
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../03-feed-forward/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    3.3 Feed-Forward Networks
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../04-cross-entropy/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    3.4 Cross-Entropy Loss
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../05-implementation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    3.5 Implementation
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    
  
    3.6 Training Dynamics
  

    
  </span>
  
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    
  
    3.6 Training Dynamics
  

    
  </span>
  
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#the-optimization-landscape" class="md-nav__link">
    <span class="md-ellipsis">
      
        The Optimization Landscape
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="The Optimization Landscape">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#what-are-we-optimizing" class="md-nav__link">
    <span class="md-ellipsis">
      
        What Are We Optimizing?
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#visualizing-the-landscape" class="md-nav__link">
    <span class="md-ellipsis">
      
        Visualizing the Landscape
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#key-properties" class="md-nav__link">
    <span class="md-ellipsis">
      
        Key Properties
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#gradient-descent-the-basic-algorithm" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gradient Descent: The Basic Algorithm
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Gradient Descent: The Basic Algorithm">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#the-update-rule" class="md-nav__link">
    <span class="md-ellipsis">
      
        The Update Rule
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-it-works" class="md-nav__link">
    <span class="md-ellipsis">
      
        Why It Works
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#stochastic-gradient-descent-sgd" class="md-nav__link">
    <span class="md-ellipsis">
      
        Stochastic Gradient Descent (SGD)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-learning-rate" class="md-nav__link">
    <span class="md-ellipsis">
      
        The Learning Rate
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="The Learning Rate">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#too-small" class="md-nav__link">
    <span class="md-ellipsis">
      
        Too Small
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#too-large" class="md-nav__link">
    <span class="md-ellipsis">
      
        Too Large
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#just-right" class="md-nav__link">
    <span class="md-ellipsis">
      
        Just Right
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#finding-the-right-learning-rate" class="md-nav__link">
    <span class="md-ellipsis">
      
        Finding the Right Learning Rate
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#our-character-lm" class="md-nav__link">
    <span class="md-ellipsis">
      
        Our Character LM
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#learning-rate-schedules" class="md-nav__link">
    <span class="md-ellipsis">
      
        Learning Rate Schedules
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Learning Rate Schedules">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#step-decay" class="md-nav__link">
    <span class="md-ellipsis">
      
        Step Decay
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#linear-decay" class="md-nav__link">
    <span class="md-ellipsis">
      
        Linear Decay
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cosine-annealing" class="md-nav__link">
    <span class="md-ellipsis">
      
        Cosine Annealing
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#warmup" class="md-nav__link">
    <span class="md-ellipsis">
      
        Warmup
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#initialization" class="md-nav__link">
    <span class="md-ellipsis">
      
        Initialization
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Initialization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#the-problem-with-zeros" class="md-nav__link">
    <span class="md-ellipsis">
      
        The Problem with Zeros
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#random-initialization" class="md-nav__link">
    <span class="md-ellipsis">
      
        Random Initialization
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-variance-problem" class="md-nav__link">
    <span class="md-ellipsis">
      
        The Variance Problem
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#xavier-initialization" class="md-nav__link">
    <span class="md-ellipsis">
      
        Xavier Initialization
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#he-initialization" class="md-nav__link">
    <span class="md-ellipsis">
      
        He Initialization
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#our-implementation" class="md-nav__link">
    <span class="md-ellipsis">
      
        Our Implementation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#batching" class="md-nav__link">
    <span class="md-ellipsis">
      
        Batching
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Batching">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mini-batch-gradient-descent" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mini-Batch Gradient Descent
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#trade-offs" class="md-nav__link">
    <span class="md-ellipsis">
      
        Trade-offs
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#implementation" class="md-nav__link">
    <span class="md-ellipsis">
      
        Implementation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#overfitting-and-regularization" class="md-nav__link">
    <span class="md-ellipsis">
      
        Overfitting and Regularization
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Overfitting and Regularization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#the-overfitting-problem" class="md-nav__link">
    <span class="md-ellipsis">
      
        The Overfitting Problem
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#trainvalidation-split" class="md-nav__link">
    <span class="md-ellipsis">
      
        Train/Validation Split
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#early-stopping" class="md-nav__link">
    <span class="md-ellipsis">
      
        Early Stopping
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#weight-decay-l2-regularization" class="md-nav__link">
    <span class="md-ellipsis">
      
        Weight Decay (L2 Regularization)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dropout" class="md-nav__link">
    <span class="md-ellipsis">
      
        Dropout
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#monitoring-training" class="md-nav__link">
    <span class="md-ellipsis">
      
        Monitoring Training
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Monitoring Training">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#what-to-track" class="md-nav__link">
    <span class="md-ellipsis">
      
        What to Track
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#implementation_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Implementation
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#what-to-watch-for" class="md-nav__link">
    <span class="md-ellipsis">
      
        What to Watch For
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#gradient-clipping" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gradient Clipping
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#a-complete-training-function" class="md-nav__link">
    <span class="md-ellipsis">
      
        A Complete Training Function
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#summary" class="md-nav__link">
    <span class="md-ellipsis">
      
        Summary
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#exercises" class="md-nav__link">
    <span class="md-ellipsis">
      
        Exercises
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#whats-next" class="md-nav__link">
    <span class="md-ellipsis">
      
        What's Next
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../07-evaluation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    3.7 Evaluation
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../stage-04-preview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Stage 4 - Coming Soon
  

    
  </span>
  
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              
              <article class="md-content__inner md-typeset">
                
                  


  
  


<h1 id="section-36-training-dynamics">Section 3.6: Training Dynamics<a class="headerlink" href="#section-36-training-dynamics" title="Permanent link">&para;</a></h1>
<p>Having a model and a loss function isn't enough. Making neural networks actually learn requires understanding training dynamics—the interplay of learning rates, initialization, and optimization.</p>
<p><strong>This section covers the practical art of training neural language models.</strong></p>
<h2 id="the-optimization-landscape">The Optimization Landscape<a class="headerlink" href="#the-optimization-landscape" title="Permanent link">&para;</a></h2>
<h3 id="what-are-we-optimizing">What Are We Optimizing?<a class="headerlink" href="#what-are-we-optimizing" title="Permanent link">&para;</a></h3>
<p>The loss function L(θ) defines a surface over parameter space. For our language model:</p>
<div class="arithmatex">\[L(\theta) = -\frac{1}{N}\sum_{i=1}^{N} \log P(y_i | x_i; \theta)\]</div>
<p>Where θ includes all embeddings, weights, and biases.</p>
<h3 id="visualizing-the-landscape">Visualizing the Landscape<a class="headerlink" href="#visualizing-the-landscape" title="Permanent link">&para;</a></h3>
<p>For 2 parameters, we can plot L as a 3D surface:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a>L(θ)
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>  │    ╱\
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>  │   /  \\     /\
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>  │  /    \___/  \
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>  │ /            \
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a>  └───────────────── θ
</code></pre></div>
<p>Real networks have millions of parameters—the surface is in million-dimensional space!</p>
<h3 id="key-properties">Key Properties<a class="headerlink" href="#key-properties" title="Permanent link">&para;</a></h3>
<p><strong>Non-convex</strong>: Multiple local minima. No guarantee of finding the global minimum.</p>
<p><strong>High-dimensional</strong>: In high dimensions, most critical points are saddle points, not local minima. This is actually good—harder to get stuck.</p>
<p><strong>Flat regions</strong>: Some directions have near-zero gradient. Training can plateau.</p>
<h2 id="gradient-descent-the-basic-algorithm">Gradient Descent: The Basic Algorithm<a class="headerlink" href="#gradient-descent-the-basic-algorithm" title="Permanent link">&para;</a></h2>
<h3 id="the-update-rule">The Update Rule<a class="headerlink" href="#the-update-rule" title="Permanent link">&para;</a></h3>
<p>Given current parameters θ and learning rate η:</p>
<div class="arithmatex">\[\theta_{t+1} = \theta_t - \eta \nabla_\theta L(\theta_t)\]</div>
<p>We move in the direction of steepest descent (negative gradient).</p>
<h3 id="why-it-works">Why It Works<a class="headerlink" href="#why-it-works" title="Permanent link">&para;</a></h3>
<p>Taylor expansion around current point:</p>
<div class="arithmatex">\[L(\theta + \Delta\theta) \approx L(\theta) + \nabla L \cdot \Delta\theta\]</div>
<p>To decrease L, we want <span class="arithmatex">\(\nabla L \cdot \Delta\theta &lt; 0\)</span>.</p>
<p>Choosing <span class="arithmatex">\(\Delta\theta = -\eta \nabla L\)</span>:</p>
<div class="arithmatex">\[\nabla L \cdot (-\eta \nabla L) = -\eta ||\nabla L||^2 &lt; 0\]</div>
<p>The loss decreases (for small enough η).</p>
<h3 id="stochastic-gradient-descent-sgd">Stochastic Gradient Descent (SGD)<a class="headerlink" href="#stochastic-gradient-descent-sgd" title="Permanent link">&para;</a></h3>
<p>Computing the gradient over all N examples is expensive. Instead:</p>
<ol>
<li>Sample a single example (or mini-batch)</li>
<li>Compute gradient on that sample</li>
<li>Update parameters</li>
<li>Repeat</li>
</ol>
<p>The gradient estimate is noisy but unbiased:</p>
<div class="arithmatex">\[\mathbb{E}[\nabla L_i] = \nabla L\]</div>
<p>This noise can actually help escape local minima!</p>
<h2 id="the-learning-rate">The Learning Rate<a class="headerlink" href="#the-learning-rate" title="Permanent link">&para;</a></h2>
<p>The learning rate η is the most important hyperparameter.</p>
<h3 id="too-small">Too Small<a class="headerlink" href="#too-small" title="Permanent link">&para;</a></h3>
<ul>
<li>Very slow progress</li>
<li>May never reach good solution</li>
<li>Training takes forever</li>
</ul>
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a>Loss
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a>  │\
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a>  │ \__________
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a>  │            \_____
<a id="__codelineno-1-5" name="__codelineno-1-5" href="#__codelineno-1-5"></a>  │                  \____...
<a id="__codelineno-1-6" name="__codelineno-1-6" href="#__codelineno-1-6"></a>  └──────────────────────── Epochs
</code></pre></div>
<h3 id="too-large">Too Large<a class="headerlink" href="#too-large" title="Permanent link">&para;</a></h3>
<ul>
<li>Overshoots optimal values</li>
<li>Loss oscillates or diverges</li>
<li>Training becomes unstable</li>
</ul>
<div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a>Loss
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#__codelineno-2-2"></a>  │
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#__codelineno-2-3"></a>  │    /\    /\    /\
<a id="__codelineno-2-4" name="__codelineno-2-4" href="#__codelineno-2-4"></a>  │   /  \  /  \  /
<a id="__codelineno-2-5" name="__codelineno-2-5" href="#__codelineno-2-5"></a>  │  /    \/    \/
<a id="__codelineno-2-6" name="__codelineno-2-6" href="#__codelineno-2-6"></a>  │ /
<a id="__codelineno-2-7" name="__codelineno-2-7" href="#__codelineno-2-7"></a>  └──────────────────── Epochs
</code></pre></div>
<h3 id="just-right">Just Right<a class="headerlink" href="#just-right" title="Permanent link">&para;</a></h3>
<ul>
<li>Steady decrease</li>
<li>Converges to good minimum</li>
<li>Can explore and then settle</li>
</ul>
<div class="highlight"><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#__codelineno-3-1"></a>Loss
<a id="__codelineno-3-2" name="__codelineno-3-2" href="#__codelineno-3-2"></a>  │\
<a id="__codelineno-3-3" name="__codelineno-3-3" href="#__codelineno-3-3"></a>  │ \
<a id="__codelineno-3-4" name="__codelineno-3-4" href="#__codelineno-3-4"></a>  │  \.
<a id="__codelineno-3-5" name="__codelineno-3-5" href="#__codelineno-3-5"></a>  │   &#39;·..
<a id="__codelineno-3-6" name="__codelineno-3-6" href="#__codelineno-3-6"></a>  │       &#39;&#39;&#39;&#39;·····
<a id="__codelineno-3-7" name="__codelineno-3-7" href="#__codelineno-3-7"></a>  └──────────────────── Epochs
</code></pre></div>
<h3 id="finding-the-right-learning-rate">Finding the Right Learning Rate<a class="headerlink" href="#finding-the-right-learning-rate" title="Permanent link">&para;</a></h3>
<p><strong>Rule of thumb</strong>: Start with 0.01, adjust by factors of 10.</p>
<p><strong>Learning rate finder</strong>: Gradually increase LR, plot loss. Use value just before loss explodes.</p>
<p><strong>Common values</strong>:
- 0.1: Often too high for deep nets
- 0.01: Good starting point
- 0.001: Common for fine-tuning
- 0.0001: Very conservative</p>
<h3 id="our-character-lm">Our Character LM<a class="headerlink" href="#our-character-lm" title="Permanent link">&para;</a></h3>
<p>For our model, try:
- Start: η = 0.1 (aggressive)
- If unstable: reduce to 0.01
- If too slow: increase to 0.5</p>
<h2 id="learning-rate-schedules">Learning Rate Schedules<a class="headerlink" href="#learning-rate-schedules" title="Permanent link">&para;</a></h2>
<p>A fixed learning rate isn't optimal. Better: change η during training.</p>
<h3 id="step-decay">Step Decay<a class="headerlink" href="#step-decay" title="Permanent link">&para;</a></h3>
<p>Reduce by factor every K epochs:</p>
<div class="arithmatex">\[\eta_t = \eta_0 \cdot \gamma^{\lfloor t/K \rfloor}\]</div>
<p>Example: Start at 0.1, multiply by 0.5 every 10 epochs.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-4-1" name="__codelineno-4-1" href="#__codelineno-4-1"></a><span class="k">def</span><span class="w"> </span><span class="nf">step_decay</span><span class="p">(</span><span class="n">initial_lr</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">decay_rate</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">decay_every</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
<a id="__codelineno-4-2" name="__codelineno-4-2" href="#__codelineno-4-2"></a>    <span class="k">return</span> <span class="n">initial_lr</span> <span class="o">*</span> <span class="p">(</span><span class="n">decay_rate</span> <span class="o">**</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">//</span> <span class="n">decay_every</span><span class="p">))</span>
</code></pre></div>
<h3 id="linear-decay">Linear Decay<a class="headerlink" href="#linear-decay" title="Permanent link">&para;</a></h3>
<p>Decrease linearly to zero:</p>
<div class="arithmatex">\[\eta_t = \eta_0 \cdot \left(1 - \frac{t}{T}\right)\]</div>
<div class="highlight"><pre><span></span><code><a id="__codelineno-5-1" name="__codelineno-5-1" href="#__codelineno-5-1"></a><span class="k">def</span><span class="w"> </span><span class="nf">linear_decay</span><span class="p">(</span><span class="n">initial_lr</span><span class="p">,</span> <span class="n">step</span><span class="p">,</span> <span class="n">total_steps</span><span class="p">):</span>
<a id="__codelineno-5-2" name="__codelineno-5-2" href="#__codelineno-5-2"></a>    <span class="k">return</span> <span class="n">initial_lr</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">step</span> <span class="o">/</span> <span class="n">total_steps</span><span class="p">)</span>
</code></pre></div>
<h3 id="cosine-annealing">Cosine Annealing<a class="headerlink" href="#cosine-annealing" title="Permanent link">&para;</a></h3>
<p>Smooth decrease following cosine:</p>
<div class="arithmatex">\[\eta_t = \eta_{\min} + \frac{1}{2}(\eta_{\max} - \eta_{\min})\left(1 + \cos\left(\frac{t}{T}\pi\right)\right)\]</div>
<p>Popular in modern training.</p>
<h3 id="warmup">Warmup<a class="headerlink" href="#warmup" title="Permanent link">&para;</a></h3>
<p>Start with tiny learning rate, gradually increase:</p>
<div class="arithmatex">\[\eta_t = \eta_{\max} \cdot \frac{t}{T_{\text{warmup}}}\]</div>
<p>Then decay. Helps stabilize early training when gradients are large.</p>
<h2 id="initialization">Initialization<a class="headerlink" href="#initialization" title="Permanent link">&para;</a></h2>
<p>How we initialize parameters affects training dramatically.</p>
<h3 id="the-problem-with-zeros">The Problem with Zeros<a class="headerlink" href="#the-problem-with-zeros" title="Permanent link">&para;</a></h3>
<p>If all weights are zero:
- All neurons compute the same thing
- All gradients are the same
- Symmetry is never broken</p>
<p><strong>Never initialize weights to zero!</strong> (Biases to zero are okay.)</p>
<h3 id="random-initialization">Random Initialization<a class="headerlink" href="#random-initialization" title="Permanent link">&para;</a></h3>
<p>Simple approach: small random values.</p>
<div class="arithmatex">\[W_{ij} \sim \mathcal{N}(0, \sigma^2)\]</div>
<p>But what should σ be?</p>
<h3 id="the-variance-problem">The Variance Problem<a class="headerlink" href="#the-variance-problem" title="Permanent link">&para;</a></h3>
<p>Consider a single layer: y = Wx where x ∈ ℝⁿ.</p>
<p>If <span class="arithmatex">\(x_i\)</span> has variance <span class="arithmatex">\(\text{Var}(x)\)</span> and <span class="arithmatex">\(W_{ij}\)</span> has variance σ²:</p>
<div class="arithmatex">\[\text{Var}(y_j) = \sum_{i=1}^{n} \text{Var}(W_{ij}) \cdot \text{Var}(x_i) = n \cdot \sigma^2 \cdot \text{Var}(x)\]</div>
<p>The variance grows by factor n!</p>
<p>For deep networks, this compounds: variance explodes or vanishes.</p>
<h3 id="xavier-initialization">Xavier Initialization<a class="headerlink" href="#xavier-initialization" title="Permanent link">&para;</a></h3>
<p>Solution: scale by input dimension.</p>
<div class="arithmatex">\[W_{ij} \sim \mathcal{N}\left(0, \frac{1}{n_{\text{in}}}\right)\]</div>
<p>Or for uniform distribution:
$<span class="arithmatex">\(W_{ij} \sim \text{Uniform}\left(-\sqrt{\frac{3}{n_{\text{in}}}}, \sqrt{\frac{3}{n_{\text{in}}}}\right)\)</span>$</p>
<p>Now:
$<span class="arithmatex">\(\text{Var}(y_j) = n \cdot \frac{1}{n} \cdot \text{Var}(x) = \text{Var}(x)\)</span>$</p>
<p>Variance is preserved!</p>
<h3 id="he-initialization">He Initialization<a class="headerlink" href="#he-initialization" title="Permanent link">&para;</a></h3>
<p>For ReLU activations, Xavier underestimates because ReLU zeros half the values.</p>
<p>He initialization:
$<span class="arithmatex">\(W_{ij} \sim \mathcal{N}\left(0, \frac{2}{n_{\text{in}}}\right)\)</span>$</p>
<p>The factor of 2 compensates for ReLU.</p>
<h3 id="our-implementation">Our Implementation<a class="headerlink" href="#our-implementation" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><a id="__codelineno-6-1" name="__codelineno-6-1" href="#__codelineno-6-1"></a><span class="k">def</span><span class="w"> </span><span class="nf">init_weights</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">):</span>
<a id="__codelineno-6-2" name="__codelineno-6-2" href="#__codelineno-6-2"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Initialize weight matrix with appropriate scaling.&quot;&quot;&quot;</span>
<a id="__codelineno-6-3" name="__codelineno-6-3" href="#__codelineno-6-3"></a>    <span class="n">n_in</span><span class="p">,</span> <span class="n">n_out</span> <span class="o">=</span> <span class="n">shape</span>
<a id="__codelineno-6-4" name="__codelineno-6-4" href="#__codelineno-6-4"></a>    <span class="k">if</span> <span class="n">activation</span> <span class="o">==</span> <span class="s1">&#39;relu&#39;</span><span class="p">:</span>
<a id="__codelineno-6-5" name="__codelineno-6-5" href="#__codelineno-6-5"></a>        <span class="n">scale</span> <span class="o">=</span> <span class="p">(</span><span class="mf">2.0</span> <span class="o">/</span> <span class="n">n_in</span><span class="p">)</span> <span class="o">**</span> <span class="mf">0.5</span>  <span class="c1"># He</span>
<a id="__codelineno-6-6" name="__codelineno-6-6" href="#__codelineno-6-6"></a>    <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-6-7" name="__codelineno-6-7" href="#__codelineno-6-7"></a>        <span class="n">scale</span> <span class="o">=</span> <span class="p">(</span><span class="mf">2.0</span> <span class="o">/</span> <span class="p">(</span><span class="n">n_in</span> <span class="o">+</span> <span class="n">n_out</span><span class="p">))</span> <span class="o">**</span> <span class="mf">0.5</span>  <span class="c1"># Xavier</span>
<a id="__codelineno-6-8" name="__codelineno-6-8" href="#__codelineno-6-8"></a>    <span class="k">return</span> <span class="p">[[</span><span class="n">Value</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">gauss</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="p">))</span>
<a id="__codelineno-6-9" name="__codelineno-6-9" href="#__codelineno-6-9"></a>             <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_in</span><span class="p">)]</span>
<a id="__codelineno-6-10" name="__codelineno-6-10" href="#__codelineno-6-10"></a>            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_out</span><span class="p">)]</span>
</code></pre></div>
<h2 id="batching">Batching<a class="headerlink" href="#batching" title="Permanent link">&para;</a></h2>
<p>Processing one example at a time is inefficient and noisy.</p>
<h3 id="mini-batch-gradient-descent">Mini-Batch Gradient Descent<a class="headerlink" href="#mini-batch-gradient-descent" title="Permanent link">&para;</a></h3>
<p>Process B examples together:</p>
<div class="arithmatex">\[\nabla L = \frac{1}{B}\sum_{i=1}^{B} \nabla L_i\]</div>
<p><strong>Advantages</strong>:
- More stable gradients (averaging reduces variance)
- Computational efficiency (parallelism)
- Better generalization (noise helps)</p>
<p><strong>Common batch sizes</strong>: 32, 64, 128, 256</p>
<h3 id="trade-offs">Trade-offs<a class="headerlink" href="#trade-offs" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Batch Size</th>
<th>Gradient Variance</th>
<th>Computation</th>
<th>Generalization</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Very high</td>
<td>Slow</td>
<td>Good</td>
</tr>
<tr>
<td>32-128</td>
<td>Medium</td>
<td>Fast</td>
<td>Good</td>
</tr>
<tr>
<td>1000+</td>
<td>Low</td>
<td>Very fast</td>
<td>May overfit</td>
</tr>
</tbody>
</table>
<h3 id="implementation">Implementation<a class="headerlink" href="#implementation" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><a id="__codelineno-7-1" name="__codelineno-7-1" href="#__codelineno-7-1"></a><span class="k">def</span><span class="w"> </span><span class="nf">create_batches</span><span class="p">(</span><span class="n">examples</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
<a id="__codelineno-7-2" name="__codelineno-7-2" href="#__codelineno-7-2"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Split examples into mini-batches.&quot;&quot;&quot;</span>
<a id="__codelineno-7-3" name="__codelineno-7-3" href="#__codelineno-7-3"></a>    <span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">examples</span><span class="p">)</span>
<a id="__codelineno-7-4" name="__codelineno-7-4" href="#__codelineno-7-4"></a>    <span class="n">batches</span> <span class="o">=</span> <span class="p">[]</span>
<a id="__codelineno-7-5" name="__codelineno-7-5" href="#__codelineno-7-5"></a>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">examples</span><span class="p">),</span> <span class="n">batch_size</span><span class="p">):</span>
<a id="__codelineno-7-6" name="__codelineno-7-6" href="#__codelineno-7-6"></a>        <span class="n">batches</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">examples</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="n">batch_size</span><span class="p">])</span>
<a id="__codelineno-7-7" name="__codelineno-7-7" href="#__codelineno-7-7"></a>    <span class="k">return</span> <span class="n">batches</span>
<a id="__codelineno-7-8" name="__codelineno-7-8" href="#__codelineno-7-8"></a>
<a id="__codelineno-7-9" name="__codelineno-7-9" href="#__codelineno-7-9"></a>
<a id="__codelineno-7-10" name="__codelineno-7-10" href="#__codelineno-7-10"></a><span class="k">def</span><span class="w"> </span><span class="nf">train_batch</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
<a id="__codelineno-7-11" name="__codelineno-7-11" href="#__codelineno-7-11"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Train on a single mini-batch.&quot;&quot;&quot;</span>
<a id="__codelineno-7-12" name="__codelineno-7-12" href="#__codelineno-7-12"></a>    <span class="n">params</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span>
<a id="__codelineno-7-13" name="__codelineno-7-13" href="#__codelineno-7-13"></a>
<a id="__codelineno-7-14" name="__codelineno-7-14" href="#__codelineno-7-14"></a>    <span class="c1"># Forward pass and accumulate loss</span>
<a id="__codelineno-7-15" name="__codelineno-7-15" href="#__codelineno-7-15"></a>    <span class="n">total_loss</span> <span class="o">=</span> <span class="n">Value</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
<a id="__codelineno-7-16" name="__codelineno-7-16" href="#__codelineno-7-16"></a>    <span class="k">for</span> <span class="n">context</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">:</span>
<a id="__codelineno-7-17" name="__codelineno-7-17" href="#__codelineno-7-17"></a>        <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<a id="__codelineno-7-18" name="__codelineno-7-18" href="#__codelineno-7-18"></a>        <span class="n">total_loss</span> <span class="o">=</span> <span class="n">total_loss</span> <span class="o">+</span> <span class="n">loss</span>
<a id="__codelineno-7-19" name="__codelineno-7-19" href="#__codelineno-7-19"></a>
<a id="__codelineno-7-20" name="__codelineno-7-20" href="#__codelineno-7-20"></a>    <span class="n">avg_loss</span> <span class="o">=</span> <span class="n">total_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
<a id="__codelineno-7-21" name="__codelineno-7-21" href="#__codelineno-7-21"></a>
<a id="__codelineno-7-22" name="__codelineno-7-22" href="#__codelineno-7-22"></a>    <span class="c1"># Zero gradients</span>
<a id="__codelineno-7-23" name="__codelineno-7-23" href="#__codelineno-7-23"></a>    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">params</span><span class="p">:</span>
<a id="__codelineno-7-24" name="__codelineno-7-24" href="#__codelineno-7-24"></a>        <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="mf">0.0</span>
<a id="__codelineno-7-25" name="__codelineno-7-25" href="#__codelineno-7-25"></a>
<a id="__codelineno-7-26" name="__codelineno-7-26" href="#__codelineno-7-26"></a>    <span class="c1"># Backward pass</span>
<a id="__codelineno-7-27" name="__codelineno-7-27" href="#__codelineno-7-27"></a>    <span class="n">avg_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<a id="__codelineno-7-28" name="__codelineno-7-28" href="#__codelineno-7-28"></a>
<a id="__codelineno-7-29" name="__codelineno-7-29" href="#__codelineno-7-29"></a>    <span class="c1"># Update</span>
<a id="__codelineno-7-30" name="__codelineno-7-30" href="#__codelineno-7-30"></a>    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">params</span><span class="p">:</span>
<a id="__codelineno-7-31" name="__codelineno-7-31" href="#__codelineno-7-31"></a>        <span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span>
<a id="__codelineno-7-32" name="__codelineno-7-32" href="#__codelineno-7-32"></a>
<a id="__codelineno-7-33" name="__codelineno-7-33" href="#__codelineno-7-33"></a>    <span class="k">return</span> <span class="n">avg_loss</span><span class="o">.</span><span class="n">data</span>
</code></pre></div>
<h2 id="overfitting-and-regularization">Overfitting and Regularization<a class="headerlink" href="#overfitting-and-regularization" title="Permanent link">&para;</a></h2>
<h3 id="the-overfitting-problem">The Overfitting Problem<a class="headerlink" href="#the-overfitting-problem" title="Permanent link">&para;</a></h3>
<p>With enough parameters, the model can memorize training data perfectly—but fail on new data.</p>
<p><strong>Signs of overfitting</strong>:
- Training loss keeps decreasing
- Validation loss starts increasing
- Large gap between train and validation loss</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-8-1" name="__codelineno-8-1" href="#__codelineno-8-1"></a>Loss
<a id="__codelineno-8-2" name="__codelineno-8-2" href="#__codelineno-8-2"></a>  │\
<a id="__codelineno-8-3" name="__codelineno-8-3" href="#__codelineno-8-3"></a>  │ \  training
<a id="__codelineno-8-4" name="__codelineno-8-4" href="#__codelineno-8-4"></a>  │  \____________________
<a id="__codelineno-8-5" name="__codelineno-8-5" href="#__codelineno-8-5"></a>  │      ╱
<a id="__codelineno-8-6" name="__codelineno-8-6" href="#__codelineno-8-6"></a>  │     /  validation
<a id="__codelineno-8-7" name="__codelineno-8-7" href="#__codelineno-8-7"></a>  │    /&#39;&#39;·····
<a id="__codelineno-8-8" name="__codelineno-8-8" href="#__codelineno-8-8"></a>  └──────────────────────── Epochs
</code></pre></div>
<h3 id="trainvalidation-split">Train/Validation Split<a class="headerlink" href="#trainvalidation-split" title="Permanent link">&para;</a></h3>
<p>Always evaluate on held-out data:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-9-1" name="__codelineno-9-1" href="#__codelineno-9-1"></a><span class="k">def</span><span class="w"> </span><span class="nf">split_data</span><span class="p">(</span><span class="n">examples</span><span class="p">,</span> <span class="n">val_fraction</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
<a id="__codelineno-9-2" name="__codelineno-9-2" href="#__codelineno-9-2"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Split examples into train and validation.&quot;&quot;&quot;</span>
<a id="__codelineno-9-3" name="__codelineno-9-3" href="#__codelineno-9-3"></a>    <span class="n">n_val</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">examples</span><span class="p">)</span> <span class="o">*</span> <span class="n">val_fraction</span><span class="p">)</span>
<a id="__codelineno-9-4" name="__codelineno-9-4" href="#__codelineno-9-4"></a>    <span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">examples</span><span class="p">)</span>
<a id="__codelineno-9-5" name="__codelineno-9-5" href="#__codelineno-9-5"></a>    <span class="k">return</span> <span class="n">examples</span><span class="p">[</span><span class="n">n_val</span><span class="p">:],</span> <span class="n">examples</span><span class="p">[:</span><span class="n">n_val</span><span class="p">]</span>
</code></pre></div>
<h3 id="early-stopping">Early Stopping<a class="headerlink" href="#early-stopping" title="Permanent link">&para;</a></h3>
<p>Stop training when validation loss stops improving:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-10-1" name="__codelineno-10-1" href="#__codelineno-10-1"></a><span class="k">def</span><span class="w"> </span><span class="nf">train_with_early_stopping</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_examples</span><span class="p">,</span> <span class="n">val_examples</span><span class="p">,</span>
<a id="__codelineno-10-2" name="__codelineno-10-2" href="#__codelineno-10-2"></a>                               <span class="n">patience</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">max_epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
<a id="__codelineno-10-3" name="__codelineno-10-3" href="#__codelineno-10-3"></a>    <span class="n">best_val_loss</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">)</span>
<a id="__codelineno-10-4" name="__codelineno-10-4" href="#__codelineno-10-4"></a>    <span class="n">epochs_without_improvement</span> <span class="o">=</span> <span class="mi">0</span>
<a id="__codelineno-10-5" name="__codelineno-10-5" href="#__codelineno-10-5"></a>
<a id="__codelineno-10-6" name="__codelineno-10-6" href="#__codelineno-10-6"></a>    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_epochs</span><span class="p">):</span>
<a id="__codelineno-10-7" name="__codelineno-10-7" href="#__codelineno-10-7"></a>        <span class="c1"># Train one epoch</span>
<a id="__codelineno-10-8" name="__codelineno-10-8" href="#__codelineno-10-8"></a>        <span class="n">train_loss</span> <span class="o">=</span> <span class="n">train_epoch</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_examples</span><span class="p">)</span>
<a id="__codelineno-10-9" name="__codelineno-10-9" href="#__codelineno-10-9"></a>
<a id="__codelineno-10-10" name="__codelineno-10-10" href="#__codelineno-10-10"></a>        <span class="c1"># Evaluate</span>
<a id="__codelineno-10-11" name="__codelineno-10-11" href="#__codelineno-10-11"></a>        <span class="n">val_loss</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">val_examples</span><span class="p">)</span>
<a id="__codelineno-10-12" name="__codelineno-10-12" href="#__codelineno-10-12"></a>
<a id="__codelineno-10-13" name="__codelineno-10-13" href="#__codelineno-10-13"></a>        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">: train=</span><span class="si">{</span><span class="n">train_loss</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, val=</span><span class="si">{</span><span class="n">val_loss</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<a id="__codelineno-10-14" name="__codelineno-10-14" href="#__codelineno-10-14"></a>
<a id="__codelineno-10-15" name="__codelineno-10-15" href="#__codelineno-10-15"></a>        <span class="c1"># Check for improvement</span>
<a id="__codelineno-10-16" name="__codelineno-10-16" href="#__codelineno-10-16"></a>        <span class="k">if</span> <span class="n">val_loss</span> <span class="o">&lt;</span> <span class="n">best_val_loss</span><span class="p">:</span>
<a id="__codelineno-10-17" name="__codelineno-10-17" href="#__codelineno-10-17"></a>            <span class="n">best_val_loss</span> <span class="o">=</span> <span class="n">val_loss</span>
<a id="__codelineno-10-18" name="__codelineno-10-18" href="#__codelineno-10-18"></a>            <span class="n">epochs_without_improvement</span> <span class="o">=</span> <span class="mi">0</span>
<a id="__codelineno-10-19" name="__codelineno-10-19" href="#__codelineno-10-19"></a>            <span class="c1"># Save best model weights here</span>
<a id="__codelineno-10-20" name="__codelineno-10-20" href="#__codelineno-10-20"></a>        <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-10-21" name="__codelineno-10-21" href="#__codelineno-10-21"></a>            <span class="n">epochs_without_improvement</span> <span class="o">+=</span> <span class="mi">1</span>
<a id="__codelineno-10-22" name="__codelineno-10-22" href="#__codelineno-10-22"></a>
<a id="__codelineno-10-23" name="__codelineno-10-23" href="#__codelineno-10-23"></a>        <span class="c1"># Early stop</span>
<a id="__codelineno-10-24" name="__codelineno-10-24" href="#__codelineno-10-24"></a>        <span class="k">if</span> <span class="n">epochs_without_improvement</span> <span class="o">&gt;=</span> <span class="n">patience</span><span class="p">:</span>
<a id="__codelineno-10-25" name="__codelineno-10-25" href="#__codelineno-10-25"></a>            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Early stopping at epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<a id="__codelineno-10-26" name="__codelineno-10-26" href="#__codelineno-10-26"></a>            <span class="k">break</span>
</code></pre></div>
<h3 id="weight-decay-l2-regularization">Weight Decay (L2 Regularization)<a class="headerlink" href="#weight-decay-l2-regularization" title="Permanent link">&para;</a></h3>
<p>Add penalty for large weights:</p>
<div class="arithmatex">\[L_{\text{total}} = L + \lambda \sum_i \theta_i^2\]</div>
<p>This encourages smaller weights, reducing overfitting.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-11-1" name="__codelineno-11-1" href="#__codelineno-11-1"></a><span class="k">def</span><span class="w"> </span><span class="nf">apply_weight_decay</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">weight_decay</span><span class="p">):</span>
<a id="__codelineno-11-2" name="__codelineno-11-2" href="#__codelineno-11-2"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Apply L2 regularization.&quot;&quot;&quot;</span>
<a id="__codelineno-11-3" name="__codelineno-11-3" href="#__codelineno-11-3"></a>    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">params</span><span class="p">:</span>
<a id="__codelineno-11-4" name="__codelineno-11-4" href="#__codelineno-11-4"></a>        <span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">weight_decay</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span>
</code></pre></div>
<p>In practice, combine with gradient update:</p>
<div class="arithmatex">\[\theta_{t+1} = \theta_t - \eta(\nabla L + \lambda \theta_t) = (1 - \eta\lambda)\theta_t - \eta \nabla L\]</div>
<h3 id="dropout">Dropout<a class="headerlink" href="#dropout" title="Permanent link">&para;</a></h3>
<p>During training, randomly zero some activations:</p>
<div class="arithmatex">\[h_i^{\text{dropped}} = h_i \cdot m_i\]</div>
<p>Where <span class="arithmatex">\(m_i \sim \text{Bernoulli}(1 - p)\)</span> and p is the dropout probability.</p>
<p>At test time, scale by (1-p) or use all activations.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-12-1" name="__codelineno-12-1" href="#__codelineno-12-1"></a><span class="k">def</span><span class="w"> </span><span class="nf">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<a id="__codelineno-12-2" name="__codelineno-12-2" href="#__codelineno-12-2"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Apply dropout to list of Values.&quot;&quot;&quot;</span>
<a id="__codelineno-12-3" name="__codelineno-12-3" href="#__codelineno-12-3"></a>    <span class="k">if</span> <span class="ow">not</span> <span class="n">training</span><span class="p">:</span>
<a id="__codelineno-12-4" name="__codelineno-12-4" href="#__codelineno-12-4"></a>        <span class="k">return</span> <span class="n">x</span>
<a id="__codelineno-12-5" name="__codelineno-12-5" href="#__codelineno-12-5"></a>    <span class="n">mask</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span> <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&gt;</span> <span class="n">p</span> <span class="k">else</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">x</span><span class="p">]</span>
<a id="__codelineno-12-6" name="__codelineno-12-6" href="#__codelineno-12-6"></a>    <span class="n">scale</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">p</span><span class="p">)</span>  <span class="c1"># Scale to maintain expected value</span>
<a id="__codelineno-12-7" name="__codelineno-12-7" href="#__codelineno-12-7"></a>    <span class="k">return</span> <span class="p">[</span><span class="n">v</span> <span class="o">*</span> <span class="n">m</span> <span class="o">*</span> <span class="n">scale</span> <span class="k">for</span> <span class="n">v</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">)]</span>
</code></pre></div>
<h2 id="monitoring-training">Monitoring Training<a class="headerlink" href="#monitoring-training" title="Permanent link">&para;</a></h2>
<h3 id="what-to-track">What to Track<a class="headerlink" href="#what-to-track" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Training loss</strong>: Should decrease</li>
<li><strong>Validation loss</strong>: Should decrease, watch for divergence from training</li>
<li><strong>Perplexity</strong>: exp(loss), more interpretable</li>
<li><strong>Gradient norms</strong>: Should be stable, not exploding/vanishing</li>
<li><strong>Parameter norms</strong>: Shouldn't grow unboundedly</li>
</ol>
<h3 id="implementation_1">Implementation<a class="headerlink" href="#implementation_1" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><a id="__codelineno-13-1" name="__codelineno-13-1" href="#__codelineno-13-1"></a><span class="k">def</span><span class="w"> </span><span class="nf">compute_gradient_norm</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
<a id="__codelineno-13-2" name="__codelineno-13-2" href="#__codelineno-13-2"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute L2 norm of all gradients.&quot;&quot;&quot;</span>
<a id="__codelineno-13-3" name="__codelineno-13-3" href="#__codelineno-13-3"></a>    <span class="n">total</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">**</span> <span class="mi">2</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">params</span><span class="p">)</span>
<a id="__codelineno-13-4" name="__codelineno-13-4" href="#__codelineno-13-4"></a>    <span class="k">return</span> <span class="n">total</span> <span class="o">**</span> <span class="mf">0.5</span>
<a id="__codelineno-13-5" name="__codelineno-13-5" href="#__codelineno-13-5"></a>
<a id="__codelineno-13-6" name="__codelineno-13-6" href="#__codelineno-13-6"></a>
<a id="__codelineno-13-7" name="__codelineno-13-7" href="#__codelineno-13-7"></a><span class="k">def</span><span class="w"> </span><span class="nf">compute_param_norm</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
<a id="__codelineno-13-8" name="__codelineno-13-8" href="#__codelineno-13-8"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute L2 norm of all parameters.&quot;&quot;&quot;</span>
<a id="__codelineno-13-9" name="__codelineno-13-9" href="#__codelineno-13-9"></a>    <span class="n">total</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">**</span> <span class="mi">2</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">params</span><span class="p">)</span>
<a id="__codelineno-13-10" name="__codelineno-13-10" href="#__codelineno-13-10"></a>    <span class="k">return</span> <span class="n">total</span> <span class="o">**</span> <span class="mf">0.5</span>
</code></pre></div>
<h3 id="what-to-watch-for">What to Watch For<a class="headerlink" href="#what-to-watch-for" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Symptom</th>
<th>Likely Cause</th>
<th>Solution</th>
</tr>
</thead>
<tbody>
<tr>
<td>Loss stays flat</td>
<td>LR too small, or stuck</td>
<td>Increase LR, reinitialize</td>
</tr>
<tr>
<td>Loss explodes</td>
<td>LR too large</td>
<td>Reduce LR, gradient clipping</td>
</tr>
<tr>
<td>Val &gt; Train</td>
<td>Overfitting</td>
<td>Regularization, early stopping</td>
</tr>
<tr>
<td>Loss oscillates</td>
<td>LR too large</td>
<td>Reduce LR</td>
</tr>
<tr>
<td>Gradients → 0</td>
<td>Vanishing gradients</td>
<td>Better init, skip connections</td>
</tr>
<tr>
<td>Gradients → ∞</td>
<td>Exploding gradients</td>
<td>Gradient clipping, smaller LR</td>
</tr>
</tbody>
</table>
<h2 id="gradient-clipping">Gradient Clipping<a class="headerlink" href="#gradient-clipping" title="Permanent link">&para;</a></h2>
<p>Prevent exploding gradients by capping the gradient norm:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-14-1" name="__codelineno-14-1" href="#__codelineno-14-1"></a><span class="k">def</span><span class="w"> </span><span class="nf">clip_gradients</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">max_norm</span><span class="p">):</span>
<a id="__codelineno-14-2" name="__codelineno-14-2" href="#__codelineno-14-2"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Clip gradients to maximum norm.&quot;&quot;&quot;</span>
<a id="__codelineno-14-3" name="__codelineno-14-3" href="#__codelineno-14-3"></a>    <span class="n">total_norm</span> <span class="o">=</span> <span class="n">compute_gradient_norm</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
<a id="__codelineno-14-4" name="__codelineno-14-4" href="#__codelineno-14-4"></a>    <span class="k">if</span> <span class="n">total_norm</span> <span class="o">&gt;</span> <span class="n">max_norm</span><span class="p">:</span>
<a id="__codelineno-14-5" name="__codelineno-14-5" href="#__codelineno-14-5"></a>        <span class="n">scale</span> <span class="o">=</span> <span class="n">max_norm</span> <span class="o">/</span> <span class="n">total_norm</span>
<a id="__codelineno-14-6" name="__codelineno-14-6" href="#__codelineno-14-6"></a>        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">params</span><span class="p">:</span>
<a id="__codelineno-14-7" name="__codelineno-14-7" href="#__codelineno-14-7"></a>            <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">*=</span> <span class="n">scale</span>
</code></pre></div>
<p>This is especially important for language models, where certain inputs can cause large gradients.</p>
<h2 id="a-complete-training-function">A Complete Training Function<a class="headerlink" href="#a-complete-training-function" title="Permanent link">&para;</a></h2>
<p>Putting it all together:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-15-1" name="__codelineno-15-1" href="#__codelineno-15-1"></a><span class="k">def</span><span class="w"> </span><span class="nf">train_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_data</span><span class="p">,</span> <span class="n">val_data</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
<a id="__codelineno-15-2" name="__codelineno-15-2" href="#__codelineno-15-2"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<a id="__codelineno-15-3" name="__codelineno-15-3" href="#__codelineno-15-3"></a><span class="sd">    Complete training loop with all best practices.</span>
<a id="__codelineno-15-4" name="__codelineno-15-4" href="#__codelineno-15-4"></a>
<a id="__codelineno-15-5" name="__codelineno-15-5" href="#__codelineno-15-5"></a><span class="sd">    config: dict with hyperparameters</span>
<a id="__codelineno-15-6" name="__codelineno-15-6" href="#__codelineno-15-6"></a><span class="sd">        - epochs: max training epochs</span>
<a id="__codelineno-15-7" name="__codelineno-15-7" href="#__codelineno-15-7"></a><span class="sd">        - batch_size: mini-batch size</span>
<a id="__codelineno-15-8" name="__codelineno-15-8" href="#__codelineno-15-8"></a><span class="sd">        - learning_rate: initial learning rate</span>
<a id="__codelineno-15-9" name="__codelineno-15-9" href="#__codelineno-15-9"></a><span class="sd">        - weight_decay: L2 regularization strength</span>
<a id="__codelineno-15-10" name="__codelineno-15-10" href="#__codelineno-15-10"></a><span class="sd">        - max_grad_norm: gradient clipping threshold</span>
<a id="__codelineno-15-11" name="__codelineno-15-11" href="#__codelineno-15-11"></a><span class="sd">        - patience: early stopping patience</span>
<a id="__codelineno-15-12" name="__codelineno-15-12" href="#__codelineno-15-12"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-15-13" name="__codelineno-15-13" href="#__codelineno-15-13"></a>    <span class="n">params</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span>
<a id="__codelineno-15-14" name="__codelineno-15-14" href="#__codelineno-15-14"></a>    <span class="n">best_val_loss</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">)</span>
<a id="__codelineno-15-15" name="__codelineno-15-15" href="#__codelineno-15-15"></a>    <span class="n">patience_counter</span> <span class="o">=</span> <span class="mi">0</span>
<a id="__codelineno-15-16" name="__codelineno-15-16" href="#__codelineno-15-16"></a>
<a id="__codelineno-15-17" name="__codelineno-15-17" href="#__codelineno-15-17"></a>    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;epochs&#39;</span><span class="p">]):</span>
<a id="__codelineno-15-18" name="__codelineno-15-18" href="#__codelineno-15-18"></a>        <span class="c1"># Learning rate schedule (linear decay)</span>
<a id="__codelineno-15-19" name="__codelineno-15-19" href="#__codelineno-15-19"></a>        <span class="n">lr</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;learning_rate&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">epoch</span> <span class="o">/</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;epochs&#39;</span><span class="p">])</span>
<a id="__codelineno-15-20" name="__codelineno-15-20" href="#__codelineno-15-20"></a>
<a id="__codelineno-15-21" name="__codelineno-15-21" href="#__codelineno-15-21"></a>        <span class="c1"># Training</span>
<a id="__codelineno-15-22" name="__codelineno-15-22" href="#__codelineno-15-22"></a>        <span class="n">model</span><span class="o">.</span><span class="n">train_mode</span> <span class="o">=</span> <span class="kc">True</span>
<a id="__codelineno-15-23" name="__codelineno-15-23" href="#__codelineno-15-23"></a>        <span class="n">batches</span> <span class="o">=</span> <span class="n">create_batches</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;batch_size&#39;</span><span class="p">])</span>
<a id="__codelineno-15-24" name="__codelineno-15-24" href="#__codelineno-15-24"></a>
<a id="__codelineno-15-25" name="__codelineno-15-25" href="#__codelineno-15-25"></a>        <span class="n">train_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
<a id="__codelineno-15-26" name="__codelineno-15-26" href="#__codelineno-15-26"></a>        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">batches</span><span class="p">:</span>
<a id="__codelineno-15-27" name="__codelineno-15-27" href="#__codelineno-15-27"></a>            <span class="c1"># Forward and backward</span>
<a id="__codelineno-15-28" name="__codelineno-15-28" href="#__codelineno-15-28"></a>            <span class="n">batch_loss</span> <span class="o">=</span> <span class="n">train_batch</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">lr</span><span class="p">)</span>
<a id="__codelineno-15-29" name="__codelineno-15-29" href="#__codelineno-15-29"></a>
<a id="__codelineno-15-30" name="__codelineno-15-30" href="#__codelineno-15-30"></a>            <span class="c1"># Gradient clipping</span>
<a id="__codelineno-15-31" name="__codelineno-15-31" href="#__codelineno-15-31"></a>            <span class="n">clip_gradients</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;max_grad_norm&#39;</span><span class="p">])</span>
<a id="__codelineno-15-32" name="__codelineno-15-32" href="#__codelineno-15-32"></a>
<a id="__codelineno-15-33" name="__codelineno-15-33" href="#__codelineno-15-33"></a>            <span class="c1"># Weight decay</span>
<a id="__codelineno-15-34" name="__codelineno-15-34" href="#__codelineno-15-34"></a>            <span class="n">apply_weight_decay</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;weight_decay&#39;</span><span class="p">])</span>
<a id="__codelineno-15-35" name="__codelineno-15-35" href="#__codelineno-15-35"></a>
<a id="__codelineno-15-36" name="__codelineno-15-36" href="#__codelineno-15-36"></a>            <span class="n">train_loss</span> <span class="o">+=</span> <span class="n">batch_loss</span>
<a id="__codelineno-15-37" name="__codelineno-15-37" href="#__codelineno-15-37"></a>
<a id="__codelineno-15-38" name="__codelineno-15-38" href="#__codelineno-15-38"></a>        <span class="n">train_loss</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">batches</span><span class="p">)</span>
<a id="__codelineno-15-39" name="__codelineno-15-39" href="#__codelineno-15-39"></a>
<a id="__codelineno-15-40" name="__codelineno-15-40" href="#__codelineno-15-40"></a>        <span class="c1"># Validation</span>
<a id="__codelineno-15-41" name="__codelineno-15-41" href="#__codelineno-15-41"></a>        <span class="n">model</span><span class="o">.</span><span class="n">train_mode</span> <span class="o">=</span> <span class="kc">False</span>
<a id="__codelineno-15-42" name="__codelineno-15-42" href="#__codelineno-15-42"></a>        <span class="n">val_loss</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">val_data</span><span class="p">)</span>
<a id="__codelineno-15-43" name="__codelineno-15-43" href="#__codelineno-15-43"></a>
<a id="__codelineno-15-44" name="__codelineno-15-44" href="#__codelineno-15-44"></a>        <span class="c1"># Logging</span>
<a id="__codelineno-15-45" name="__codelineno-15-45" href="#__codelineno-15-45"></a>        <span class="n">train_ppl</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">train_loss</span><span class="p">)</span>
<a id="__codelineno-15-46" name="__codelineno-15-46" href="#__codelineno-15-46"></a>        <span class="n">val_ppl</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">val_loss</span><span class="p">)</span>
<a id="__codelineno-15-47" name="__codelineno-15-47" href="#__codelineno-15-47"></a>        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">: &quot;</span>
<a id="__codelineno-15-48" name="__codelineno-15-48" href="#__codelineno-15-48"></a>              <span class="sa">f</span><span class="s2">&quot;train_loss=</span><span class="si">{</span><span class="n">train_loss</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (PPL=</span><span class="si">{</span><span class="n">train_ppl</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">), &quot;</span>
<a id="__codelineno-15-49" name="__codelineno-15-49" href="#__codelineno-15-49"></a>              <span class="sa">f</span><span class="s2">&quot;val_loss=</span><span class="si">{</span><span class="n">val_loss</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> (PPL=</span><span class="si">{</span><span class="n">val_ppl</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<a id="__codelineno-15-50" name="__codelineno-15-50" href="#__codelineno-15-50"></a>
<a id="__codelineno-15-51" name="__codelineno-15-51" href="#__codelineno-15-51"></a>        <span class="c1"># Early stopping check</span>
<a id="__codelineno-15-52" name="__codelineno-15-52" href="#__codelineno-15-52"></a>        <span class="k">if</span> <span class="n">val_loss</span> <span class="o">&lt;</span> <span class="n">best_val_loss</span><span class="p">:</span>
<a id="__codelineno-15-53" name="__codelineno-15-53" href="#__codelineno-15-53"></a>            <span class="n">best_val_loss</span> <span class="o">=</span> <span class="n">val_loss</span>
<a id="__codelineno-15-54" name="__codelineno-15-54" href="#__codelineno-15-54"></a>            <span class="n">patience_counter</span> <span class="o">=</span> <span class="mi">0</span>
<a id="__codelineno-15-55" name="__codelineno-15-55" href="#__codelineno-15-55"></a>            <span class="c1"># Save best model</span>
<a id="__codelineno-15-56" name="__codelineno-15-56" href="#__codelineno-15-56"></a>        <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-15-57" name="__codelineno-15-57" href="#__codelineno-15-57"></a>            <span class="n">patience_counter</span> <span class="o">+=</span> <span class="mi">1</span>
<a id="__codelineno-15-58" name="__codelineno-15-58" href="#__codelineno-15-58"></a>            <span class="k">if</span> <span class="n">patience_counter</span> <span class="o">&gt;=</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;patience&#39;</span><span class="p">]:</span>
<a id="__codelineno-15-59" name="__codelineno-15-59" href="#__codelineno-15-59"></a>                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Early stopping at epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<a id="__codelineno-15-60" name="__codelineno-15-60" href="#__codelineno-15-60"></a>                <span class="k">break</span>
<a id="__codelineno-15-61" name="__codelineno-15-61" href="#__codelineno-15-61"></a>
<a id="__codelineno-15-62" name="__codelineno-15-62" href="#__codelineno-15-62"></a>    <span class="k">return</span> <span class="n">model</span>
</code></pre></div>
<h2 id="summary">Summary<a class="headerlink" href="#summary" title="Permanent link">&para;</a></h2>
<table>
<thead>
<tr>
<th>Concept</th>
<th>Description</th>
<th>Practical Tip</th>
</tr>
</thead>
<tbody>
<tr>
<td>Learning rate</td>
<td>Step size for updates</td>
<td>Start at 0.01, adjust</td>
</tr>
<tr>
<td>LR schedule</td>
<td>Change LR over time</td>
<td>Decay helps convergence</td>
</tr>
<tr>
<td>Initialization</td>
<td>Starting parameter values</td>
<td>Use He for ReLU</td>
</tr>
<tr>
<td>Batch size</td>
<td>Examples per update</td>
<td>32-128 typical</td>
</tr>
<tr>
<td>Weight decay</td>
<td>L2 regularization</td>
<td>1e-4 to 1e-2</td>
</tr>
<tr>
<td>Gradient clipping</td>
<td>Prevent explosion</td>
<td>Max norm 1-5</td>
</tr>
<tr>
<td>Early stopping</td>
<td>Prevent overfitting</td>
<td>Patience 5-10</td>
</tr>
</tbody>
</table>
<p><strong>Key insight</strong>: Training neural networks is empirical. Start with defaults, monitor carefully, adjust based on what you observe. There's no substitute for running experiments.</p>
<h2 id="exercises">Exercises<a class="headerlink" href="#exercises" title="Permanent link">&para;</a></h2>
<ol>
<li>
<p><strong>Learning rate experiment</strong>: Train the model with learning rates 0.001, 0.01, 0.1, and 1.0. Plot the training curves. What do you observe?</p>
</li>
<li>
<p><strong>Initialization comparison</strong>: Compare training with Xavier init vs. random N(0, 1). How long until each converges?</p>
</li>
<li>
<p><strong>Batch size trade-off</strong>: Train with batch sizes 1, 16, 64, and 256. Compare wall-clock time to reach the same loss.</p>
</li>
<li>
<p><strong>Early stopping</strong>: Implement early stopping and compare final validation loss with and without it.</p>
</li>
<li>
<p><strong>Gradient analysis</strong>: Add logging for gradient norms. At what point in training are gradients largest?</p>
</li>
</ol>
<h2 id="whats-next">What's Next<a class="headerlink" href="#whats-next" title="Permanent link">&para;</a></h2>
<p>We can train our model. But how good is it really?</p>
<p>In Section 3.7, we'll <strong>evaluate our neural language model</strong> and compare it directly to the Markov models from Stage 1. We'll see concrete evidence of the neural advantage.</p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "../../..", "features": ["navigation.sections", "navigation.expand", "navigation.top", "toc.integrate", "content.code.copy"], "search": "../../../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../../assets/javascripts/bundle.79ae519e.min.js"></script>
      
        <script src="../../../javascripts/mathjax.js"></script>
      
        <script src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>